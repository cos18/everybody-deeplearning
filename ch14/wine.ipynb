{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예제 : 베스트 모델 만들기\n",
    "### 와인의 종류 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4655</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.016</td>\n",
       "      <td>6.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.98876</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.40</td>\n",
       "      <td>13.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.23</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.051</td>\n",
       "      <td>20.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.99430</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.42</td>\n",
       "      <td>10.7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.036</td>\n",
       "      <td>45.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.98964</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.51</td>\n",
       "      <td>12.4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>8.7</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.086</td>\n",
       "      <td>23.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1.00020</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.74</td>\n",
       "      <td>11.6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6098</th>\n",
       "      <td>5.2</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.26</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.053</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.99250</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.45</td>\n",
       "      <td>12.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2    3      4     5      6        7     8     9     10  11  \\\n",
       "4655  6.9  0.28  0.41  1.4  0.016   6.0   55.0  0.98876  3.16  0.40  13.4   5   \n",
       "6103  6.5  0.32  0.23  8.5  0.051  20.0  138.0  0.99430  3.03  0.42  10.7   5   \n",
       "4398  6.7  0.16  0.37  1.3  0.036  45.0  125.0  0.98964  3.19  0.51  12.4   7   \n",
       "499   8.7  0.69  0.31  3.0  0.086  23.0   81.0  1.00020  3.48  0.74  11.6   6   \n",
       "6098  5.2  0.38  0.26  7.7  0.053  20.0  103.0  0.99250  3.27  0.45  12.2   6   \n",
       "\n",
       "      12  \n",
       "4655   0  \n",
       "6103   0  \n",
       "4398   0  \n",
       "499    1  \n",
       "6098   0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 전체 값 중 샘플 뽑기 (frac=1 == 100%) => 순서가 랜덤\n",
    "wine_df = pd.read_csv('../dataset/wine.csv', header=None).sample(frac=1)\n",
    "wine_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6497 entries, 4655 to 6316\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       6497 non-null   float64\n",
      " 1   1       6497 non-null   float64\n",
      " 2   2       6497 non-null   float64\n",
      " 3   3       6497 non-null   float64\n",
      " 4   4       6497 non-null   float64\n",
      " 5   5       6497 non-null   float64\n",
      " 6   6       6497 non-null   float64\n",
      " 7   7       6497 non-null   float64\n",
      " 8   8       6497 non-null   float64\n",
      " 9   9       6497 non-null   float64\n",
      " 10  10      6497 non-null   float64\n",
      " 11  11      6497 non-null   int64  \n",
      " 12  12      6497 non-null   int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 710.6 KB\n"
     ]
    }
   ],
   "source": [
    "wine_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_dataset = wine_df.values\n",
    "X = wine_dataset[:,0:12]\n",
    "Y = wine_dataset[:,12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 설계 & 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6497/6497 [==============================] - 1s 180us/step - loss: 0.4326 - accuracy: 0.7868\n",
      "Epoch 2/200\n",
      "6497/6497 [==============================] - 0s 34us/step - loss: 0.2745 - accuracy: 0.9052\n",
      "Epoch 3/200\n",
      "6497/6497 [==============================] - 0s 31us/step - loss: 0.2187 - accuracy: 0.9266\n",
      "Epoch 4/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.2073 - accuracy: 0.9278\n",
      "Epoch 5/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.1984 - accuracy: 0.9306\n",
      "Epoch 6/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.1913 - accuracy: 0.9326\n",
      "Epoch 7/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.1875 - accuracy: 0.9347\n",
      "Epoch 8/200\n",
      "6497/6497 [==============================] - 0s 18us/step - loss: 0.1830 - accuracy: 0.9367\n",
      "Epoch 9/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.1814 - accuracy: 0.9378\n",
      "Epoch 10/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.1732 - accuracy: 0.9394\n",
      "Epoch 11/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.1646 - accuracy: 0.9418\n",
      "Epoch 12/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.1581 - accuracy: 0.9427\n",
      "Epoch 13/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.1557 - accuracy: 0.9435\n",
      "Epoch 14/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.1469 - accuracy: 0.9457\n",
      "Epoch 15/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.1436 - accuracy: 0.9484\n",
      "Epoch 16/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.1350 - accuracy: 0.9504\n",
      "Epoch 17/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.1311 - accuracy: 0.9544\n",
      "Epoch 18/200\n",
      "6497/6497 [==============================] - 0s 19us/step - loss: 0.1278 - accuracy: 0.9537\n",
      "Epoch 19/200\n",
      "6497/6497 [==============================] - 0s 19us/step - loss: 0.1198 - accuracy: 0.9575\n",
      "Epoch 20/200\n",
      "6497/6497 [==============================] - 0s 19us/step - loss: 0.1200 - accuracy: 0.9575\n",
      "Epoch 21/200\n",
      "6497/6497 [==============================] - 0s 19us/step - loss: 0.1126 - accuracy: 0.9578\n",
      "Epoch 22/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.1080 - accuracy: 0.9618\n",
      "Epoch 23/200\n",
      "6497/6497 [==============================] - 0s 20us/step - loss: 0.1052 - accuracy: 0.9643\n",
      "Epoch 24/200\n",
      "6497/6497 [==============================] - 0s 17us/step - loss: 0.1011 - accuracy: 0.9663\n",
      "Epoch 25/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0987 - accuracy: 0.9672\n",
      "Epoch 26/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0988 - accuracy: 0.9669\n",
      "Epoch 27/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0946 - accuracy: 0.9686\n",
      "Epoch 28/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0920 - accuracy: 0.9709\n",
      "Epoch 29/200\n",
      "6497/6497 [==============================] - 0s 20us/step - loss: 0.0908 - accuracy: 0.9724\n",
      "Epoch 30/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.0927 - accuracy: 0.9695\n",
      "Epoch 31/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0856 - accuracy: 0.9729\n",
      "Epoch 32/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0837 - accuracy: 0.9738\n",
      "Epoch 33/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0811 - accuracy: 0.9746\n",
      "Epoch 34/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0890 - accuracy: 0.9717\n",
      "Epoch 35/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0810 - accuracy: 0.9751\n",
      "Epoch 36/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0789 - accuracy: 0.9754\n",
      "Epoch 37/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0818 - accuracy: 0.9744\n",
      "Epoch 38/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0766 - accuracy: 0.9771\n",
      "Epoch 39/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0764 - accuracy: 0.9768\n",
      "Epoch 40/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0736 - accuracy: 0.9768\n",
      "Epoch 41/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0764 - accuracy: 0.9761\n",
      "Epoch 42/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0750 - accuracy: 0.9781\n",
      "Epoch 43/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0776 - accuracy: 0.9752\n",
      "Epoch 44/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0711 - accuracy: 0.9789\n",
      "Epoch 45/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0702 - accuracy: 0.9786\n",
      "Epoch 46/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0710 - accuracy: 0.9786\n",
      "Epoch 47/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0687 - accuracy: 0.9774\n",
      "Epoch 48/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0674 - accuracy: 0.9794\n",
      "Epoch 49/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.0674 - accuracy: 0.9797\n",
      "Epoch 50/200\n",
      "6497/6497 [==============================] - 0s 22us/step - loss: 0.0736 - accuracy: 0.9785\n",
      "Epoch 51/200\n",
      "6497/6497 [==============================] - 0s 18us/step - loss: 0.0699 - accuracy: 0.9789\n",
      "Epoch 52/200\n",
      "6497/6497 [==============================] - 0s 17us/step - loss: 0.0718 - accuracy: 0.9778\n",
      "Epoch 53/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.0670 - accuracy: 0.9801\n",
      "Epoch 54/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0688 - accuracy: 0.9780\n",
      "Epoch 55/200\n",
      "6497/6497 [==============================] - 0s 18us/step - loss: 0.0659 - accuracy: 0.9820\n",
      "Epoch 56/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0629 - accuracy: 0.9811\n",
      "Epoch 57/200\n",
      "6497/6497 [==============================] - 0s 20us/step - loss: 0.0654 - accuracy: 0.9795\n",
      "Epoch 58/200\n",
      "6497/6497 [==============================] - 0s 24us/step - loss: 0.0706 - accuracy: 0.9771\n",
      "Epoch 59/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0659 - accuracy: 0.9815\n",
      "Epoch 60/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0644 - accuracy: 0.9820\n",
      "Epoch 61/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0645 - accuracy: 0.9808\n",
      "Epoch 62/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0622 - accuracy: 0.9817\n",
      "Epoch 63/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.0610 - accuracy: 0.9825\n",
      "Epoch 64/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0600 - accuracy: 0.9825\n",
      "Epoch 65/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0600 - accuracy: 0.9832\n",
      "Epoch 66/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0648 - accuracy: 0.9806\n",
      "Epoch 67/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0619 - accuracy: 0.9806\n",
      "Epoch 68/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0591 - accuracy: 0.9829\n",
      "Epoch 69/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0596 - accuracy: 0.9834\n",
      "Epoch 70/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0598 - accuracy: 0.9821\n",
      "Epoch 71/200\n",
      "6497/6497 [==============================] - 0s 12us/step - loss: 0.0601 - accuracy: 0.9831\n",
      "Epoch 72/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0584 - accuracy: 0.9840\n",
      "Epoch 73/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0583 - accuracy: 0.9834\n",
      "Epoch 74/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0588 - accuracy: 0.9831\n",
      "Epoch 75/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0610 - accuracy: 0.9821\n",
      "Epoch 76/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0568 - accuracy: 0.9837\n",
      "Epoch 77/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0655 - accuracy: 0.9812\n",
      "Epoch 78/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0625 - accuracy: 0.9841\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0628 - accuracy: 0.9808\n",
      "Epoch 80/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0560 - accuracy: 0.9843\n",
      "Epoch 81/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0565 - accuracy: 0.9837\n",
      "Epoch 82/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0558 - accuracy: 0.9841\n",
      "Epoch 83/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0576 - accuracy: 0.9843\n",
      "Epoch 84/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0579 - accuracy: 0.9826\n",
      "Epoch 85/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0569 - accuracy: 0.9841\n",
      "Epoch 86/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0573 - accuracy: 0.9840\n",
      "Epoch 87/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0659 - accuracy: 0.9798\n",
      "Epoch 88/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0564 - accuracy: 0.9840\n",
      "Epoch 89/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0550 - accuracy: 0.9841\n",
      "Epoch 90/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0586 - accuracy: 0.9825\n",
      "Epoch 91/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0561 - accuracy: 0.9832\n",
      "Epoch 92/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0575 - accuracy: 0.9835\n",
      "Epoch 93/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0579 - accuracy: 0.9843\n",
      "Epoch 94/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0556 - accuracy: 0.9837\n",
      "Epoch 95/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0559 - accuracy: 0.9849\n",
      "Epoch 96/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0551 - accuracy: 0.9849\n",
      "Epoch 97/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0580 - accuracy: 0.9826\n",
      "Epoch 98/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0555 - accuracy: 0.9845\n",
      "Epoch 99/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0577 - accuracy: 0.9835\n",
      "Epoch 100/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0553 - accuracy: 0.9849\n",
      "Epoch 101/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0566 - accuracy: 0.9846\n",
      "Epoch 102/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0571 - accuracy: 0.9838\n",
      "Epoch 103/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0530 - accuracy: 0.9851\n",
      "Epoch 104/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0556 - accuracy: 0.9845\n",
      "Epoch 105/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0537 - accuracy: 0.9852\n",
      "Epoch 106/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0537 - accuracy: 0.9845\n",
      "Epoch 107/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0582 - accuracy: 0.9820\n",
      "Epoch 108/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0538 - accuracy: 0.9855\n",
      "Epoch 109/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.0564 - accuracy: 0.9840\n",
      "Epoch 110/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0552 - accuracy: 0.9845\n",
      "Epoch 111/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0532 - accuracy: 0.9858\n",
      "Epoch 112/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0559 - accuracy: 0.9835\n",
      "Epoch 113/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0587 - accuracy: 0.9829\n",
      "Epoch 114/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0543 - accuracy: 0.9854\n",
      "Epoch 115/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0532 - accuracy: 0.9860\n",
      "Epoch 116/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0520 - accuracy: 0.9854\n",
      "Epoch 117/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0551 - accuracy: 0.9835\n",
      "Epoch 118/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0556 - accuracy: 0.9855\n",
      "Epoch 119/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0572 - accuracy: 0.9840\n",
      "Epoch 120/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0535 - accuracy: 0.9840\n",
      "Epoch 121/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0525 - accuracy: 0.9863\n",
      "Epoch 122/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0566 - accuracy: 0.9845\n",
      "Epoch 123/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0631 - accuracy: 0.9815\n",
      "Epoch 124/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0529 - accuracy: 0.9861\n",
      "Epoch 125/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0527 - accuracy: 0.9858\n",
      "Epoch 126/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0524 - accuracy: 0.9858\n",
      "Epoch 127/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0542 - accuracy: 0.9848\n",
      "Epoch 128/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0510 - accuracy: 0.9868\n",
      "Epoch 129/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0564 - accuracy: 0.9831\n",
      "Epoch 130/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0520 - accuracy: 0.9855\n",
      "Epoch 131/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0527 - accuracy: 0.9861\n",
      "Epoch 132/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0571 - accuracy: 0.9835\n",
      "Epoch 133/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0540 - accuracy: 0.9841\n",
      "Epoch 134/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0519 - accuracy: 0.9851\n",
      "Epoch 135/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0522 - accuracy: 0.9863\n",
      "Epoch 136/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0519 - accuracy: 0.9854\n",
      "Epoch 137/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0524 - accuracy: 0.9854\n",
      "Epoch 138/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0562 - accuracy: 0.9860\n",
      "Epoch 139/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0529 - accuracy: 0.9841\n",
      "Epoch 140/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0514 - accuracy: 0.9865\n",
      "Epoch 141/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0499 - accuracy: 0.9869\n",
      "Epoch 142/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0563 - accuracy: 0.9848\n",
      "Epoch 143/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0506 - accuracy: 0.9861\n",
      "Epoch 144/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0540 - accuracy: 0.9857\n",
      "Epoch 145/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0560 - accuracy: 0.9851\n",
      "Epoch 146/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0565 - accuracy: 0.9846\n",
      "Epoch 147/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0495 - accuracy: 0.9866\n",
      "Epoch 148/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0511 - accuracy: 0.9858\n",
      "Epoch 149/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0500 - accuracy: 0.9868\n",
      "Epoch 150/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0539 - accuracy: 0.9852\n",
      "Epoch 151/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0531 - accuracy: 0.9855\n",
      "Epoch 152/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0502 - accuracy: 0.9872\n",
      "Epoch 153/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0509 - accuracy: 0.9861\n",
      "Epoch 154/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0507 - accuracy: 0.9861\n",
      "Epoch 155/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0524 - accuracy: 0.9860\n",
      "Epoch 156/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0509 - accuracy: 0.9852\n",
      "Epoch 157/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0521 - accuracy: 0.9857\n",
      "Epoch 158/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0513 - accuracy: 0.9858\n",
      "Epoch 159/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0499 - accuracy: 0.9863\n",
      "Epoch 160/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0497 - accuracy: 0.9878\n",
      "Epoch 161/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0498 - accuracy: 0.9865\n",
      "Epoch 162/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0497 - accuracy: 0.9872\n",
      "Epoch 163/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0540 - accuracy: 0.9854\n",
      "Epoch 164/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0506 - accuracy: 0.9858\n",
      "Epoch 165/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0484 - accuracy: 0.9871\n",
      "Epoch 166/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0543 - accuracy: 0.9849\n",
      "Epoch 167/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0497 - accuracy: 0.9869\n",
      "Epoch 168/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0498 - accuracy: 0.9861\n",
      "Epoch 169/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0495 - accuracy: 0.9863\n",
      "Epoch 170/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0495 - accuracy: 0.9866\n",
      "Epoch 171/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0502 - accuracy: 0.9863\n",
      "Epoch 172/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0492 - accuracy: 0.9868\n",
      "Epoch 173/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0532 - accuracy: 0.9852\n",
      "Epoch 174/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0532 - accuracy: 0.9857\n",
      "Epoch 175/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0484 - accuracy: 0.9883\n",
      "Epoch 176/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0485 - accuracy: 0.9872\n",
      "Epoch 177/200\n",
      "6497/6497 [==============================] - 0s 16us/step - loss: 0.0487 - accuracy: 0.9878\n",
      "Epoch 178/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0484 - accuracy: 0.9865\n",
      "Epoch 179/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0491 - accuracy: 0.9852\n",
      "Epoch 180/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0563 - accuracy: 0.9825\n",
      "Epoch 181/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0557 - accuracy: 0.9838\n",
      "Epoch 182/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0481 - accuracy: 0.9875\n",
      "Epoch 183/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0487 - accuracy: 0.9865\n",
      "Epoch 184/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0490 - accuracy: 0.9874\n",
      "Epoch 185/200\n",
      "6497/6497 [==============================] - 0s 22us/step - loss: 0.0574 - accuracy: 0.9843\n",
      "Epoch 186/200\n",
      "6497/6497 [==============================] - 0s 30us/step - loss: 0.0514 - accuracy: 0.9858\n",
      "Epoch 187/200\n",
      "6497/6497 [==============================] - 0s 21us/step - loss: 0.0512 - accuracy: 0.9861\n",
      "Epoch 188/200\n",
      "6497/6497 [==============================] - 0s 22us/step - loss: 0.0516 - accuracy: 0.9861\n",
      "Epoch 189/200\n",
      "6497/6497 [==============================] - 0s 17us/step - loss: 0.0476 - accuracy: 0.9872\n",
      "Epoch 190/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0475 - accuracy: 0.9874\n",
      "Epoch 191/200\n",
      "6497/6497 [==============================] - 0s 13us/step - loss: 0.0488 - accuracy: 0.9868\n",
      "Epoch 192/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0509 - accuracy: 0.9865\n",
      "Epoch 193/200\n",
      "6497/6497 [==============================] - 0s 19us/step - loss: 0.0565 - accuracy: 0.9841\n",
      "Epoch 194/200\n",
      "6497/6497 [==============================] - 0s 18us/step - loss: 0.0531 - accuracy: 0.9869\n",
      "Epoch 195/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0508 - accuracy: 0.9866\n",
      "Epoch 196/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0503 - accuracy: 0.9858\n",
      "Epoch 197/200\n",
      "6497/6497 [==============================] - 0s 15us/step - loss: 0.0497 - accuracy: 0.9858\n",
      "Epoch 198/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0490 - accuracy: 0.9866\n",
      "Epoch 199/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0489 - accuracy: 0.9880\n",
      "Epoch 200/200\n",
      "6497/6497 [==============================] - 0s 14us/step - loss: 0.0487 - accuracy: 0.9878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x642c54dd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=200, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497/6497 [==============================] - 0s 44us/step\n",
      "Accuracy: 0.9884561896324158\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 모델 저장해 비교해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "modelpath = ''.join([MODEL_DIR, \"{epoch:02d}-{val_loss:.4f}.hdf5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26113, saving model to ./model/01-0.2611.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26113 to 0.21556, saving model to ./model/02-0.2156.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.21556 to 0.20116, saving model to ./model/03-0.2012.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.20116 to 0.19232, saving model to ./model/04-0.1923.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.19232 to 0.18469, saving model to ./model/05-0.1847.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.18469 to 0.17815, saving model to ./model/06-0.1781.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.17815 to 0.17412, saving model to ./model/07-0.1741.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.17412 to 0.16907, saving model to ./model/08-0.1691.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.16907 to 0.16373, saving model to ./model/09-0.1637.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.16373 to 0.16237, saving model to ./model/10-0.1624.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.16237 to 0.15054, saving model to ./model/11-0.1505.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.15054 to 0.14866, saving model to ./model/12-0.1487.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.14866 to 0.14117, saving model to ./model/13-0.1412.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.14117 to 0.13412, saving model to ./model/14-0.1341.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.13412 to 0.12912, saving model to ./model/15-0.1291.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.12912 to 0.12495, saving model to ./model/16-0.1249.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.12495 to 0.12149, saving model to ./model/17-0.1215.hdf5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.12149 to 0.11583, saving model to ./model/18-0.1158.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.11583 to 0.10872, saving model to ./model/19-0.1087.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.10872 to 0.10830, saving model to ./model/20-0.1083.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.10830 to 0.09987, saving model to ./model/21-0.0999.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.09987 to 0.09287, saving model to ./model/22-0.0929.hdf5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.09287 to 0.08832, saving model to ./model/23-0.0883.hdf5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.08832\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.08832 to 0.08577, saving model to ./model/25-0.0858.hdf5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.08577\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.08577 to 0.08085, saving model to ./model/27-0.0809.hdf5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.08085\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.08085 to 0.07860, saving model to ./model/29-0.0786.hdf5\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.07860 to 0.07850, saving model to ./model/30-0.0785.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.07850 to 0.07247, saving model to ./model/31-0.0725.hdf5\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.07247 to 0.07174, saving model to ./model/32-0.0717.hdf5\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.07174 to 0.06780, saving model to ./model/33-0.0678.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.06780 to 0.06651, saving model to ./model/34-0.0665.hdf5\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.06651\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.06651 to 0.06548, saving model to ./model/36-0.0655.hdf5\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.06548 to 0.06189, saving model to ./model/37-0.0619.hdf5\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.06189 to 0.06150, saving model to ./model/38-0.0615.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.06150 to 0.06068, saving model to ./model/39-0.0607.hdf5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.06068\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.06068\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.06068 to 0.05562, saving model to ./model/42-0.0556.hdf5\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.05562 to 0.05520, saving model to ./model/43-0.0552.hdf5\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.05520 to 0.05337, saving model to ./model/44-0.0534.hdf5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.05337 to 0.05289, saving model to ./model/45-0.0529.hdf5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.05289 to 0.05209, saving model to ./model/46-0.0521.hdf5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.05209\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.05209 to 0.05159, saving model to ./model/48-0.0516.hdf5\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.05159\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.05159 to 0.05008, saving model to ./model/50-0.0501.hdf5\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.05008 to 0.04903, saving model to ./model/51-0.0490.hdf5\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.04903 to 0.04783, saving model to ./model/52-0.0478.hdf5\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.04783 to 0.04729, saving model to ./model/53-0.0473.hdf5\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.04729 to 0.04651, saving model to ./model/54-0.0465.hdf5\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.04651\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.04651\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.04651\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.04651 to 0.04650, saving model to ./model/58-0.0465.hdf5\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.04650\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.04650 to 0.04506, saving model to ./model/60-0.0451.hdf5\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.04506 to 0.04394, saving model to ./model/61-0.0439.hdf5\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.04394\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.04394\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.04394\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.04394 to 0.04339, saving model to ./model/65-0.0434.hdf5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.04339\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.04339 to 0.04236, saving model to ./model/67-0.0424.hdf5\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.04236\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.04236 to 0.04207, saving model to ./model/69-0.0421.hdf5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.04207\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.04207 to 0.04167, saving model to ./model/71-0.0417.hdf5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.04167\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.04167\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.04167 to 0.04068, saving model to ./model/74-0.0407.hdf5\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.04068 to 0.04053, saving model to ./model/75-0.0405.hdf5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.04053\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.04053 to 0.03981, saving model to ./model/77-0.0398.hdf5\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.03981\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.03981\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.03981\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.03981\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.03981\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.03981\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.03981\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.03981\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.03981\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.03981 to 0.03929, saving model to ./model/87-0.0393.hdf5\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.03929 to 0.03884, saving model to ./model/88-0.0388.hdf5\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.03884 to 0.03824, saving model to ./model/89-0.0382.hdf5\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.03824\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.03824\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.03824\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.03824 to 0.03809, saving model to ./model/93-0.0381.hdf5\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.03809\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.03809\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.03809\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.03809\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.03809\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.03809 to 0.03664, saving model to ./model/99-0.0366.hdf5\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.03664\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.03664\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.03664\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.03664\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.03664\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.03664 to 0.03634, saving model to ./model/105-0.0363.hdf5\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.03634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00107: val_loss did not improve from 0.03634\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.03634\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.03634\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.03634 to 0.03604, saving model to ./model/110-0.0360.hdf5\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.03604\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.03604\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.03604\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.03604\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.03604 to 0.03541, saving model to ./model/115-0.0354.hdf5\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.03541\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.03541\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.03541\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.03541\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.03541\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.03541\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.03541\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.03541\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.03541\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.03541 to 0.03532, saving model to ./model/125-0.0353.hdf5\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.03532\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.03532\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.03532\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.03532 to 0.03429, saving model to ./model/129-0.0343.hdf5\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.03429\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.03429 to 0.03336, saving model to ./model/144-0.0334.hdf5\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.03336\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.03336 to 0.03315, saving model to ./model/146-0.0332.hdf5\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.03315\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.03315\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.03315\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.03315\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.03315 to 0.03136, saving model to ./model/151-0.0314.hdf5\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.03136\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.03136 to 0.03076, saving model to ./model/153-0.0308.hdf5\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.03076\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.03076 to 0.03051, saving model to ./model/155-0.0305.hdf5\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.03051\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.03051\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.03051\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.03051\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.03051 to 0.02996, saving model to ./model/160-0.0300.hdf5\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.02996\n",
      "\n",
      "Epoch 00174: val_loss improved from 0.02996 to 0.02895, saving model to ./model/174-0.0289.hdf5\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.02895\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.02895 to 0.02815, saving model to ./model/195-0.0282.hdf5\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.02815 to 0.02783, saving model to ./model/196-0.0278.hdf5\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.02783\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.02783\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.02783\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.02783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x642c413d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200, verbose=0, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그래프로 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 653 samples, validate on 322 samples\n",
      "Epoch 1/3500\n",
      "653/653 [==============================] - 1s 2ms/step - loss: 3.1995 - accuracy: 0.3216 - val_loss: 2.0184 - val_accuracy: 0.3789\n",
      "Epoch 2/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 1.8952 - accuracy: 0.4288 - val_loss: 1.1032 - val_accuracy: 0.4969\n",
      "Epoch 3/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 1.0041 - accuracy: 0.5789 - val_loss: 0.7161 - val_accuracy: 0.7081\n",
      "Epoch 4/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.6289 - accuracy: 0.7412 - val_loss: 0.6226 - val_accuracy: 0.7484\n",
      "Epoch 5/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.5121 - accuracy: 0.7825 - val_loss: 0.6346 - val_accuracy: 0.7547\n",
      "Epoch 6/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.5014 - accuracy: 0.7688 - val_loss: 0.6685 - val_accuracy: 0.7484\n",
      "Epoch 7/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.5213 - accuracy: 0.7688 - val_loss: 0.6893 - val_accuracy: 0.7391\n",
      "Epoch 8/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.5374 - accuracy: 0.7565 - val_loss: 0.6915 - val_accuracy: 0.7360\n",
      "Epoch 9/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.5386 - accuracy: 0.7565 - val_loss: 0.6754 - val_accuracy: 0.7360\n",
      "Epoch 10/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.5260 - accuracy: 0.7565 - val_loss: 0.6462 - val_accuracy: 0.7360\n",
      "Epoch 11/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.5022 - accuracy: 0.7596 - val_loss: 0.6090 - val_accuracy: 0.7360\n",
      "Epoch 12/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.4723 - accuracy: 0.7688 - val_loss: 0.5662 - val_accuracy: 0.7391\n",
      "Epoch 13/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.4391 - accuracy: 0.7749 - val_loss: 0.5260 - val_accuracy: 0.7516\n",
      "Epoch 14/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.4064 - accuracy: 0.7902 - val_loss: 0.4901 - val_accuracy: 0.7578\n",
      "Epoch 15/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.3787 - accuracy: 0.7994 - val_loss: 0.4602 - val_accuracy: 0.7702\n",
      "Epoch 16/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.3568 - accuracy: 0.8086 - val_loss: 0.4367 - val_accuracy: 0.7888\n",
      "Epoch 17/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.3410 - accuracy: 0.8116 - val_loss: 0.4193 - val_accuracy: 0.8043\n",
      "Epoch 18/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.3298 - accuracy: 0.8208 - val_loss: 0.4072 - val_accuracy: 0.8137\n",
      "Epoch 19/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.3228 - accuracy: 0.8300 - val_loss: 0.3981 - val_accuracy: 0.8416\n",
      "Epoch 20/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.3181 - accuracy: 0.8438 - val_loss: 0.3890 - val_accuracy: 0.8509\n",
      "Epoch 21/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.3126 - accuracy: 0.8499 - val_loss: 0.3791 - val_accuracy: 0.8540\n",
      "Epoch 22/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.3058 - accuracy: 0.8560 - val_loss: 0.3712 - val_accuracy: 0.8571\n",
      "Epoch 23/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.2980 - accuracy: 0.8606 - val_loss: 0.3640 - val_accuracy: 0.8634\n",
      "Epoch 24/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.2890 - accuracy: 0.8668 - val_loss: 0.3586 - val_accuracy: 0.8696\n",
      "Epoch 25/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.2815 - accuracy: 0.8668 - val_loss: 0.3557 - val_accuracy: 0.8727\n",
      "Epoch 26/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2758 - accuracy: 0.8668 - val_loss: 0.3536 - val_accuracy: 0.8727\n",
      "Epoch 27/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2713 - accuracy: 0.8760 - val_loss: 0.3519 - val_accuracy: 0.8727\n",
      "Epoch 28/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.2677 - accuracy: 0.8806 - val_loss: 0.3493 - val_accuracy: 0.8789\n",
      "Epoch 29/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.2639 - accuracy: 0.8897 - val_loss: 0.3458 - val_accuracy: 0.8975\n",
      "Epoch 30/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2606 - accuracy: 0.8974 - val_loss: 0.3412 - val_accuracy: 0.9068\n",
      "Epoch 31/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2563 - accuracy: 0.9035 - val_loss: 0.3353 - val_accuracy: 0.9224\n",
      "Epoch 32/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2521 - accuracy: 0.9142 - val_loss: 0.3291 - val_accuracy: 0.9286\n",
      "Epoch 33/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2474 - accuracy: 0.9173 - val_loss: 0.3233 - val_accuracy: 0.9317\n",
      "Epoch 34/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2441 - accuracy: 0.9204 - val_loss: 0.3180 - val_accuracy: 0.9317\n",
      "Epoch 35/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.2418 - accuracy: 0.9219 - val_loss: 0.3137 - val_accuracy: 0.9379\n",
      "Epoch 36/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2391 - accuracy: 0.9250 - val_loss: 0.3098 - val_accuracy: 0.9317\n",
      "Epoch 37/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.2364 - accuracy: 0.9280 - val_loss: 0.3063 - val_accuracy: 0.9286\n",
      "Epoch 38/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.2333 - accuracy: 0.9280 - val_loss: 0.3030 - val_accuracy: 0.9286\n",
      "Epoch 39/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.2305 - accuracy: 0.9311 - val_loss: 0.2999 - val_accuracy: 0.9317\n",
      "Epoch 40/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2279 - accuracy: 0.9342 - val_loss: 0.2968 - val_accuracy: 0.9317\n",
      "Epoch 41/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.2255 - accuracy: 0.9357 - val_loss: 0.2941 - val_accuracy: 0.9317\n",
      "Epoch 42/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.2231 - accuracy: 0.9372 - val_loss: 0.2915 - val_accuracy: 0.9317\n",
      "Epoch 43/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.2207 - accuracy: 0.9387 - val_loss: 0.2887 - val_accuracy: 0.9317\n",
      "Epoch 44/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2182 - accuracy: 0.9387 - val_loss: 0.2859 - val_accuracy: 0.9317\n",
      "Epoch 45/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.2157 - accuracy: 0.9403 - val_loss: 0.2829 - val_accuracy: 0.9317\n",
      "Epoch 46/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2134 - accuracy: 0.9387 - val_loss: 0.2804 - val_accuracy: 0.9348\n",
      "Epoch 47/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2112 - accuracy: 0.9403 - val_loss: 0.2778 - val_accuracy: 0.9348\n",
      "Epoch 48/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.2090 - accuracy: 0.9418 - val_loss: 0.2757 - val_accuracy: 0.9379\n",
      "Epoch 49/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.2071 - accuracy: 0.9372 - val_loss: 0.2742 - val_accuracy: 0.9379\n",
      "Epoch 50/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2049 - accuracy: 0.9372 - val_loss: 0.2732 - val_accuracy: 0.9379\n",
      "Epoch 51/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.2031 - accuracy: 0.9387 - val_loss: 0.2725 - val_accuracy: 0.9379\n",
      "Epoch 52/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.2019 - accuracy: 0.9387 - val_loss: 0.2718 - val_accuracy: 0.9379\n",
      "Epoch 53/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.2001 - accuracy: 0.9387 - val_loss: 0.2706 - val_accuracy: 0.9379\n",
      "Epoch 54/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1985 - accuracy: 0.9387 - val_loss: 0.2697 - val_accuracy: 0.9379\n",
      "Epoch 55/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1972 - accuracy: 0.9342 - val_loss: 0.2689 - val_accuracy: 0.9379\n",
      "Epoch 56/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1958 - accuracy: 0.9326 - val_loss: 0.2680 - val_accuracy: 0.9379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1947 - accuracy: 0.9342 - val_loss: 0.2671 - val_accuracy: 0.9379\n",
      "Epoch 58/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1936 - accuracy: 0.9357 - val_loss: 0.2663 - val_accuracy: 0.9379\n",
      "Epoch 59/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1924 - accuracy: 0.9372 - val_loss: 0.2658 - val_accuracy: 0.9379\n",
      "Epoch 60/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1917 - accuracy: 0.9403 - val_loss: 0.2658 - val_accuracy: 0.9379\n",
      "Epoch 61/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1914 - accuracy: 0.9372 - val_loss: 0.2657 - val_accuracy: 0.9379\n",
      "Epoch 62/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1913 - accuracy: 0.9387 - val_loss: 0.2647 - val_accuracy: 0.9379\n",
      "Epoch 63/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1900 - accuracy: 0.9403 - val_loss: 0.2630 - val_accuracy: 0.9379\n",
      "Epoch 64/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1885 - accuracy: 0.9403 - val_loss: 0.2619 - val_accuracy: 0.9379\n",
      "Epoch 65/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1880 - accuracy: 0.9403 - val_loss: 0.2614 - val_accuracy: 0.9410\n",
      "Epoch 66/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1881 - accuracy: 0.9387 - val_loss: 0.2610 - val_accuracy: 0.9379\n",
      "Epoch 67/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1876 - accuracy: 0.9403 - val_loss: 0.2602 - val_accuracy: 0.9410\n",
      "Epoch 68/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1868 - accuracy: 0.9403 - val_loss: 0.2595 - val_accuracy: 0.9410\n",
      "Epoch 69/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1857 - accuracy: 0.9387 - val_loss: 0.2588 - val_accuracy: 0.9410\n",
      "Epoch 70/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1849 - accuracy: 0.9387 - val_loss: 0.2584 - val_accuracy: 0.9379\n",
      "Epoch 71/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1845 - accuracy: 0.9403 - val_loss: 0.2580 - val_accuracy: 0.9379\n",
      "Epoch 72/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1838 - accuracy: 0.9418 - val_loss: 0.2574 - val_accuracy: 0.9379\n",
      "Epoch 73/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1832 - accuracy: 0.9418 - val_loss: 0.2570 - val_accuracy: 0.9410\n",
      "Epoch 74/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1825 - accuracy: 0.9418 - val_loss: 0.2565 - val_accuracy: 0.9410\n",
      "Epoch 75/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1821 - accuracy: 0.9403 - val_loss: 0.2561 - val_accuracy: 0.9410\n",
      "Epoch 76/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1815 - accuracy: 0.9403 - val_loss: 0.2559 - val_accuracy: 0.9410\n",
      "Epoch 77/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1809 - accuracy: 0.9403 - val_loss: 0.2557 - val_accuracy: 0.9410\n",
      "Epoch 78/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1803 - accuracy: 0.9403 - val_loss: 0.2558 - val_accuracy: 0.9379\n",
      "Epoch 79/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1796 - accuracy: 0.9418 - val_loss: 0.2560 - val_accuracy: 0.9379\n",
      "Epoch 80/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1792 - accuracy: 0.9403 - val_loss: 0.2558 - val_accuracy: 0.9379\n",
      "Epoch 81/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1784 - accuracy: 0.9418 - val_loss: 0.2551 - val_accuracy: 0.9379\n",
      "Epoch 82/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1779 - accuracy: 0.9403 - val_loss: 0.2544 - val_accuracy: 0.9441\n",
      "Epoch 83/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1776 - accuracy: 0.9403 - val_loss: 0.2539 - val_accuracy: 0.9472\n",
      "Epoch 84/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1775 - accuracy: 0.9403 - val_loss: 0.2535 - val_accuracy: 0.9472\n",
      "Epoch 85/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1769 - accuracy: 0.9403 - val_loss: 0.2534 - val_accuracy: 0.9472\n",
      "Epoch 86/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1767 - accuracy: 0.9403 - val_loss: 0.2535 - val_accuracy: 0.9410\n",
      "Epoch 87/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1759 - accuracy: 0.9418 - val_loss: 0.2534 - val_accuracy: 0.9410\n",
      "Epoch 88/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1753 - accuracy: 0.9418 - val_loss: 0.2534 - val_accuracy: 0.9410\n",
      "Epoch 89/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1748 - accuracy: 0.9418 - val_loss: 0.2533 - val_accuracy: 0.9441\n",
      "Epoch 90/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1744 - accuracy: 0.9418 - val_loss: 0.2532 - val_accuracy: 0.9472\n",
      "Epoch 91/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1741 - accuracy: 0.9418 - val_loss: 0.2531 - val_accuracy: 0.9472\n",
      "Epoch 92/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1737 - accuracy: 0.9403 - val_loss: 0.2530 - val_accuracy: 0.9472\n",
      "Epoch 93/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1733 - accuracy: 0.9418 - val_loss: 0.2527 - val_accuracy: 0.9472\n",
      "Epoch 94/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1729 - accuracy: 0.9418 - val_loss: 0.2525 - val_accuracy: 0.9472\n",
      "Epoch 95/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1725 - accuracy: 0.9418 - val_loss: 0.2526 - val_accuracy: 0.9441\n",
      "Epoch 96/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1723 - accuracy: 0.9418 - val_loss: 0.2525 - val_accuracy: 0.9441\n",
      "Epoch 97/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1719 - accuracy: 0.9433 - val_loss: 0.2518 - val_accuracy: 0.9472\n",
      "Epoch 98/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1716 - accuracy: 0.9418 - val_loss: 0.2509 - val_accuracy: 0.9472\n",
      "Epoch 99/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1711 - accuracy: 0.9403 - val_loss: 0.2504 - val_accuracy: 0.9472\n",
      "Epoch 100/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1708 - accuracy: 0.9403 - val_loss: 0.2498 - val_accuracy: 0.9472\n",
      "Epoch 101/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1704 - accuracy: 0.9403 - val_loss: 0.2492 - val_accuracy: 0.9472\n",
      "Epoch 102/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1705 - accuracy: 0.9372 - val_loss: 0.2489 - val_accuracy: 0.9472\n",
      "Epoch 103/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1705 - accuracy: 0.9372 - val_loss: 0.2488 - val_accuracy: 0.9472\n",
      "Epoch 104/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1704 - accuracy: 0.9387 - val_loss: 0.2487 - val_accuracy: 0.9472\n",
      "Epoch 105/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1698 - accuracy: 0.9387 - val_loss: 0.2485 - val_accuracy: 0.9472\n",
      "Epoch 106/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1693 - accuracy: 0.9403 - val_loss: 0.2484 - val_accuracy: 0.9472\n",
      "Epoch 107/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1687 - accuracy: 0.9418 - val_loss: 0.2487 - val_accuracy: 0.9472\n",
      "Epoch 108/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1684 - accuracy: 0.9464 - val_loss: 0.2492 - val_accuracy: 0.9472\n",
      "Epoch 109/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1685 - accuracy: 0.9449 - val_loss: 0.2498 - val_accuracy: 0.9441\n",
      "Epoch 110/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1688 - accuracy: 0.9495 - val_loss: 0.2496 - val_accuracy: 0.9441\n",
      "Epoch 111/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1684 - accuracy: 0.9495 - val_loss: 0.2484 - val_accuracy: 0.9472\n",
      "Epoch 112/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1677 - accuracy: 0.9449 - val_loss: 0.2473 - val_accuracy: 0.9472\n",
      "Epoch 113/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 15us/step - loss: 0.1672 - accuracy: 0.9449 - val_loss: 0.2465 - val_accuracy: 0.9472\n",
      "Epoch 114/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1671 - accuracy: 0.9418 - val_loss: 0.2461 - val_accuracy: 0.9472\n",
      "Epoch 115/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1668 - accuracy: 0.9403 - val_loss: 0.2461 - val_accuracy: 0.9472\n",
      "Epoch 116/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1664 - accuracy: 0.9418 - val_loss: 0.2463 - val_accuracy: 0.9472\n",
      "Epoch 117/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1662 - accuracy: 0.9464 - val_loss: 0.2465 - val_accuracy: 0.9472\n",
      "Epoch 118/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1661 - accuracy: 0.9464 - val_loss: 0.2466 - val_accuracy: 0.9472\n",
      "Epoch 119/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1659 - accuracy: 0.9464 - val_loss: 0.2473 - val_accuracy: 0.9472\n",
      "Epoch 120/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1661 - accuracy: 0.9495 - val_loss: 0.2479 - val_accuracy: 0.9441\n",
      "Epoch 121/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1662 - accuracy: 0.9495 - val_loss: 0.2478 - val_accuracy: 0.9472\n",
      "Epoch 122/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1659 - accuracy: 0.9495 - val_loss: 0.2468 - val_accuracy: 0.9472\n",
      "Epoch 123/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1652 - accuracy: 0.9464 - val_loss: 0.2454 - val_accuracy: 0.9472\n",
      "Epoch 124/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1644 - accuracy: 0.9464 - val_loss: 0.2443 - val_accuracy: 0.9472\n",
      "Epoch 125/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1645 - accuracy: 0.9418 - val_loss: 0.2436 - val_accuracy: 0.9472\n",
      "Epoch 126/3500\n",
      "653/653 [==============================] - 0s 46us/step - loss: 0.1645 - accuracy: 0.9387 - val_loss: 0.2434 - val_accuracy: 0.9472\n",
      "Epoch 127/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.1642 - accuracy: 0.9387 - val_loss: 0.2431 - val_accuracy: 0.9472\n",
      "Epoch 128/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.1639 - accuracy: 0.9387 - val_loss: 0.2430 - val_accuracy: 0.9472\n",
      "Epoch 129/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1632 - accuracy: 0.9403 - val_loss: 0.2431 - val_accuracy: 0.9472\n",
      "Epoch 130/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1630 - accuracy: 0.9403 - val_loss: 0.2431 - val_accuracy: 0.9472\n",
      "Epoch 131/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1628 - accuracy: 0.9418 - val_loss: 0.2430 - val_accuracy: 0.9472\n",
      "Epoch 132/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1626 - accuracy: 0.9418 - val_loss: 0.2431 - val_accuracy: 0.9472\n",
      "Epoch 133/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1622 - accuracy: 0.9418 - val_loss: 0.2425 - val_accuracy: 0.9472\n",
      "Epoch 134/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1620 - accuracy: 0.9418 - val_loss: 0.2417 - val_accuracy: 0.9472\n",
      "Epoch 135/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1618 - accuracy: 0.9418 - val_loss: 0.2410 - val_accuracy: 0.9472\n",
      "Epoch 136/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1616 - accuracy: 0.9418 - val_loss: 0.2405 - val_accuracy: 0.9472\n",
      "Epoch 137/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1613 - accuracy: 0.9418 - val_loss: 0.2402 - val_accuracy: 0.9472\n",
      "Epoch 138/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1612 - accuracy: 0.9449 - val_loss: 0.2402 - val_accuracy: 0.9472\n",
      "Epoch 139/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1608 - accuracy: 0.9449 - val_loss: 0.2404 - val_accuracy: 0.9472\n",
      "Epoch 140/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1606 - accuracy: 0.9464 - val_loss: 0.2408 - val_accuracy: 0.9472\n",
      "Epoch 141/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1603 - accuracy: 0.9449 - val_loss: 0.2404 - val_accuracy: 0.9472\n",
      "Epoch 142/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1600 - accuracy: 0.9449 - val_loss: 0.2399 - val_accuracy: 0.9472\n",
      "Epoch 143/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1598 - accuracy: 0.9433 - val_loss: 0.2392 - val_accuracy: 0.9472\n",
      "Epoch 144/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1598 - accuracy: 0.9479 - val_loss: 0.2385 - val_accuracy: 0.9472\n",
      "Epoch 145/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1599 - accuracy: 0.9449 - val_loss: 0.2378 - val_accuracy: 0.9472\n",
      "Epoch 146/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1598 - accuracy: 0.9449 - val_loss: 0.2371 - val_accuracy: 0.9472\n",
      "Epoch 147/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1591 - accuracy: 0.9464 - val_loss: 0.2368 - val_accuracy: 0.9472\n",
      "Epoch 148/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1586 - accuracy: 0.9433 - val_loss: 0.2374 - val_accuracy: 0.9472\n",
      "Epoch 149/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1587 - accuracy: 0.9464 - val_loss: 0.2380 - val_accuracy: 0.9472\n",
      "Epoch 150/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1588 - accuracy: 0.9495 - val_loss: 0.2373 - val_accuracy: 0.9472\n",
      "Epoch 151/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1581 - accuracy: 0.9510 - val_loss: 0.2355 - val_accuracy: 0.9472\n",
      "Epoch 152/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1578 - accuracy: 0.9495 - val_loss: 0.2343 - val_accuracy: 0.9472\n",
      "Epoch 153/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1577 - accuracy: 0.9464 - val_loss: 0.2340 - val_accuracy: 0.9472\n",
      "Epoch 154/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1581 - accuracy: 0.9449 - val_loss: 0.2338 - val_accuracy: 0.9472\n",
      "Epoch 155/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1578 - accuracy: 0.9449 - val_loss: 0.2335 - val_accuracy: 0.9472\n",
      "Epoch 156/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1569 - accuracy: 0.9464 - val_loss: 0.2339 - val_accuracy: 0.9472\n",
      "Epoch 157/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1565 - accuracy: 0.9464 - val_loss: 0.2348 - val_accuracy: 0.9472\n",
      "Epoch 158/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1563 - accuracy: 0.9495 - val_loss: 0.2352 - val_accuracy: 0.9472\n",
      "Epoch 159/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1562 - accuracy: 0.9510 - val_loss: 0.2352 - val_accuracy: 0.9472\n",
      "Epoch 160/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1560 - accuracy: 0.9510 - val_loss: 0.2349 - val_accuracy: 0.9472\n",
      "Epoch 161/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1559 - accuracy: 0.9510 - val_loss: 0.2344 - val_accuracy: 0.9472\n",
      "Epoch 162/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1556 - accuracy: 0.9525 - val_loss: 0.2341 - val_accuracy: 0.9472\n",
      "Epoch 163/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1554 - accuracy: 0.9525 - val_loss: 0.2339 - val_accuracy: 0.9472\n",
      "Epoch 164/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1550 - accuracy: 0.9525 - val_loss: 0.2337 - val_accuracy: 0.9472\n",
      "Epoch 165/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1547 - accuracy: 0.9525 - val_loss: 0.2334 - val_accuracy: 0.9472\n",
      "Epoch 166/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1545 - accuracy: 0.9525 - val_loss: 0.2333 - val_accuracy: 0.9472\n",
      "Epoch 167/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1541 - accuracy: 0.9525 - val_loss: 0.2334 - val_accuracy: 0.9472\n",
      "Epoch 168/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1539 - accuracy: 0.9510 - val_loss: 0.2333 - val_accuracy: 0.9472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1538 - accuracy: 0.9479 - val_loss: 0.2328 - val_accuracy: 0.9472\n",
      "Epoch 170/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1535 - accuracy: 0.9510 - val_loss: 0.2319 - val_accuracy: 0.9472\n",
      "Epoch 171/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1532 - accuracy: 0.9525 - val_loss: 0.2309 - val_accuracy: 0.9472\n",
      "Epoch 172/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1533 - accuracy: 0.9464 - val_loss: 0.2301 - val_accuracy: 0.9472\n",
      "Epoch 173/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1535 - accuracy: 0.9464 - val_loss: 0.2297 - val_accuracy: 0.9472\n",
      "Epoch 174/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1527 - accuracy: 0.9464 - val_loss: 0.2303 - val_accuracy: 0.9472\n",
      "Epoch 175/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1520 - accuracy: 0.9510 - val_loss: 0.2320 - val_accuracy: 0.9441\n",
      "Epoch 176/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1524 - accuracy: 0.9525 - val_loss: 0.2330 - val_accuracy: 0.9441\n",
      "Epoch 177/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1530 - accuracy: 0.9525 - val_loss: 0.2315 - val_accuracy: 0.9441\n",
      "Epoch 178/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1518 - accuracy: 0.9541 - val_loss: 0.2282 - val_accuracy: 0.9472\n",
      "Epoch 179/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1508 - accuracy: 0.9525 - val_loss: 0.2260 - val_accuracy: 0.9472\n",
      "Epoch 180/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1520 - accuracy: 0.9449 - val_loss: 0.2255 - val_accuracy: 0.9472\n",
      "Epoch 181/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1531 - accuracy: 0.9464 - val_loss: 0.2251 - val_accuracy: 0.9472\n",
      "Epoch 182/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1528 - accuracy: 0.9449 - val_loss: 0.2246 - val_accuracy: 0.9472\n",
      "Epoch 183/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1516 - accuracy: 0.9449 - val_loss: 0.2247 - val_accuracy: 0.9472\n",
      "Epoch 184/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1500 - accuracy: 0.9510 - val_loss: 0.2264 - val_accuracy: 0.9441\n",
      "Epoch 185/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1501 - accuracy: 0.9556 - val_loss: 0.2286 - val_accuracy: 0.9441\n",
      "Epoch 186/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1508 - accuracy: 0.9556 - val_loss: 0.2278 - val_accuracy: 0.9441\n",
      "Epoch 187/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1502 - accuracy: 0.9541 - val_loss: 0.2254 - val_accuracy: 0.9441\n",
      "Epoch 188/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1493 - accuracy: 0.9556 - val_loss: 0.2245 - val_accuracy: 0.9472\n",
      "Epoch 189/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1490 - accuracy: 0.9510 - val_loss: 0.2246 - val_accuracy: 0.9441\n",
      "Epoch 190/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1488 - accuracy: 0.9556 - val_loss: 0.2251 - val_accuracy: 0.9441\n",
      "Epoch 191/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1484 - accuracy: 0.9587 - val_loss: 0.2248 - val_accuracy: 0.9441\n",
      "Epoch 192/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1481 - accuracy: 0.9587 - val_loss: 0.2245 - val_accuracy: 0.9441\n",
      "Epoch 193/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.1479 - accuracy: 0.9587 - val_loss: 0.2244 - val_accuracy: 0.9441\n",
      "Epoch 194/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.1478 - accuracy: 0.9587 - val_loss: 0.2242 - val_accuracy: 0.9441\n",
      "Epoch 195/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1474 - accuracy: 0.9587 - val_loss: 0.2230 - val_accuracy: 0.9441\n",
      "Epoch 196/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1471 - accuracy: 0.9525 - val_loss: 0.2219 - val_accuracy: 0.9472\n",
      "Epoch 197/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1474 - accuracy: 0.9510 - val_loss: 0.2213 - val_accuracy: 0.9472\n",
      "Epoch 198/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1472 - accuracy: 0.9495 - val_loss: 0.2213 - val_accuracy: 0.9472\n",
      "Epoch 199/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1466 - accuracy: 0.9510 - val_loss: 0.2217 - val_accuracy: 0.9441\n",
      "Epoch 200/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1463 - accuracy: 0.9556 - val_loss: 0.2225 - val_accuracy: 0.9472\n",
      "Epoch 201/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1462 - accuracy: 0.9587 - val_loss: 0.2219 - val_accuracy: 0.9472\n",
      "Epoch 202/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1458 - accuracy: 0.9587 - val_loss: 0.2203 - val_accuracy: 0.9441\n",
      "Epoch 203/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1456 - accuracy: 0.9571 - val_loss: 0.2188 - val_accuracy: 0.9441\n",
      "Epoch 204/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1458 - accuracy: 0.9510 - val_loss: 0.2183 - val_accuracy: 0.9441\n",
      "Epoch 205/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1452 - accuracy: 0.9525 - val_loss: 0.2192 - val_accuracy: 0.9472\n",
      "Epoch 206/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1450 - accuracy: 0.9587 - val_loss: 0.2200 - val_accuracy: 0.9472\n",
      "Epoch 207/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1448 - accuracy: 0.9587 - val_loss: 0.2192 - val_accuracy: 0.9472\n",
      "Epoch 208/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1445 - accuracy: 0.9587 - val_loss: 0.2177 - val_accuracy: 0.9472\n",
      "Epoch 209/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1441 - accuracy: 0.9571 - val_loss: 0.2163 - val_accuracy: 0.9472\n",
      "Epoch 210/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1439 - accuracy: 0.9571 - val_loss: 0.2145 - val_accuracy: 0.9472\n",
      "Epoch 211/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1440 - accuracy: 0.9541 - val_loss: 0.2131 - val_accuracy: 0.9503\n",
      "Epoch 212/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1441 - accuracy: 0.9541 - val_loss: 0.2122 - val_accuracy: 0.9503\n",
      "Epoch 213/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1440 - accuracy: 0.9556 - val_loss: 0.2118 - val_accuracy: 0.9503\n",
      "Epoch 214/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1437 - accuracy: 0.9556 - val_loss: 0.2123 - val_accuracy: 0.9503\n",
      "Epoch 215/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1431 - accuracy: 0.9587 - val_loss: 0.2139 - val_accuracy: 0.9472\n",
      "Epoch 216/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1429 - accuracy: 0.9587 - val_loss: 0.2156 - val_accuracy: 0.9472\n",
      "Epoch 217/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1428 - accuracy: 0.9602 - val_loss: 0.2157 - val_accuracy: 0.9472\n",
      "Epoch 218/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1424 - accuracy: 0.9602 - val_loss: 0.2155 - val_accuracy: 0.9472\n",
      "Epoch 219/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1420 - accuracy: 0.9602 - val_loss: 0.2145 - val_accuracy: 0.9472\n",
      "Epoch 220/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1416 - accuracy: 0.9571 - val_loss: 0.2136 - val_accuracy: 0.9472\n",
      "Epoch 221/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1416 - accuracy: 0.9571 - val_loss: 0.2130 - val_accuracy: 0.9472\n",
      "Epoch 222/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1415 - accuracy: 0.9556 - val_loss: 0.2125 - val_accuracy: 0.9503\n",
      "Epoch 223/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1411 - accuracy: 0.9556 - val_loss: 0.2129 - val_accuracy: 0.9472\n",
      "Epoch 224/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1408 - accuracy: 0.9571 - val_loss: 0.2138 - val_accuracy: 0.9472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1406 - accuracy: 0.9602 - val_loss: 0.2138 - val_accuracy: 0.9472\n",
      "Epoch 226/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1405 - accuracy: 0.9602 - val_loss: 0.2129 - val_accuracy: 0.9472\n",
      "Epoch 227/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1403 - accuracy: 0.9602 - val_loss: 0.2114 - val_accuracy: 0.9534\n",
      "Epoch 228/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1399 - accuracy: 0.9587 - val_loss: 0.2108 - val_accuracy: 0.9534\n",
      "Epoch 229/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1399 - accuracy: 0.9571 - val_loss: 0.2112 - val_accuracy: 0.9534\n",
      "Epoch 230/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1394 - accuracy: 0.9602 - val_loss: 0.2132 - val_accuracy: 0.9472\n",
      "Epoch 231/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1396 - accuracy: 0.9602 - val_loss: 0.2137 - val_accuracy: 0.9472\n",
      "Epoch 232/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1397 - accuracy: 0.9602 - val_loss: 0.2127 - val_accuracy: 0.9472\n",
      "Epoch 233/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1390 - accuracy: 0.9602 - val_loss: 0.2125 - val_accuracy: 0.9472\n",
      "Epoch 234/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1388 - accuracy: 0.9602 - val_loss: 0.2123 - val_accuracy: 0.9472\n",
      "Epoch 235/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1385 - accuracy: 0.9602 - val_loss: 0.2112 - val_accuracy: 0.9534\n",
      "Epoch 236/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1382 - accuracy: 0.9602 - val_loss: 0.2093 - val_accuracy: 0.9534\n",
      "Epoch 237/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1378 - accuracy: 0.9587 - val_loss: 0.2080 - val_accuracy: 0.9534\n",
      "Epoch 238/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1377 - accuracy: 0.9571 - val_loss: 0.2075 - val_accuracy: 0.9534\n",
      "Epoch 239/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1375 - accuracy: 0.9587 - val_loss: 0.2077 - val_accuracy: 0.9534\n",
      "Epoch 240/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1373 - accuracy: 0.9602 - val_loss: 0.2077 - val_accuracy: 0.9534\n",
      "Epoch 241/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1371 - accuracy: 0.9587 - val_loss: 0.2075 - val_accuracy: 0.9534\n",
      "Epoch 242/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1368 - accuracy: 0.9587 - val_loss: 0.2081 - val_accuracy: 0.9534\n",
      "Epoch 243/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1368 - accuracy: 0.9602 - val_loss: 0.2084 - val_accuracy: 0.9534\n",
      "Epoch 244/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1367 - accuracy: 0.9602 - val_loss: 0.2076 - val_accuracy: 0.9534\n",
      "Epoch 245/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1362 - accuracy: 0.9587 - val_loss: 0.2072 - val_accuracy: 0.9534\n",
      "Epoch 246/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1360 - accuracy: 0.9602 - val_loss: 0.2058 - val_accuracy: 0.9534\n",
      "Epoch 247/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1359 - accuracy: 0.9602 - val_loss: 0.2050 - val_accuracy: 0.9534\n",
      "Epoch 248/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1357 - accuracy: 0.9602 - val_loss: 0.2043 - val_accuracy: 0.9534\n",
      "Epoch 249/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1354 - accuracy: 0.9602 - val_loss: 0.2044 - val_accuracy: 0.9534\n",
      "Epoch 250/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1351 - accuracy: 0.9587 - val_loss: 0.2050 - val_accuracy: 0.9534\n",
      "Epoch 251/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1347 - accuracy: 0.9587 - val_loss: 0.2059 - val_accuracy: 0.9534\n",
      "Epoch 252/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1347 - accuracy: 0.9602 - val_loss: 0.2058 - val_accuracy: 0.9534\n",
      "Epoch 253/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1345 - accuracy: 0.9602 - val_loss: 0.2044 - val_accuracy: 0.9534\n",
      "Epoch 254/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1341 - accuracy: 0.9587 - val_loss: 0.2021 - val_accuracy: 0.9534\n",
      "Epoch 255/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1338 - accuracy: 0.9587 - val_loss: 0.2000 - val_accuracy: 0.9534\n",
      "Epoch 256/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1337 - accuracy: 0.9587 - val_loss: 0.1983 - val_accuracy: 0.9534\n",
      "Epoch 257/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1338 - accuracy: 0.9602 - val_loss: 0.1976 - val_accuracy: 0.9534\n",
      "Epoch 258/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1341 - accuracy: 0.9587 - val_loss: 0.1980 - val_accuracy: 0.9534\n",
      "Epoch 259/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1334 - accuracy: 0.9571 - val_loss: 0.2001 - val_accuracy: 0.9534\n",
      "Epoch 260/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1325 - accuracy: 0.9587 - val_loss: 0.2014 - val_accuracy: 0.9534\n",
      "Epoch 261/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1323 - accuracy: 0.9587 - val_loss: 0.2019 - val_accuracy: 0.9534\n",
      "Epoch 262/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1322 - accuracy: 0.9587 - val_loss: 0.2019 - val_accuracy: 0.9534\n",
      "Epoch 263/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1321 - accuracy: 0.9602 - val_loss: 0.1996 - val_accuracy: 0.9534\n",
      "Epoch 264/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1317 - accuracy: 0.9587 - val_loss: 0.1960 - val_accuracy: 0.9534\n",
      "Epoch 265/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1315 - accuracy: 0.9602 - val_loss: 0.1955 - val_accuracy: 0.9534\n",
      "Epoch 266/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1310 - accuracy: 0.9617 - val_loss: 0.1967 - val_accuracy: 0.9534\n",
      "Epoch 267/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1303 - accuracy: 0.9602 - val_loss: 0.1992 - val_accuracy: 0.9534\n",
      "Epoch 268/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1304 - accuracy: 0.9587 - val_loss: 0.2019 - val_accuracy: 0.9534\n",
      "Epoch 269/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1306 - accuracy: 0.9587 - val_loss: 0.2004 - val_accuracy: 0.9534\n",
      "Epoch 270/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1300 - accuracy: 0.9571 - val_loss: 0.1968 - val_accuracy: 0.9534\n",
      "Epoch 271/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1295 - accuracy: 0.9587 - val_loss: 0.1936 - val_accuracy: 0.9534\n",
      "Epoch 272/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1288 - accuracy: 0.9617 - val_loss: 0.1923 - val_accuracy: 0.9534\n",
      "Epoch 273/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1286 - accuracy: 0.9617 - val_loss: 0.1912 - val_accuracy: 0.9534\n",
      "Epoch 274/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1285 - accuracy: 0.9617 - val_loss: 0.1907 - val_accuracy: 0.9534\n",
      "Epoch 275/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1282 - accuracy: 0.9617 - val_loss: 0.1896 - val_accuracy: 0.9534\n",
      "Epoch 276/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1279 - accuracy: 0.9617 - val_loss: 0.1882 - val_accuracy: 0.9534\n",
      "Epoch 277/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1279 - accuracy: 0.9648 - val_loss: 0.1874 - val_accuracy: 0.9534\n",
      "Epoch 278/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1278 - accuracy: 0.9648 - val_loss: 0.1873 - val_accuracy: 0.9534\n",
      "Epoch 279/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1273 - accuracy: 0.9648 - val_loss: 0.1892 - val_accuracy: 0.9534\n",
      "Epoch 280/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1277 - accuracy: 0.9602 - val_loss: 0.1915 - val_accuracy: 0.9534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1263 - accuracy: 0.9617 - val_loss: 0.1895 - val_accuracy: 0.9534\n",
      "Epoch 282/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1262 - accuracy: 0.9617 - val_loss: 0.1886 - val_accuracy: 0.9534\n",
      "Epoch 283/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.1259 - accuracy: 0.9617 - val_loss: 0.1887 - val_accuracy: 0.9534\n",
      "Epoch 284/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1256 - accuracy: 0.9617 - val_loss: 0.1891 - val_accuracy: 0.9534\n",
      "Epoch 285/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1253 - accuracy: 0.9617 - val_loss: 0.1905 - val_accuracy: 0.9534\n",
      "Epoch 286/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1252 - accuracy: 0.9602 - val_loss: 0.1919 - val_accuracy: 0.9534\n",
      "Epoch 287/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1252 - accuracy: 0.9587 - val_loss: 0.1897 - val_accuracy: 0.9534\n",
      "Epoch 288/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1244 - accuracy: 0.9617 - val_loss: 0.1845 - val_accuracy: 0.9534\n",
      "Epoch 289/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1253 - accuracy: 0.9648 - val_loss: 0.1812 - val_accuracy: 0.9534\n",
      "Epoch 290/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1257 - accuracy: 0.9648 - val_loss: 0.1819 - val_accuracy: 0.9534\n",
      "Epoch 291/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1245 - accuracy: 0.9648 - val_loss: 0.1850 - val_accuracy: 0.9534\n",
      "Epoch 292/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1234 - accuracy: 0.9632 - val_loss: 0.1876 - val_accuracy: 0.9534\n",
      "Epoch 293/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1233 - accuracy: 0.9617 - val_loss: 0.1878 - val_accuracy: 0.9565\n",
      "Epoch 294/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1232 - accuracy: 0.9617 - val_loss: 0.1871 - val_accuracy: 0.9565\n",
      "Epoch 295/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1228 - accuracy: 0.9617 - val_loss: 0.1864 - val_accuracy: 0.9565\n",
      "Epoch 296/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1225 - accuracy: 0.9617 - val_loss: 0.1837 - val_accuracy: 0.9534\n",
      "Epoch 297/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1221 - accuracy: 0.9617 - val_loss: 0.1804 - val_accuracy: 0.9534\n",
      "Epoch 298/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1216 - accuracy: 0.9648 - val_loss: 0.1793 - val_accuracy: 0.9534\n",
      "Epoch 299/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1216 - accuracy: 0.9648 - val_loss: 0.1798 - val_accuracy: 0.9534\n",
      "Epoch 300/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1210 - accuracy: 0.9632 - val_loss: 0.1833 - val_accuracy: 0.9565\n",
      "Epoch 301/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1214 - accuracy: 0.9617 - val_loss: 0.1835 - val_accuracy: 0.9565\n",
      "Epoch 302/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1209 - accuracy: 0.9617 - val_loss: 0.1777 - val_accuracy: 0.9534\n",
      "Epoch 303/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1197 - accuracy: 0.9648 - val_loss: 0.1729 - val_accuracy: 0.9534\n",
      "Epoch 304/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1231 - accuracy: 0.9648 - val_loss: 0.1726 - val_accuracy: 0.9534\n",
      "Epoch 305/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1244 - accuracy: 0.9617 - val_loss: 0.1735 - val_accuracy: 0.9534\n",
      "Epoch 306/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1205 - accuracy: 0.9663 - val_loss: 0.1799 - val_accuracy: 0.9565\n",
      "Epoch 307/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1198 - accuracy: 0.9602 - val_loss: 0.1860 - val_accuracy: 0.9565\n",
      "Epoch 308/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1206 - accuracy: 0.9617 - val_loss: 0.1824 - val_accuracy: 0.9565\n",
      "Epoch 309/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1191 - accuracy: 0.9632 - val_loss: 0.1753 - val_accuracy: 0.9534\n",
      "Epoch 310/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1183 - accuracy: 0.9648 - val_loss: 0.1719 - val_accuracy: 0.9534\n",
      "Epoch 311/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1198 - accuracy: 0.9663 - val_loss: 0.1713 - val_accuracy: 0.9534\n",
      "Epoch 312/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1187 - accuracy: 0.9663 - val_loss: 0.1750 - val_accuracy: 0.9565\n",
      "Epoch 313/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1175 - accuracy: 0.9632 - val_loss: 0.1809 - val_accuracy: 0.9565\n",
      "Epoch 314/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1183 - accuracy: 0.9632 - val_loss: 0.1812 - val_accuracy: 0.9565\n",
      "Epoch 315/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1182 - accuracy: 0.9632 - val_loss: 0.1759 - val_accuracy: 0.9565\n",
      "Epoch 316/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1170 - accuracy: 0.9617 - val_loss: 0.1708 - val_accuracy: 0.9565\n",
      "Epoch 317/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1165 - accuracy: 0.9632 - val_loss: 0.1679 - val_accuracy: 0.9534\n",
      "Epoch 318/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1170 - accuracy: 0.9648 - val_loss: 0.1664 - val_accuracy: 0.9534\n",
      "Epoch 319/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1176 - accuracy: 0.9632 - val_loss: 0.1671 - val_accuracy: 0.9534\n",
      "Epoch 320/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1157 - accuracy: 0.9648 - val_loss: 0.1740 - val_accuracy: 0.9565\n",
      "Epoch 321/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1167 - accuracy: 0.9617 - val_loss: 0.1823 - val_accuracy: 0.9565\n",
      "Epoch 322/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1177 - accuracy: 0.9617 - val_loss: 0.1777 - val_accuracy: 0.9565\n",
      "Epoch 323/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1156 - accuracy: 0.9632 - val_loss: 0.1696 - val_accuracy: 0.9565\n",
      "Epoch 324/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1156 - accuracy: 0.9663 - val_loss: 0.1669 - val_accuracy: 0.9534\n",
      "Epoch 325/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1159 - accuracy: 0.9648 - val_loss: 0.1693 - val_accuracy: 0.9565\n",
      "Epoch 326/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1141 - accuracy: 0.9663 - val_loss: 0.1765 - val_accuracy: 0.9565\n",
      "Epoch 327/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1156 - accuracy: 0.9632 - val_loss: 0.1793 - val_accuracy: 0.9565\n",
      "Epoch 328/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1157 - accuracy: 0.9632 - val_loss: 0.1721 - val_accuracy: 0.9565\n",
      "Epoch 329/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1133 - accuracy: 0.9632 - val_loss: 0.1653 - val_accuracy: 0.9565\n",
      "Epoch 330/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1139 - accuracy: 0.9663 - val_loss: 0.1618 - val_accuracy: 0.9565\n",
      "Epoch 331/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1143 - accuracy: 0.9617 - val_loss: 0.1634 - val_accuracy: 0.9565\n",
      "Epoch 332/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1139 - accuracy: 0.9663 - val_loss: 0.1681 - val_accuracy: 0.9565\n",
      "Epoch 333/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1128 - accuracy: 0.9632 - val_loss: 0.1671 - val_accuracy: 0.9565\n",
      "Epoch 334/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1123 - accuracy: 0.9632 - val_loss: 0.1633 - val_accuracy: 0.9565\n",
      "Epoch 335/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1119 - accuracy: 0.9663 - val_loss: 0.1605 - val_accuracy: 0.9565\n",
      "Epoch 336/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1124 - accuracy: 0.9632 - val_loss: 0.1607 - val_accuracy: 0.9565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 337/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1123 - accuracy: 0.9632 - val_loss: 0.1641 - val_accuracy: 0.9565\n",
      "Epoch 338/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1110 - accuracy: 0.9648 - val_loss: 0.1717 - val_accuracy: 0.9565\n",
      "Epoch 339/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1123 - accuracy: 0.9632 - val_loss: 0.1736 - val_accuracy: 0.9565\n",
      "Epoch 340/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.1122 - accuracy: 0.9632 - val_loss: 0.1682 - val_accuracy: 0.9565\n",
      "Epoch 341/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1107 - accuracy: 0.9648 - val_loss: 0.1633 - val_accuracy: 0.9565\n",
      "Epoch 342/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1110 - accuracy: 0.9663 - val_loss: 0.1616 - val_accuracy: 0.9565\n",
      "Epoch 343/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1106 - accuracy: 0.9648 - val_loss: 0.1644 - val_accuracy: 0.9565\n",
      "Epoch 344/3500\n",
      "653/653 [==============================] - 0s 40us/step - loss: 0.1101 - accuracy: 0.9663 - val_loss: 0.1675 - val_accuracy: 0.9565\n",
      "Epoch 345/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.1100 - accuracy: 0.9632 - val_loss: 0.1675 - val_accuracy: 0.9565\n",
      "Epoch 346/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1098 - accuracy: 0.9632 - val_loss: 0.1653 - val_accuracy: 0.9565\n",
      "Epoch 347/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1097 - accuracy: 0.9648 - val_loss: 0.1621 - val_accuracy: 0.9565\n",
      "Epoch 348/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1090 - accuracy: 0.9663 - val_loss: 0.1631 - val_accuracy: 0.9565\n",
      "Epoch 349/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1089 - accuracy: 0.9632 - val_loss: 0.1649 - val_accuracy: 0.9565\n",
      "Epoch 350/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1091 - accuracy: 0.9632 - val_loss: 0.1630 - val_accuracy: 0.9565\n",
      "Epoch 351/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1087 - accuracy: 0.9648 - val_loss: 0.1606 - val_accuracy: 0.9565\n",
      "Epoch 352/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1082 - accuracy: 0.9663 - val_loss: 0.1596 - val_accuracy: 0.9565\n",
      "Epoch 353/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1077 - accuracy: 0.9663 - val_loss: 0.1590 - val_accuracy: 0.9565\n",
      "Epoch 354/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1075 - accuracy: 0.9648 - val_loss: 0.1588 - val_accuracy: 0.9565\n",
      "Epoch 355/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1070 - accuracy: 0.9663 - val_loss: 0.1596 - val_accuracy: 0.9565\n",
      "Epoch 356/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1069 - accuracy: 0.9663 - val_loss: 0.1602 - val_accuracy: 0.9565\n",
      "Epoch 357/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1066 - accuracy: 0.9663 - val_loss: 0.1625 - val_accuracy: 0.9565\n",
      "Epoch 358/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1065 - accuracy: 0.9663 - val_loss: 0.1611 - val_accuracy: 0.9565\n",
      "Epoch 359/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1059 - accuracy: 0.9663 - val_loss: 0.1563 - val_accuracy: 0.9565\n",
      "Epoch 360/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.1059 - accuracy: 0.9663 - val_loss: 0.1528 - val_accuracy: 0.9596\n",
      "Epoch 361/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1069 - accuracy: 0.9663 - val_loss: 0.1528 - val_accuracy: 0.9596\n",
      "Epoch 362/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1061 - accuracy: 0.9663 - val_loss: 0.1568 - val_accuracy: 0.9565\n",
      "Epoch 363/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1049 - accuracy: 0.9632 - val_loss: 0.1614 - val_accuracy: 0.9565\n",
      "Epoch 364/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1053 - accuracy: 0.9663 - val_loss: 0.1628 - val_accuracy: 0.9565\n",
      "Epoch 365/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1053 - accuracy: 0.9663 - val_loss: 0.1600 - val_accuracy: 0.9565\n",
      "Epoch 366/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1050 - accuracy: 0.9663 - val_loss: 0.1578 - val_accuracy: 0.9565\n",
      "Epoch 367/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.1044 - accuracy: 0.9648 - val_loss: 0.1583 - val_accuracy: 0.9565\n",
      "Epoch 368/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1043 - accuracy: 0.9648 - val_loss: 0.1583 - val_accuracy: 0.9565\n",
      "Epoch 369/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1041 - accuracy: 0.9648 - val_loss: 0.1571 - val_accuracy: 0.9565\n",
      "Epoch 370/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.1035 - accuracy: 0.9663 - val_loss: 0.1540 - val_accuracy: 0.9596\n",
      "Epoch 371/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1034 - accuracy: 0.9663 - val_loss: 0.1526 - val_accuracy: 0.9596\n",
      "Epoch 372/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.1031 - accuracy: 0.9663 - val_loss: 0.1534 - val_accuracy: 0.9565\n",
      "Epoch 373/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1026 - accuracy: 0.9663 - val_loss: 0.1527 - val_accuracy: 0.9565\n",
      "Epoch 374/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1026 - accuracy: 0.9663 - val_loss: 0.1515 - val_accuracy: 0.9565\n",
      "Epoch 375/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1022 - accuracy: 0.9663 - val_loss: 0.1482 - val_accuracy: 0.9596\n",
      "Epoch 376/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1026 - accuracy: 0.9648 - val_loss: 0.1452 - val_accuracy: 0.9596\n",
      "Epoch 377/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1035 - accuracy: 0.9648 - val_loss: 0.1440 - val_accuracy: 0.9596\n",
      "Epoch 378/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1035 - accuracy: 0.9663 - val_loss: 0.1473 - val_accuracy: 0.9596\n",
      "Epoch 379/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1017 - accuracy: 0.9648 - val_loss: 0.1499 - val_accuracy: 0.9565\n",
      "Epoch 380/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1012 - accuracy: 0.9648 - val_loss: 0.1506 - val_accuracy: 0.9565\n",
      "Epoch 381/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1012 - accuracy: 0.9648 - val_loss: 0.1515 - val_accuracy: 0.9565\n",
      "Epoch 382/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1008 - accuracy: 0.9663 - val_loss: 0.1527 - val_accuracy: 0.9565\n",
      "Epoch 383/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.1003 - accuracy: 0.9663 - val_loss: 0.1492 - val_accuracy: 0.9596\n",
      "Epoch 384/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1003 - accuracy: 0.9678 - val_loss: 0.1475 - val_accuracy: 0.9596\n",
      "Epoch 385/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1006 - accuracy: 0.9678 - val_loss: 0.1501 - val_accuracy: 0.9596\n",
      "Epoch 386/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.1001 - accuracy: 0.9678 - val_loss: 0.1539 - val_accuracy: 0.9596\n",
      "Epoch 387/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1001 - accuracy: 0.9663 - val_loss: 0.1551 - val_accuracy: 0.9596\n",
      "Epoch 388/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.1001 - accuracy: 0.9678 - val_loss: 0.1526 - val_accuracy: 0.9596\n",
      "Epoch 389/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0997 - accuracy: 0.9678 - val_loss: 0.1496 - val_accuracy: 0.9596\n",
      "Epoch 390/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0995 - accuracy: 0.9678 - val_loss: 0.1486 - val_accuracy: 0.9596\n",
      "Epoch 391/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0989 - accuracy: 0.9678 - val_loss: 0.1516 - val_accuracy: 0.9596\n",
      "Epoch 392/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0989 - accuracy: 0.9678 - val_loss: 0.1530 - val_accuracy: 0.9596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0987 - accuracy: 0.9663 - val_loss: 0.1469 - val_accuracy: 0.9596\n",
      "Epoch 394/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0980 - accuracy: 0.9678 - val_loss: 0.1418 - val_accuracy: 0.9596\n",
      "Epoch 395/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0979 - accuracy: 0.9678 - val_loss: 0.1401 - val_accuracy: 0.9627\n",
      "Epoch 396/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0978 - accuracy: 0.9678 - val_loss: 0.1397 - val_accuracy: 0.9627\n",
      "Epoch 397/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0975 - accuracy: 0.9678 - val_loss: 0.1427 - val_accuracy: 0.9596\n",
      "Epoch 398/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0972 - accuracy: 0.9678 - val_loss: 0.1465 - val_accuracy: 0.9565\n",
      "Epoch 399/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0977 - accuracy: 0.9678 - val_loss: 0.1460 - val_accuracy: 0.9565\n",
      "Epoch 400/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0971 - accuracy: 0.9663 - val_loss: 0.1403 - val_accuracy: 0.9627\n",
      "Epoch 401/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0964 - accuracy: 0.9678 - val_loss: 0.1362 - val_accuracy: 0.9658\n",
      "Epoch 402/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0976 - accuracy: 0.9663 - val_loss: 0.1366 - val_accuracy: 0.9658\n",
      "Epoch 403/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0970 - accuracy: 0.9663 - val_loss: 0.1409 - val_accuracy: 0.9596\n",
      "Epoch 404/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0954 - accuracy: 0.9678 - val_loss: 0.1479 - val_accuracy: 0.9596\n",
      "Epoch 405/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0963 - accuracy: 0.9678 - val_loss: 0.1517 - val_accuracy: 0.9565\n",
      "Epoch 406/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0968 - accuracy: 0.9678 - val_loss: 0.1459 - val_accuracy: 0.9596\n",
      "Epoch 407/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0955 - accuracy: 0.9678 - val_loss: 0.1397 - val_accuracy: 0.9658\n",
      "Epoch 408/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0955 - accuracy: 0.9678 - val_loss: 0.1400 - val_accuracy: 0.9658\n",
      "Epoch 409/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0947 - accuracy: 0.9694 - val_loss: 0.1455 - val_accuracy: 0.9596\n",
      "Epoch 410/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0950 - accuracy: 0.9678 - val_loss: 0.1474 - val_accuracy: 0.9565\n",
      "Epoch 411/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0950 - accuracy: 0.9694 - val_loss: 0.1399 - val_accuracy: 0.9627\n",
      "Epoch 412/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0937 - accuracy: 0.9678 - val_loss: 0.1337 - val_accuracy: 0.9720\n",
      "Epoch 413/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0964 - accuracy: 0.9663 - val_loss: 0.1335 - val_accuracy: 0.9720\n",
      "Epoch 414/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0962 - accuracy: 0.9678 - val_loss: 0.1378 - val_accuracy: 0.9658\n",
      "Epoch 415/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0935 - accuracy: 0.9678 - val_loss: 0.1408 - val_accuracy: 0.9596\n",
      "Epoch 416/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0934 - accuracy: 0.9678 - val_loss: 0.1413 - val_accuracy: 0.9596\n",
      "Epoch 417/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0937 - accuracy: 0.9694 - val_loss: 0.1373 - val_accuracy: 0.9658\n",
      "Epoch 418/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0938 - accuracy: 0.9678 - val_loss: 0.1335 - val_accuracy: 0.9720\n",
      "Epoch 419/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0935 - accuracy: 0.9663 - val_loss: 0.1388 - val_accuracy: 0.9658\n",
      "Epoch 420/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0926 - accuracy: 0.9694 - val_loss: 0.1466 - val_accuracy: 0.9596\n",
      "Epoch 421/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0939 - accuracy: 0.9709 - val_loss: 0.1454 - val_accuracy: 0.9596\n",
      "Epoch 422/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0933 - accuracy: 0.9724 - val_loss: 0.1389 - val_accuracy: 0.9658\n",
      "Epoch 423/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0917 - accuracy: 0.9724 - val_loss: 0.1330 - val_accuracy: 0.9720\n",
      "Epoch 424/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0929 - accuracy: 0.9678 - val_loss: 0.1317 - val_accuracy: 0.9720\n",
      "Epoch 425/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0938 - accuracy: 0.9663 - val_loss: 0.1341 - val_accuracy: 0.9658\n",
      "Epoch 426/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0916 - accuracy: 0.9678 - val_loss: 0.1387 - val_accuracy: 0.9627\n",
      "Epoch 427/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0916 - accuracy: 0.9724 - val_loss: 0.1432 - val_accuracy: 0.9596\n",
      "Epoch 428/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0920 - accuracy: 0.9724 - val_loss: 0.1395 - val_accuracy: 0.9658\n",
      "Epoch 429/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0912 - accuracy: 0.9709 - val_loss: 0.1340 - val_accuracy: 0.9658\n",
      "Epoch 430/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0912 - accuracy: 0.9678 - val_loss: 0.1330 - val_accuracy: 0.9689\n",
      "Epoch 431/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0911 - accuracy: 0.9678 - val_loss: 0.1363 - val_accuracy: 0.9689\n",
      "Epoch 432/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0902 - accuracy: 0.9694 - val_loss: 0.1427 - val_accuracy: 0.9596\n",
      "Epoch 433/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0914 - accuracy: 0.9740 - val_loss: 0.1441 - val_accuracy: 0.9596\n",
      "Epoch 434/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0915 - accuracy: 0.9724 - val_loss: 0.1386 - val_accuracy: 0.9658\n",
      "Epoch 435/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0898 - accuracy: 0.9724 - val_loss: 0.1312 - val_accuracy: 0.9752\n",
      "Epoch 436/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0908 - accuracy: 0.9694 - val_loss: 0.1298 - val_accuracy: 0.9752\n",
      "Epoch 437/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0927 - accuracy: 0.9678 - val_loss: 0.1308 - val_accuracy: 0.9752\n",
      "Epoch 438/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0911 - accuracy: 0.9694 - val_loss: 0.1354 - val_accuracy: 0.9689\n",
      "Epoch 439/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0892 - accuracy: 0.9724 - val_loss: 0.1425 - val_accuracy: 0.9596\n",
      "Epoch 440/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0907 - accuracy: 0.9724 - val_loss: 0.1433 - val_accuracy: 0.9596\n",
      "Epoch 441/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0903 - accuracy: 0.9724 - val_loss: 0.1340 - val_accuracy: 0.9689\n",
      "Epoch 442/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0892 - accuracy: 0.9709 - val_loss: 0.1308 - val_accuracy: 0.9752\n",
      "Epoch 443/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0889 - accuracy: 0.9694 - val_loss: 0.1344 - val_accuracy: 0.9658\n",
      "Epoch 444/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0882 - accuracy: 0.9740 - val_loss: 0.1409 - val_accuracy: 0.9596\n",
      "Epoch 445/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0895 - accuracy: 0.9740 - val_loss: 0.1439 - val_accuracy: 0.9596\n",
      "Epoch 446/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0898 - accuracy: 0.9724 - val_loss: 0.1333 - val_accuracy: 0.9689\n",
      "Epoch 447/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0890 - accuracy: 0.9724 - val_loss: 0.1269 - val_accuracy: 0.9752\n",
      "Epoch 448/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0910 - accuracy: 0.9694 - val_loss: 0.1305 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0907 - accuracy: 0.9694 - val_loss: 0.1384 - val_accuracy: 0.9658\n",
      "Epoch 450/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0880 - accuracy: 0.9740 - val_loss: 0.1336 - val_accuracy: 0.9658\n",
      "Epoch 451/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0872 - accuracy: 0.9724 - val_loss: 0.1307 - val_accuracy: 0.9752\n",
      "Epoch 452/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0875 - accuracy: 0.9694 - val_loss: 0.1312 - val_accuracy: 0.9752\n",
      "Epoch 453/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0873 - accuracy: 0.9709 - val_loss: 0.1351 - val_accuracy: 0.9658\n",
      "Epoch 454/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0877 - accuracy: 0.9740 - val_loss: 0.1332 - val_accuracy: 0.9658\n",
      "Epoch 455/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0873 - accuracy: 0.9709 - val_loss: 0.1289 - val_accuracy: 0.9752\n",
      "Epoch 456/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0874 - accuracy: 0.9694 - val_loss: 0.1306 - val_accuracy: 0.9752\n",
      "Epoch 457/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0866 - accuracy: 0.9709 - val_loss: 0.1337 - val_accuracy: 0.9658\n",
      "Epoch 458/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0863 - accuracy: 0.9740 - val_loss: 0.1356 - val_accuracy: 0.9658\n",
      "Epoch 459/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0864 - accuracy: 0.9724 - val_loss: 0.1381 - val_accuracy: 0.9658\n",
      "Epoch 460/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0870 - accuracy: 0.9740 - val_loss: 0.1344 - val_accuracy: 0.9658\n",
      "Epoch 461/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0866 - accuracy: 0.9740 - val_loss: 0.1283 - val_accuracy: 0.9752\n",
      "Epoch 462/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0868 - accuracy: 0.9709 - val_loss: 0.1278 - val_accuracy: 0.9752\n",
      "Epoch 463/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0868 - accuracy: 0.9694 - val_loss: 0.1295 - val_accuracy: 0.9752\n",
      "Epoch 464/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0856 - accuracy: 0.9709 - val_loss: 0.1370 - val_accuracy: 0.9658\n",
      "Epoch 465/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0861 - accuracy: 0.9740 - val_loss: 0.1399 - val_accuracy: 0.9627\n",
      "Epoch 466/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0867 - accuracy: 0.9740 - val_loss: 0.1335 - val_accuracy: 0.9658\n",
      "Epoch 467/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0853 - accuracy: 0.9740 - val_loss: 0.1282 - val_accuracy: 0.9752\n",
      "Epoch 468/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0864 - accuracy: 0.9709 - val_loss: 0.1276 - val_accuracy: 0.9752\n",
      "Epoch 469/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0849 - accuracy: 0.9709 - val_loss: 0.1367 - val_accuracy: 0.9658\n",
      "Epoch 470/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0853 - accuracy: 0.9740 - val_loss: 0.1439 - val_accuracy: 0.9596\n",
      "Epoch 471/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0869 - accuracy: 0.9740 - val_loss: 0.1384 - val_accuracy: 0.9658\n",
      "Epoch 472/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0852 - accuracy: 0.9755 - val_loss: 0.1289 - val_accuracy: 0.9752\n",
      "Epoch 473/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0858 - accuracy: 0.9709 - val_loss: 0.1280 - val_accuracy: 0.9752\n",
      "Epoch 474/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0854 - accuracy: 0.9709 - val_loss: 0.1347 - val_accuracy: 0.9658\n",
      "Epoch 475/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0845 - accuracy: 0.9740 - val_loss: 0.1436 - val_accuracy: 0.9627\n",
      "Epoch 476/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0862 - accuracy: 0.9724 - val_loss: 0.1435 - val_accuracy: 0.9627\n",
      "Epoch 477/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0862 - accuracy: 0.9740 - val_loss: 0.1367 - val_accuracy: 0.9658\n",
      "Epoch 478/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0839 - accuracy: 0.9724 - val_loss: 0.1272 - val_accuracy: 0.9752\n",
      "Epoch 479/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0838 - accuracy: 0.9709 - val_loss: 0.1224 - val_accuracy: 0.9783\n",
      "Epoch 480/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0876 - accuracy: 0.9694 - val_loss: 0.1244 - val_accuracy: 0.9752\n",
      "Epoch 481/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0834 - accuracy: 0.9724 - val_loss: 0.1400 - val_accuracy: 0.9658\n",
      "Epoch 482/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0868 - accuracy: 0.9740 - val_loss: 0.1473 - val_accuracy: 0.9596\n",
      "Epoch 483/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0866 - accuracy: 0.9740 - val_loss: 0.1287 - val_accuracy: 0.9720\n",
      "Epoch 484/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0812 - accuracy: 0.9740 - val_loss: 0.1212 - val_accuracy: 0.9814\n",
      "Epoch 485/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0938 - accuracy: 0.9709 - val_loss: 0.1208 - val_accuracy: 0.9814\n",
      "Epoch 486/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0876 - accuracy: 0.9663 - val_loss: 0.1375 - val_accuracy: 0.9658\n",
      "Epoch 487/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0865 - accuracy: 0.9724 - val_loss: 0.1546 - val_accuracy: 0.9596\n",
      "Epoch 488/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0894 - accuracy: 0.9694 - val_loss: 0.1346 - val_accuracy: 0.9658\n",
      "Epoch 489/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0819 - accuracy: 0.9755 - val_loss: 0.1201 - val_accuracy: 0.9814\n",
      "Epoch 490/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0890 - accuracy: 0.9694 - val_loss: 0.1198 - val_accuracy: 0.9814\n",
      "Epoch 491/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0873 - accuracy: 0.9694 - val_loss: 0.1307 - val_accuracy: 0.9720\n",
      "Epoch 492/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0830 - accuracy: 0.9755 - val_loss: 0.1457 - val_accuracy: 0.9596\n",
      "Epoch 493/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0865 - accuracy: 0.9724 - val_loss: 0.1361 - val_accuracy: 0.9658\n",
      "Epoch 494/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0818 - accuracy: 0.9740 - val_loss: 0.1216 - val_accuracy: 0.9783\n",
      "Epoch 495/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0837 - accuracy: 0.9709 - val_loss: 0.1203 - val_accuracy: 0.9814\n",
      "Epoch 496/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0882 - accuracy: 0.9709 - val_loss: 0.1250 - val_accuracy: 0.9752\n",
      "Epoch 497/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0823 - accuracy: 0.9724 - val_loss: 0.1483 - val_accuracy: 0.9596\n",
      "Epoch 498/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0873 - accuracy: 0.9709 - val_loss: 0.1480 - val_accuracy: 0.9596\n",
      "Epoch 499/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0861 - accuracy: 0.9724 - val_loss: 0.1283 - val_accuracy: 0.9720\n",
      "Epoch 500/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0812 - accuracy: 0.9740 - val_loss: 0.1198 - val_accuracy: 0.9783\n",
      "Epoch 501/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0839 - accuracy: 0.9709 - val_loss: 0.1204 - val_accuracy: 0.9783\n",
      "Epoch 502/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0821 - accuracy: 0.9724 - val_loss: 0.1295 - val_accuracy: 0.9720\n",
      "Epoch 503/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0819 - accuracy: 0.9755 - val_loss: 0.1341 - val_accuracy: 0.9658\n",
      "Epoch 504/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0816 - accuracy: 0.9740 - val_loss: 0.1260 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 505/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0804 - accuracy: 0.9755 - val_loss: 0.1199 - val_accuracy: 0.9783\n",
      "Epoch 506/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0828 - accuracy: 0.9724 - val_loss: 0.1189 - val_accuracy: 0.9783\n",
      "Epoch 507/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0822 - accuracy: 0.9709 - val_loss: 0.1272 - val_accuracy: 0.9752\n",
      "Epoch 508/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0805 - accuracy: 0.9770 - val_loss: 0.1410 - val_accuracy: 0.9658\n",
      "Epoch 509/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0840 - accuracy: 0.9740 - val_loss: 0.1387 - val_accuracy: 0.9658\n",
      "Epoch 510/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0824 - accuracy: 0.9740 - val_loss: 0.1251 - val_accuracy: 0.9752\n",
      "Epoch 511/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0807 - accuracy: 0.9755 - val_loss: 0.1194 - val_accuracy: 0.9783\n",
      "Epoch 512/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0816 - accuracy: 0.9709 - val_loss: 0.1233 - val_accuracy: 0.9752\n",
      "Epoch 513/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0808 - accuracy: 0.9740 - val_loss: 0.1308 - val_accuracy: 0.9689\n",
      "Epoch 514/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0800 - accuracy: 0.9755 - val_loss: 0.1289 - val_accuracy: 0.9752\n",
      "Epoch 515/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0795 - accuracy: 0.9755 - val_loss: 0.1249 - val_accuracy: 0.9752\n",
      "Epoch 516/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0797 - accuracy: 0.9740 - val_loss: 0.1237 - val_accuracy: 0.9752\n",
      "Epoch 517/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0798 - accuracy: 0.9740 - val_loss: 0.1246 - val_accuracy: 0.9752\n",
      "Epoch 518/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0792 - accuracy: 0.9740 - val_loss: 0.1228 - val_accuracy: 0.9752\n",
      "Epoch 519/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0791 - accuracy: 0.9740 - val_loss: 0.1245 - val_accuracy: 0.9752\n",
      "Epoch 520/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0788 - accuracy: 0.9740 - val_loss: 0.1279 - val_accuracy: 0.9752\n",
      "Epoch 521/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0789 - accuracy: 0.9755 - val_loss: 0.1331 - val_accuracy: 0.9720\n",
      "Epoch 522/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0800 - accuracy: 0.9755 - val_loss: 0.1297 - val_accuracy: 0.9752\n",
      "Epoch 523/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0790 - accuracy: 0.9755 - val_loss: 0.1213 - val_accuracy: 0.9752\n",
      "Epoch 524/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0789 - accuracy: 0.9755 - val_loss: 0.1192 - val_accuracy: 0.9752\n",
      "Epoch 525/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0796 - accuracy: 0.9755 - val_loss: 0.1219 - val_accuracy: 0.9752\n",
      "Epoch 526/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0786 - accuracy: 0.9770 - val_loss: 0.1295 - val_accuracy: 0.9752\n",
      "Epoch 527/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0793 - accuracy: 0.9755 - val_loss: 0.1293 - val_accuracy: 0.9752\n",
      "Epoch 528/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0787 - accuracy: 0.9755 - val_loss: 0.1220 - val_accuracy: 0.9752\n",
      "Epoch 529/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0786 - accuracy: 0.9740 - val_loss: 0.1196 - val_accuracy: 0.9752\n",
      "Epoch 530/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0785 - accuracy: 0.9755 - val_loss: 0.1247 - val_accuracy: 0.9752\n",
      "Epoch 531/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0775 - accuracy: 0.9755 - val_loss: 0.1336 - val_accuracy: 0.9720\n",
      "Epoch 532/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0800 - accuracy: 0.9740 - val_loss: 0.1344 - val_accuracy: 0.9720\n",
      "Epoch 533/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0798 - accuracy: 0.9740 - val_loss: 0.1245 - val_accuracy: 0.9752\n",
      "Epoch 534/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0776 - accuracy: 0.9740 - val_loss: 0.1200 - val_accuracy: 0.9752\n",
      "Epoch 535/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0780 - accuracy: 0.9755 - val_loss: 0.1208 - val_accuracy: 0.9752\n",
      "Epoch 536/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0775 - accuracy: 0.9755 - val_loss: 0.1255 - val_accuracy: 0.9752\n",
      "Epoch 537/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0776 - accuracy: 0.9740 - val_loss: 0.1279 - val_accuracy: 0.9752\n",
      "Epoch 538/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0773 - accuracy: 0.9755 - val_loss: 0.1243 - val_accuracy: 0.9752\n",
      "Epoch 539/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0775 - accuracy: 0.9724 - val_loss: 0.1256 - val_accuracy: 0.9752\n",
      "Epoch 540/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0769 - accuracy: 0.9740 - val_loss: 0.1309 - val_accuracy: 0.9752\n",
      "Epoch 541/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0775 - accuracy: 0.9755 - val_loss: 0.1299 - val_accuracy: 0.9752\n",
      "Epoch 542/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0772 - accuracy: 0.9755 - val_loss: 0.1239 - val_accuracy: 0.9752\n",
      "Epoch 543/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0767 - accuracy: 0.9770 - val_loss: 0.1203 - val_accuracy: 0.9752\n",
      "Epoch 544/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0772 - accuracy: 0.9755 - val_loss: 0.1230 - val_accuracy: 0.9752\n",
      "Epoch 545/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0764 - accuracy: 0.9770 - val_loss: 0.1309 - val_accuracy: 0.9720\n",
      "Epoch 546/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0772 - accuracy: 0.9755 - val_loss: 0.1349 - val_accuracy: 0.9720\n",
      "Epoch 547/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0782 - accuracy: 0.9755 - val_loss: 0.1317 - val_accuracy: 0.9720\n",
      "Epoch 548/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0766 - accuracy: 0.9755 - val_loss: 0.1209 - val_accuracy: 0.9752\n",
      "Epoch 549/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0770 - accuracy: 0.9755 - val_loss: 0.1167 - val_accuracy: 0.9783\n",
      "Epoch 550/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0777 - accuracy: 0.9709 - val_loss: 0.1227 - val_accuracy: 0.9752\n",
      "Epoch 551/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0761 - accuracy: 0.9755 - val_loss: 0.1349 - val_accuracy: 0.9720\n",
      "Epoch 552/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0784 - accuracy: 0.9740 - val_loss: 0.1314 - val_accuracy: 0.9752\n",
      "Epoch 553/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0780 - accuracy: 0.9755 - val_loss: 0.1217 - val_accuracy: 0.9752\n",
      "Epoch 554/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0756 - accuracy: 0.9755 - val_loss: 0.1189 - val_accuracy: 0.9752\n",
      "Epoch 555/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0767 - accuracy: 0.9755 - val_loss: 0.1196 - val_accuracy: 0.9752\n",
      "Epoch 556/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0758 - accuracy: 0.9770 - val_loss: 0.1280 - val_accuracy: 0.9752\n",
      "Epoch 557/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0759 - accuracy: 0.9755 - val_loss: 0.1296 - val_accuracy: 0.9752\n",
      "Epoch 558/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0762 - accuracy: 0.9755 - val_loss: 0.1230 - val_accuracy: 0.9752\n",
      "Epoch 559/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0747 - accuracy: 0.9770 - val_loss: 0.1156 - val_accuracy: 0.9783\n",
      "Epoch 560/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0774 - accuracy: 0.9724 - val_loss: 0.1156 - val_accuracy: 0.9783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 561/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0772 - accuracy: 0.9740 - val_loss: 0.1217 - val_accuracy: 0.9752\n",
      "Epoch 562/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0747 - accuracy: 0.9770 - val_loss: 0.1267 - val_accuracy: 0.9752\n",
      "Epoch 563/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0756 - accuracy: 0.9770 - val_loss: 0.1272 - val_accuracy: 0.9752\n",
      "Epoch 564/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0754 - accuracy: 0.9770 - val_loss: 0.1208 - val_accuracy: 0.9752\n",
      "Epoch 565/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0746 - accuracy: 0.9770 - val_loss: 0.1174 - val_accuracy: 0.9752\n",
      "Epoch 566/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0753 - accuracy: 0.9786 - val_loss: 0.1176 - val_accuracy: 0.9752\n",
      "Epoch 567/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0748 - accuracy: 0.9786 - val_loss: 0.1185 - val_accuracy: 0.9752\n",
      "Epoch 568/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0743 - accuracy: 0.9770 - val_loss: 0.1235 - val_accuracy: 0.9752\n",
      "Epoch 569/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0747 - accuracy: 0.9770 - val_loss: 0.1264 - val_accuracy: 0.9752\n",
      "Epoch 570/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0745 - accuracy: 0.9770 - val_loss: 0.1208 - val_accuracy: 0.9752\n",
      "Epoch 571/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0738 - accuracy: 0.9770 - val_loss: 0.1166 - val_accuracy: 0.9752\n",
      "Epoch 572/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0747 - accuracy: 0.9770 - val_loss: 0.1156 - val_accuracy: 0.9752\n",
      "Epoch 573/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0748 - accuracy: 0.9786 - val_loss: 0.1202 - val_accuracy: 0.9752\n",
      "Epoch 574/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0731 - accuracy: 0.9786 - val_loss: 0.1311 - val_accuracy: 0.9752\n",
      "Epoch 575/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0764 - accuracy: 0.9740 - val_loss: 0.1349 - val_accuracy: 0.9720\n",
      "Epoch 576/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0763 - accuracy: 0.9724 - val_loss: 0.1220 - val_accuracy: 0.9752\n",
      "Epoch 577/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0731 - accuracy: 0.9770 - val_loss: 0.1141 - val_accuracy: 0.9814\n",
      "Epoch 578/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0776 - accuracy: 0.9755 - val_loss: 0.1154 - val_accuracy: 0.9783\n",
      "Epoch 579/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0742 - accuracy: 0.9755 - val_loss: 0.1305 - val_accuracy: 0.9752\n",
      "Epoch 580/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0747 - accuracy: 0.9740 - val_loss: 0.1412 - val_accuracy: 0.9689\n",
      "Epoch 581/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0786 - accuracy: 0.9724 - val_loss: 0.1344 - val_accuracy: 0.9720\n",
      "Epoch 582/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0750 - accuracy: 0.9755 - val_loss: 0.1218 - val_accuracy: 0.9752\n",
      "Epoch 583/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0742 - accuracy: 0.9770 - val_loss: 0.1162 - val_accuracy: 0.9752\n",
      "Epoch 584/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0739 - accuracy: 0.9755 - val_loss: 0.1204 - val_accuracy: 0.9752\n",
      "Epoch 585/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0736 - accuracy: 0.9755 - val_loss: 0.1238 - val_accuracy: 0.9752\n",
      "Epoch 586/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0731 - accuracy: 0.9770 - val_loss: 0.1224 - val_accuracy: 0.9752\n",
      "Epoch 587/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0725 - accuracy: 0.9770 - val_loss: 0.1236 - val_accuracy: 0.9752\n",
      "Epoch 588/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0726 - accuracy: 0.9770 - val_loss: 0.1215 - val_accuracy: 0.9752\n",
      "Epoch 589/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0726 - accuracy: 0.9770 - val_loss: 0.1200 - val_accuracy: 0.9752\n",
      "Epoch 590/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0723 - accuracy: 0.9770 - val_loss: 0.1222 - val_accuracy: 0.9752\n",
      "Epoch 591/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0725 - accuracy: 0.9755 - val_loss: 0.1212 - val_accuracy: 0.9752\n",
      "Epoch 592/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0722 - accuracy: 0.9770 - val_loss: 0.1171 - val_accuracy: 0.9752\n",
      "Epoch 593/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0723 - accuracy: 0.9770 - val_loss: 0.1166 - val_accuracy: 0.9752\n",
      "Epoch 594/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0727 - accuracy: 0.9770 - val_loss: 0.1194 - val_accuracy: 0.9752\n",
      "Epoch 595/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0726 - accuracy: 0.9770 - val_loss: 0.1254 - val_accuracy: 0.9752\n",
      "Epoch 596/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0730 - accuracy: 0.9770 - val_loss: 0.1201 - val_accuracy: 0.9752\n",
      "Epoch 597/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0725 - accuracy: 0.9770 - val_loss: 0.1142 - val_accuracy: 0.9752\n",
      "Epoch 598/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0726 - accuracy: 0.9786 - val_loss: 0.1161 - val_accuracy: 0.9752\n",
      "Epoch 599/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0719 - accuracy: 0.9786 - val_loss: 0.1215 - val_accuracy: 0.9752\n",
      "Epoch 600/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0721 - accuracy: 0.9770 - val_loss: 0.1229 - val_accuracy: 0.9752\n",
      "Epoch 601/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0718 - accuracy: 0.9770 - val_loss: 0.1182 - val_accuracy: 0.9752\n",
      "Epoch 602/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0718 - accuracy: 0.9770 - val_loss: 0.1186 - val_accuracy: 0.9752\n",
      "Epoch 603/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0714 - accuracy: 0.9770 - val_loss: 0.1226 - val_accuracy: 0.9752\n",
      "Epoch 604/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0715 - accuracy: 0.9786 - val_loss: 0.1222 - val_accuracy: 0.9752\n",
      "Epoch 605/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0712 - accuracy: 0.9786 - val_loss: 0.1207 - val_accuracy: 0.9752\n",
      "Epoch 606/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0706 - accuracy: 0.9770 - val_loss: 0.1156 - val_accuracy: 0.9783\n",
      "Epoch 607/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0721 - accuracy: 0.9786 - val_loss: 0.1154 - val_accuracy: 0.9783\n",
      "Epoch 608/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0714 - accuracy: 0.9786 - val_loss: 0.1243 - val_accuracy: 0.9752\n",
      "Epoch 609/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0710 - accuracy: 0.9770 - val_loss: 0.1320 - val_accuracy: 0.9752\n",
      "Epoch 610/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0734 - accuracy: 0.9740 - val_loss: 0.1225 - val_accuracy: 0.9752\n",
      "Epoch 611/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0715 - accuracy: 0.9755 - val_loss: 0.1128 - val_accuracy: 0.9814\n",
      "Epoch 612/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0742 - accuracy: 0.9770 - val_loss: 0.1145 - val_accuracy: 0.9783\n",
      "Epoch 613/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0710 - accuracy: 0.9755 - val_loss: 0.1246 - val_accuracy: 0.9752\n",
      "Epoch 614/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0711 - accuracy: 0.9786 - val_loss: 0.1340 - val_accuracy: 0.9689\n",
      "Epoch 615/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0738 - accuracy: 0.9740 - val_loss: 0.1271 - val_accuracy: 0.9752\n",
      "Epoch 616/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0722 - accuracy: 0.9740 - val_loss: 0.1178 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0700 - accuracy: 0.9770 - val_loss: 0.1164 - val_accuracy: 0.9752\n",
      "Epoch 618/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0701 - accuracy: 0.9770 - val_loss: 0.1174 - val_accuracy: 0.9752\n",
      "Epoch 619/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0698 - accuracy: 0.9770 - val_loss: 0.1195 - val_accuracy: 0.9752\n",
      "Epoch 620/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0697 - accuracy: 0.9770 - val_loss: 0.1204 - val_accuracy: 0.9752\n",
      "Epoch 621/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0699 - accuracy: 0.9786 - val_loss: 0.1203 - val_accuracy: 0.9752\n",
      "Epoch 622/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0696 - accuracy: 0.9770 - val_loss: 0.1219 - val_accuracy: 0.9752\n",
      "Epoch 623/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0696 - accuracy: 0.9770 - val_loss: 0.1198 - val_accuracy: 0.9752\n",
      "Epoch 624/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0693 - accuracy: 0.9755 - val_loss: 0.1179 - val_accuracy: 0.9752\n",
      "Epoch 625/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0694 - accuracy: 0.9770 - val_loss: 0.1186 - val_accuracy: 0.9752\n",
      "Epoch 626/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0692 - accuracy: 0.9770 - val_loss: 0.1224 - val_accuracy: 0.9752\n",
      "Epoch 627/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0695 - accuracy: 0.9786 - val_loss: 0.1239 - val_accuracy: 0.9752\n",
      "Epoch 628/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0698 - accuracy: 0.9770 - val_loss: 0.1218 - val_accuracy: 0.9752\n",
      "Epoch 629/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0692 - accuracy: 0.9770 - val_loss: 0.1193 - val_accuracy: 0.9752\n",
      "Epoch 630/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0689 - accuracy: 0.9786 - val_loss: 0.1154 - val_accuracy: 0.9752\n",
      "Epoch 631/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0694 - accuracy: 0.9786 - val_loss: 0.1157 - val_accuracy: 0.9752\n",
      "Epoch 632/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0687 - accuracy: 0.9770 - val_loss: 0.1227 - val_accuracy: 0.9752\n",
      "Epoch 633/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0697 - accuracy: 0.9786 - val_loss: 0.1280 - val_accuracy: 0.9752\n",
      "Epoch 634/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0705 - accuracy: 0.9770 - val_loss: 0.1207 - val_accuracy: 0.9752\n",
      "Epoch 635/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0686 - accuracy: 0.9801 - val_loss: 0.1139 - val_accuracy: 0.9752\n",
      "Epoch 636/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0694 - accuracy: 0.9786 - val_loss: 0.1152 - val_accuracy: 0.9752\n",
      "Epoch 637/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0681 - accuracy: 0.9786 - val_loss: 0.1252 - val_accuracy: 0.9752\n",
      "Epoch 638/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0693 - accuracy: 0.9770 - val_loss: 0.1320 - val_accuracy: 0.9752\n",
      "Epoch 639/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0714 - accuracy: 0.9740 - val_loss: 0.1227 - val_accuracy: 0.9752\n",
      "Epoch 640/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0698 - accuracy: 0.9801 - val_loss: 0.1123 - val_accuracy: 0.9752\n",
      "Epoch 641/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0692 - accuracy: 0.9786 - val_loss: 0.1148 - val_accuracy: 0.9752\n",
      "Epoch 642/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0681 - accuracy: 0.9786 - val_loss: 0.1226 - val_accuracy: 0.9752\n",
      "Epoch 643/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0686 - accuracy: 0.9801 - val_loss: 0.1241 - val_accuracy: 0.9752\n",
      "Epoch 644/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0685 - accuracy: 0.9786 - val_loss: 0.1176 - val_accuracy: 0.9752\n",
      "Epoch 645/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0673 - accuracy: 0.9770 - val_loss: 0.1109 - val_accuracy: 0.9783\n",
      "Epoch 646/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0699 - accuracy: 0.9770 - val_loss: 0.1107 - val_accuracy: 0.9783\n",
      "Epoch 647/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0702 - accuracy: 0.9770 - val_loss: 0.1168 - val_accuracy: 0.9752\n",
      "Epoch 648/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0675 - accuracy: 0.9786 - val_loss: 0.1182 - val_accuracy: 0.9752\n",
      "Epoch 649/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0675 - accuracy: 0.9786 - val_loss: 0.1171 - val_accuracy: 0.9752\n",
      "Epoch 650/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0674 - accuracy: 0.9786 - val_loss: 0.1146 - val_accuracy: 0.9752\n",
      "Epoch 651/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0675 - accuracy: 0.9786 - val_loss: 0.1131 - val_accuracy: 0.9752\n",
      "Epoch 652/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0675 - accuracy: 0.9786 - val_loss: 0.1171 - val_accuracy: 0.9752\n",
      "Epoch 653/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0677 - accuracy: 0.9801 - val_loss: 0.1197 - val_accuracy: 0.9752\n",
      "Epoch 654/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0675 - accuracy: 0.9786 - val_loss: 0.1131 - val_accuracy: 0.9752\n",
      "Epoch 655/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0684 - accuracy: 0.9786 - val_loss: 0.1097 - val_accuracy: 0.9783\n",
      "Epoch 656/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0695 - accuracy: 0.9770 - val_loss: 0.1150 - val_accuracy: 0.9752\n",
      "Epoch 657/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0679 - accuracy: 0.9770 - val_loss: 0.1195 - val_accuracy: 0.9752\n",
      "Epoch 658/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0672 - accuracy: 0.9786 - val_loss: 0.1140 - val_accuracy: 0.9752\n",
      "Epoch 659/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0671 - accuracy: 0.9786 - val_loss: 0.1097 - val_accuracy: 0.9783\n",
      "Epoch 660/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0689 - accuracy: 0.9770 - val_loss: 0.1122 - val_accuracy: 0.9752\n",
      "Epoch 661/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0679 - accuracy: 0.9770 - val_loss: 0.1161 - val_accuracy: 0.9752\n",
      "Epoch 662/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0667 - accuracy: 0.9786 - val_loss: 0.1152 - val_accuracy: 0.9752\n",
      "Epoch 663/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0664 - accuracy: 0.9786 - val_loss: 0.1130 - val_accuracy: 0.9752\n",
      "Epoch 664/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0667 - accuracy: 0.9786 - val_loss: 0.1117 - val_accuracy: 0.9783\n",
      "Epoch 665/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0670 - accuracy: 0.9786 - val_loss: 0.1129 - val_accuracy: 0.9752\n",
      "Epoch 666/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0666 - accuracy: 0.9801 - val_loss: 0.1171 - val_accuracy: 0.9752\n",
      "Epoch 667/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0659 - accuracy: 0.9770 - val_loss: 0.1213 - val_accuracy: 0.9752\n",
      "Epoch 668/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0664 - accuracy: 0.9786 - val_loss: 0.1230 - val_accuracy: 0.9752\n",
      "Epoch 669/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0666 - accuracy: 0.9786 - val_loss: 0.1154 - val_accuracy: 0.9752\n",
      "Epoch 670/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0654 - accuracy: 0.9786 - val_loss: 0.1082 - val_accuracy: 0.9814\n",
      "Epoch 671/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0714 - accuracy: 0.9755 - val_loss: 0.1093 - val_accuracy: 0.9783\n",
      "Epoch 672/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0678 - accuracy: 0.9801 - val_loss: 0.1219 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 673/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0671 - accuracy: 0.9786 - val_loss: 0.1273 - val_accuracy: 0.9752\n",
      "Epoch 674/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0683 - accuracy: 0.9770 - val_loss: 0.1171 - val_accuracy: 0.9752\n",
      "Epoch 675/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0658 - accuracy: 0.9816 - val_loss: 0.1087 - val_accuracy: 0.9783\n",
      "Epoch 676/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0684 - accuracy: 0.9770 - val_loss: 0.1102 - val_accuracy: 0.9752\n",
      "Epoch 677/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0665 - accuracy: 0.9801 - val_loss: 0.1223 - val_accuracy: 0.9752\n",
      "Epoch 678/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0674 - accuracy: 0.9786 - val_loss: 0.1226 - val_accuracy: 0.9752\n",
      "Epoch 679/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0666 - accuracy: 0.9786 - val_loss: 0.1134 - val_accuracy: 0.9752\n",
      "Epoch 680/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0658 - accuracy: 0.9801 - val_loss: 0.1128 - val_accuracy: 0.9752\n",
      "Epoch 681/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0654 - accuracy: 0.9786 - val_loss: 0.1179 - val_accuracy: 0.9752\n",
      "Epoch 682/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0652 - accuracy: 0.9786 - val_loss: 0.1228 - val_accuracy: 0.9752\n",
      "Epoch 683/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0658 - accuracy: 0.9786 - val_loss: 0.1199 - val_accuracy: 0.9752\n",
      "Epoch 684/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0647 - accuracy: 0.9801 - val_loss: 0.1123 - val_accuracy: 0.9752\n",
      "Epoch 685/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0650 - accuracy: 0.9801 - val_loss: 0.1083 - val_accuracy: 0.9814\n",
      "Epoch 686/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0671 - accuracy: 0.9786 - val_loss: 0.1112 - val_accuracy: 0.9752\n",
      "Epoch 687/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0649 - accuracy: 0.9786 - val_loss: 0.1218 - val_accuracy: 0.9752\n",
      "Epoch 688/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0660 - accuracy: 0.9770 - val_loss: 0.1263 - val_accuracy: 0.9752\n",
      "Epoch 689/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0668 - accuracy: 0.9786 - val_loss: 0.1171 - val_accuracy: 0.9752\n",
      "Epoch 690/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0648 - accuracy: 0.9801 - val_loss: 0.1103 - val_accuracy: 0.9752\n",
      "Epoch 691/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0651 - accuracy: 0.9786 - val_loss: 0.1101 - val_accuracy: 0.9752\n",
      "Epoch 692/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0651 - accuracy: 0.9801 - val_loss: 0.1126 - val_accuracy: 0.9752\n",
      "Epoch 693/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0642 - accuracy: 0.9801 - val_loss: 0.1149 - val_accuracy: 0.9752\n",
      "Epoch 694/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0641 - accuracy: 0.9786 - val_loss: 0.1175 - val_accuracy: 0.9752\n",
      "Epoch 695/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0646 - accuracy: 0.9801 - val_loss: 0.1186 - val_accuracy: 0.9752\n",
      "Epoch 696/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0642 - accuracy: 0.9786 - val_loss: 0.1135 - val_accuracy: 0.9752\n",
      "Epoch 697/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0641 - accuracy: 0.9786 - val_loss: 0.1126 - val_accuracy: 0.9752\n",
      "Epoch 698/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0641 - accuracy: 0.9786 - val_loss: 0.1154 - val_accuracy: 0.9752\n",
      "Epoch 699/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0640 - accuracy: 0.9786 - val_loss: 0.1207 - val_accuracy: 0.9752\n",
      "Epoch 700/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0654 - accuracy: 0.9786 - val_loss: 0.1243 - val_accuracy: 0.9752\n",
      "Epoch 701/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0648 - accuracy: 0.9786 - val_loss: 0.1128 - val_accuracy: 0.9752\n",
      "Epoch 702/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0633 - accuracy: 0.9786 - val_loss: 0.1070 - val_accuracy: 0.9814\n",
      "Epoch 703/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0671 - accuracy: 0.9801 - val_loss: 0.1109 - val_accuracy: 0.9783\n",
      "Epoch 704/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0641 - accuracy: 0.9786 - val_loss: 0.1247 - val_accuracy: 0.9752\n",
      "Epoch 705/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0651 - accuracy: 0.9770 - val_loss: 0.1291 - val_accuracy: 0.9752\n",
      "Epoch 706/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0665 - accuracy: 0.9755 - val_loss: 0.1221 - val_accuracy: 0.9752\n",
      "Epoch 707/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0647 - accuracy: 0.9786 - val_loss: 0.1129 - val_accuracy: 0.9752\n",
      "Epoch 708/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0639 - accuracy: 0.9786 - val_loss: 0.1144 - val_accuracy: 0.9752\n",
      "Epoch 709/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0629 - accuracy: 0.9786 - val_loss: 0.1240 - val_accuracy: 0.9752\n",
      "Epoch 710/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0646 - accuracy: 0.9770 - val_loss: 0.1265 - val_accuracy: 0.9752\n",
      "Epoch 711/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0651 - accuracy: 0.9770 - val_loss: 0.1186 - val_accuracy: 0.9752\n",
      "Epoch 712/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0628 - accuracy: 0.9786 - val_loss: 0.1126 - val_accuracy: 0.9752\n",
      "Epoch 713/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0634 - accuracy: 0.9786 - val_loss: 0.1110 - val_accuracy: 0.9752\n",
      "Epoch 714/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0630 - accuracy: 0.9786 - val_loss: 0.1167 - val_accuracy: 0.9752\n",
      "Epoch 715/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0626 - accuracy: 0.9786 - val_loss: 0.1232 - val_accuracy: 0.9752\n",
      "Epoch 716/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0640 - accuracy: 0.9786 - val_loss: 0.1230 - val_accuracy: 0.9752\n",
      "Epoch 717/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0637 - accuracy: 0.9786 - val_loss: 0.1172 - val_accuracy: 0.9752\n",
      "Epoch 718/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0621 - accuracy: 0.9801 - val_loss: 0.1104 - val_accuracy: 0.9783\n",
      "Epoch 719/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0638 - accuracy: 0.9801 - val_loss: 0.1100 - val_accuracy: 0.9783\n",
      "Epoch 720/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0634 - accuracy: 0.9801 - val_loss: 0.1171 - val_accuracy: 0.9752\n",
      "Epoch 721/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0622 - accuracy: 0.9786 - val_loss: 0.1196 - val_accuracy: 0.9752\n",
      "Epoch 722/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0631 - accuracy: 0.9786 - val_loss: 0.1147 - val_accuracy: 0.9752\n",
      "Epoch 723/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0617 - accuracy: 0.9786 - val_loss: 0.1074 - val_accuracy: 0.9814\n",
      "Epoch 724/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0638 - accuracy: 0.9816 - val_loss: 0.1093 - val_accuracy: 0.9752\n",
      "Epoch 725/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0633 - accuracy: 0.9786 - val_loss: 0.1158 - val_accuracy: 0.9752\n",
      "Epoch 726/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0619 - accuracy: 0.9801 - val_loss: 0.1156 - val_accuracy: 0.9752\n",
      "Epoch 727/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0617 - accuracy: 0.9801 - val_loss: 0.1130 - val_accuracy: 0.9752\n",
      "Epoch 728/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0618 - accuracy: 0.9786 - val_loss: 0.1128 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 729/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0617 - accuracy: 0.9786 - val_loss: 0.1158 - val_accuracy: 0.9752\n",
      "Epoch 730/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0621 - accuracy: 0.9786 - val_loss: 0.1118 - val_accuracy: 0.9752\n",
      "Epoch 731/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0614 - accuracy: 0.9801 - val_loss: 0.1062 - val_accuracy: 0.9814\n",
      "Epoch 732/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0640 - accuracy: 0.9816 - val_loss: 0.1091 - val_accuracy: 0.9752\n",
      "Epoch 733/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0617 - accuracy: 0.9786 - val_loss: 0.1191 - val_accuracy: 0.9752\n",
      "Epoch 734/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0623 - accuracy: 0.9786 - val_loss: 0.1224 - val_accuracy: 0.9752\n",
      "Epoch 735/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0627 - accuracy: 0.9786 - val_loss: 0.1149 - val_accuracy: 0.9752\n",
      "Epoch 736/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0616 - accuracy: 0.9786 - val_loss: 0.1102 - val_accuracy: 0.9752\n",
      "Epoch 737/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0614 - accuracy: 0.9786 - val_loss: 0.1127 - val_accuracy: 0.9752\n",
      "Epoch 738/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0610 - accuracy: 0.9786 - val_loss: 0.1178 - val_accuracy: 0.9752\n",
      "Epoch 739/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0613 - accuracy: 0.9786 - val_loss: 0.1206 - val_accuracy: 0.9752\n",
      "Epoch 740/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0616 - accuracy: 0.9786 - val_loss: 0.1219 - val_accuracy: 0.9752\n",
      "Epoch 741/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0619 - accuracy: 0.9786 - val_loss: 0.1160 - val_accuracy: 0.9752\n",
      "Epoch 742/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0613 - accuracy: 0.9801 - val_loss: 0.1095 - val_accuracy: 0.9814\n",
      "Epoch 743/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0630 - accuracy: 0.9801 - val_loss: 0.1120 - val_accuracy: 0.9783\n",
      "Epoch 744/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0614 - accuracy: 0.9786 - val_loss: 0.1230 - val_accuracy: 0.9752\n",
      "Epoch 745/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0620 - accuracy: 0.9786 - val_loss: 0.1292 - val_accuracy: 0.9752\n",
      "Epoch 746/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0636 - accuracy: 0.9770 - val_loss: 0.1185 - val_accuracy: 0.9752\n",
      "Epoch 747/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0603 - accuracy: 0.9801 - val_loss: 0.1067 - val_accuracy: 0.9814\n",
      "Epoch 748/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0623 - accuracy: 0.9832 - val_loss: 0.1043 - val_accuracy: 0.9814\n",
      "Epoch 749/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0645 - accuracy: 0.9801 - val_loss: 0.1087 - val_accuracy: 0.9752\n",
      "Epoch 750/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0606 - accuracy: 0.9816 - val_loss: 0.1200 - val_accuracy: 0.9752\n",
      "Epoch 751/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0619 - accuracy: 0.9786 - val_loss: 0.1236 - val_accuracy: 0.9752\n",
      "Epoch 752/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0624 - accuracy: 0.9770 - val_loss: 0.1124 - val_accuracy: 0.9752\n",
      "Epoch 753/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0616 - accuracy: 0.9816 - val_loss: 0.1058 - val_accuracy: 0.9783\n",
      "Epoch 754/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0625 - accuracy: 0.9816 - val_loss: 0.1074 - val_accuracy: 0.9752\n",
      "Epoch 755/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0614 - accuracy: 0.9801 - val_loss: 0.1069 - val_accuracy: 0.9783\n",
      "Epoch 756/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0609 - accuracy: 0.9786 - val_loss: 0.1062 - val_accuracy: 0.9783\n",
      "Epoch 757/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0609 - accuracy: 0.9801 - val_loss: 0.1122 - val_accuracy: 0.9752\n",
      "Epoch 758/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0598 - accuracy: 0.9801 - val_loss: 0.1204 - val_accuracy: 0.9752\n",
      "Epoch 759/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0619 - accuracy: 0.9770 - val_loss: 0.1173 - val_accuracy: 0.9752\n",
      "Epoch 760/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0610 - accuracy: 0.9801 - val_loss: 0.1088 - val_accuracy: 0.9752\n",
      "Epoch 761/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0597 - accuracy: 0.9786 - val_loss: 0.1070 - val_accuracy: 0.9783\n",
      "Epoch 762/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0602 - accuracy: 0.9786 - val_loss: 0.1091 - val_accuracy: 0.9752\n",
      "Epoch 763/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0599 - accuracy: 0.9786 - val_loss: 0.1122 - val_accuracy: 0.9752\n",
      "Epoch 764/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0596 - accuracy: 0.9801 - val_loss: 0.1102 - val_accuracy: 0.9752\n",
      "Epoch 765/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0595 - accuracy: 0.9786 - val_loss: 0.1081 - val_accuracy: 0.9752\n",
      "Epoch 766/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0596 - accuracy: 0.9786 - val_loss: 0.1106 - val_accuracy: 0.9752\n",
      "Epoch 767/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0591 - accuracy: 0.9786 - val_loss: 0.1129 - val_accuracy: 0.9752\n",
      "Epoch 768/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0591 - accuracy: 0.9801 - val_loss: 0.1116 - val_accuracy: 0.9752\n",
      "Epoch 769/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0594 - accuracy: 0.9786 - val_loss: 0.1104 - val_accuracy: 0.9752\n",
      "Epoch 770/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0594 - accuracy: 0.9801 - val_loss: 0.1134 - val_accuracy: 0.9752\n",
      "Epoch 771/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0590 - accuracy: 0.9801 - val_loss: 0.1135 - val_accuracy: 0.9752\n",
      "Epoch 772/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0590 - accuracy: 0.9801 - val_loss: 0.1157 - val_accuracy: 0.9752\n",
      "Epoch 773/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0591 - accuracy: 0.9801 - val_loss: 0.1174 - val_accuracy: 0.9752\n",
      "Epoch 774/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0592 - accuracy: 0.9786 - val_loss: 0.1162 - val_accuracy: 0.9752\n",
      "Epoch 775/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0590 - accuracy: 0.9786 - val_loss: 0.1119 - val_accuracy: 0.9752\n",
      "Epoch 776/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0586 - accuracy: 0.9786 - val_loss: 0.1109 - val_accuracy: 0.9752\n",
      "Epoch 777/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0584 - accuracy: 0.9786 - val_loss: 0.1140 - val_accuracy: 0.9752\n",
      "Epoch 778/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0585 - accuracy: 0.9801 - val_loss: 0.1175 - val_accuracy: 0.9752\n",
      "Epoch 779/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0589 - accuracy: 0.9786 - val_loss: 0.1209 - val_accuracy: 0.9752\n",
      "Epoch 780/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0598 - accuracy: 0.9770 - val_loss: 0.1156 - val_accuracy: 0.9752\n",
      "Epoch 781/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0587 - accuracy: 0.9801 - val_loss: 0.1074 - val_accuracy: 0.9814\n",
      "Epoch 782/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0595 - accuracy: 0.9801 - val_loss: 0.1088 - val_accuracy: 0.9752\n",
      "Epoch 783/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0581 - accuracy: 0.9801 - val_loss: 0.1179 - val_accuracy: 0.9752\n",
      "Epoch 784/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0586 - accuracy: 0.9786 - val_loss: 0.1252 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 785/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0610 - accuracy: 0.9770 - val_loss: 0.1186 - val_accuracy: 0.9752\n",
      "Epoch 786/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0598 - accuracy: 0.9786 - val_loss: 0.1077 - val_accuracy: 0.9752\n",
      "Epoch 787/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0582 - accuracy: 0.9801 - val_loss: 0.1082 - val_accuracy: 0.9752\n",
      "Epoch 788/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0581 - accuracy: 0.9786 - val_loss: 0.1111 - val_accuracy: 0.9752\n",
      "Epoch 789/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0577 - accuracy: 0.9801 - val_loss: 0.1098 - val_accuracy: 0.9752\n",
      "Epoch 790/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0576 - accuracy: 0.9801 - val_loss: 0.1091 - val_accuracy: 0.9752\n",
      "Epoch 791/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0580 - accuracy: 0.9801 - val_loss: 0.1082 - val_accuracy: 0.9752\n",
      "Epoch 792/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0578 - accuracy: 0.9786 - val_loss: 0.1069 - val_accuracy: 0.9752\n",
      "Epoch 793/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0578 - accuracy: 0.9801 - val_loss: 0.1102 - val_accuracy: 0.9752\n",
      "Epoch 794/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0572 - accuracy: 0.9801 - val_loss: 0.1132 - val_accuracy: 0.9752\n",
      "Epoch 795/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0575 - accuracy: 0.9801 - val_loss: 0.1145 - val_accuracy: 0.9752\n",
      "Epoch 796/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0578 - accuracy: 0.9801 - val_loss: 0.1145 - val_accuracy: 0.9752\n",
      "Epoch 797/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0580 - accuracy: 0.9801 - val_loss: 0.1097 - val_accuracy: 0.9752\n",
      "Epoch 798/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0575 - accuracy: 0.9801 - val_loss: 0.1040 - val_accuracy: 0.9783\n",
      "Epoch 799/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0589 - accuracy: 0.9816 - val_loss: 0.1067 - val_accuracy: 0.9752\n",
      "Epoch 800/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0572 - accuracy: 0.9801 - val_loss: 0.1132 - val_accuracy: 0.9752\n",
      "Epoch 801/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0571 - accuracy: 0.9801 - val_loss: 0.1196 - val_accuracy: 0.9752\n",
      "Epoch 802/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0589 - accuracy: 0.9786 - val_loss: 0.1143 - val_accuracy: 0.9752\n",
      "Epoch 803/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0560 - accuracy: 0.9801 - val_loss: 0.1025 - val_accuracy: 0.9814\n",
      "Epoch 804/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0620 - accuracy: 0.9816 - val_loss: 0.1024 - val_accuracy: 0.9814\n",
      "Epoch 805/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0607 - accuracy: 0.9786 - val_loss: 0.1165 - val_accuracy: 0.9752\n",
      "Epoch 806/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0580 - accuracy: 0.9801 - val_loss: 0.1223 - val_accuracy: 0.9752\n",
      "Epoch 807/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0590 - accuracy: 0.9770 - val_loss: 0.1133 - val_accuracy: 0.9752\n",
      "Epoch 808/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0569 - accuracy: 0.9801 - val_loss: 0.1087 - val_accuracy: 0.9752\n",
      "Epoch 809/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0570 - accuracy: 0.9801 - val_loss: 0.1105 - val_accuracy: 0.9752\n",
      "Epoch 810/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0562 - accuracy: 0.9801 - val_loss: 0.1187 - val_accuracy: 0.9752\n",
      "Epoch 811/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0582 - accuracy: 0.9770 - val_loss: 0.1159 - val_accuracy: 0.9752\n",
      "Epoch 812/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0582 - accuracy: 0.9801 - val_loss: 0.1066 - val_accuracy: 0.9783\n",
      "Epoch 813/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0572 - accuracy: 0.9801 - val_loss: 0.1137 - val_accuracy: 0.9752\n",
      "Epoch 814/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0567 - accuracy: 0.9801 - val_loss: 0.1274 - val_accuracy: 0.9752\n",
      "Epoch 815/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0601 - accuracy: 0.9770 - val_loss: 0.1199 - val_accuracy: 0.9752\n",
      "Epoch 816/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0581 - accuracy: 0.9786 - val_loss: 0.1079 - val_accuracy: 0.9752\n",
      "Epoch 817/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0566 - accuracy: 0.9801 - val_loss: 0.1081 - val_accuracy: 0.9752\n",
      "Epoch 818/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0559 - accuracy: 0.9816 - val_loss: 0.1153 - val_accuracy: 0.9752\n",
      "Epoch 819/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0567 - accuracy: 0.9801 - val_loss: 0.1150 - val_accuracy: 0.9752\n",
      "Epoch 820/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0559 - accuracy: 0.9801 - val_loss: 0.1066 - val_accuracy: 0.9752\n",
      "Epoch 821/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0561 - accuracy: 0.9801 - val_loss: 0.1036 - val_accuracy: 0.9814\n",
      "Epoch 822/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0578 - accuracy: 0.9832 - val_loss: 0.1074 - val_accuracy: 0.9752\n",
      "Epoch 823/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0554 - accuracy: 0.9816 - val_loss: 0.1202 - val_accuracy: 0.9752\n",
      "Epoch 824/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0582 - accuracy: 0.9770 - val_loss: 0.1274 - val_accuracy: 0.9752\n",
      "Epoch 825/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0584 - accuracy: 0.9786 - val_loss: 0.1105 - val_accuracy: 0.9752\n",
      "Epoch 826/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0575 - accuracy: 0.9801 - val_loss: 0.1037 - val_accuracy: 0.9814\n",
      "Epoch 827/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0584 - accuracy: 0.9847 - val_loss: 0.1117 - val_accuracy: 0.9752\n",
      "Epoch 828/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0555 - accuracy: 0.9801 - val_loss: 0.1225 - val_accuracy: 0.9752\n",
      "Epoch 829/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0575 - accuracy: 0.9770 - val_loss: 0.1194 - val_accuracy: 0.9752\n",
      "Epoch 830/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0572 - accuracy: 0.9786 - val_loss: 0.1093 - val_accuracy: 0.9783\n",
      "Epoch 831/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0557 - accuracy: 0.9801 - val_loss: 0.1082 - val_accuracy: 0.9783\n",
      "Epoch 832/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0555 - accuracy: 0.9801 - val_loss: 0.1129 - val_accuracy: 0.9752\n",
      "Epoch 833/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0567 - accuracy: 0.9801 - val_loss: 0.1134 - val_accuracy: 0.9752\n",
      "Epoch 834/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0554 - accuracy: 0.9786 - val_loss: 0.1055 - val_accuracy: 0.9783\n",
      "Epoch 835/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0555 - accuracy: 0.9816 - val_loss: 0.1058 - val_accuracy: 0.9752\n",
      "Epoch 836/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0550 - accuracy: 0.9832 - val_loss: 0.1102 - val_accuracy: 0.9752\n",
      "Epoch 837/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0549 - accuracy: 0.9801 - val_loss: 0.1127 - val_accuracy: 0.9752\n",
      "Epoch 838/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0556 - accuracy: 0.9801 - val_loss: 0.1109 - val_accuracy: 0.9752\n",
      "Epoch 839/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0558 - accuracy: 0.9801 - val_loss: 0.1134 - val_accuracy: 0.9752\n",
      "Epoch 840/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0561 - accuracy: 0.9801 - val_loss: 0.1148 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 841/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0571 - accuracy: 0.9801 - val_loss: 0.1094 - val_accuracy: 0.9752\n",
      "Epoch 842/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0551 - accuracy: 0.9801 - val_loss: 0.1095 - val_accuracy: 0.9752\n",
      "Epoch 843/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0551 - accuracy: 0.9801 - val_loss: 0.1054 - val_accuracy: 0.9752\n",
      "Epoch 844/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0555 - accuracy: 0.9786 - val_loss: 0.1029 - val_accuracy: 0.9752\n",
      "Epoch 845/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0556 - accuracy: 0.9816 - val_loss: 0.1092 - val_accuracy: 0.9752\n",
      "Epoch 846/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0545 - accuracy: 0.9801 - val_loss: 0.1165 - val_accuracy: 0.9752\n",
      "Epoch 847/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0560 - accuracy: 0.9801 - val_loss: 0.1153 - val_accuracy: 0.9752\n",
      "Epoch 848/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0553 - accuracy: 0.9801 - val_loss: 0.1055 - val_accuracy: 0.9752\n",
      "Epoch 849/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0548 - accuracy: 0.9816 - val_loss: 0.1006 - val_accuracy: 0.9814\n",
      "Epoch 850/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0564 - accuracy: 0.9816 - val_loss: 0.1062 - val_accuracy: 0.9752\n",
      "Epoch 851/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0535 - accuracy: 0.9801 - val_loss: 0.1202 - val_accuracy: 0.9752\n",
      "Epoch 852/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0564 - accuracy: 0.9770 - val_loss: 0.1239 - val_accuracy: 0.9752\n",
      "Epoch 853/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0572 - accuracy: 0.9770 - val_loss: 0.1130 - val_accuracy: 0.9752\n",
      "Epoch 854/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0551 - accuracy: 0.9801 - val_loss: 0.1054 - val_accuracy: 0.9783\n",
      "Epoch 855/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0545 - accuracy: 0.9801 - val_loss: 0.1074 - val_accuracy: 0.9783\n",
      "Epoch 856/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0544 - accuracy: 0.9801 - val_loss: 0.1101 - val_accuracy: 0.9752\n",
      "Epoch 857/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0539 - accuracy: 0.9816 - val_loss: 0.1093 - val_accuracy: 0.9783\n",
      "Epoch 858/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0539 - accuracy: 0.9816 - val_loss: 0.1108 - val_accuracy: 0.9752\n",
      "Epoch 859/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0546 - accuracy: 0.9801 - val_loss: 0.1102 - val_accuracy: 0.9783\n",
      "Epoch 860/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0543 - accuracy: 0.9801 - val_loss: 0.1062 - val_accuracy: 0.9783\n",
      "Epoch 861/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0540 - accuracy: 0.9801 - val_loss: 0.1121 - val_accuracy: 0.9752\n",
      "Epoch 862/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0539 - accuracy: 0.9801 - val_loss: 0.1176 - val_accuracy: 0.9752\n",
      "Epoch 863/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0546 - accuracy: 0.9786 - val_loss: 0.1120 - val_accuracy: 0.9752\n",
      "Epoch 864/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0532 - accuracy: 0.9816 - val_loss: 0.1040 - val_accuracy: 0.9814\n",
      "Epoch 865/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0544 - accuracy: 0.9816 - val_loss: 0.1026 - val_accuracy: 0.9814\n",
      "Epoch 866/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0540 - accuracy: 0.9832 - val_loss: 0.1120 - val_accuracy: 0.9752\n",
      "Epoch 867/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0552 - accuracy: 0.9786 - val_loss: 0.1188 - val_accuracy: 0.9752\n",
      "Epoch 868/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0551 - accuracy: 0.9786 - val_loss: 0.1074 - val_accuracy: 0.9752\n",
      "Epoch 869/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0539 - accuracy: 0.9816 - val_loss: 0.1050 - val_accuracy: 0.9752\n",
      "Epoch 870/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0527 - accuracy: 0.9816 - val_loss: 0.1145 - val_accuracy: 0.9752\n",
      "Epoch 871/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0547 - accuracy: 0.9801 - val_loss: 0.1195 - val_accuracy: 0.9752\n",
      "Epoch 872/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0543 - accuracy: 0.9801 - val_loss: 0.1058 - val_accuracy: 0.9752\n",
      "Epoch 873/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0529 - accuracy: 0.9816 - val_loss: 0.0996 - val_accuracy: 0.9814\n",
      "Epoch 874/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0561 - accuracy: 0.9847 - val_loss: 0.1047 - val_accuracy: 0.9752\n",
      "Epoch 875/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0526 - accuracy: 0.9816 - val_loss: 0.1221 - val_accuracy: 0.9752\n",
      "Epoch 876/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0558 - accuracy: 0.9786 - val_loss: 0.1231 - val_accuracy: 0.9752\n",
      "Epoch 877/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0555 - accuracy: 0.9801 - val_loss: 0.1084 - val_accuracy: 0.9752\n",
      "Epoch 878/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0521 - accuracy: 0.9801 - val_loss: 0.0991 - val_accuracy: 0.9814\n",
      "Epoch 879/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0562 - accuracy: 0.9847 - val_loss: 0.1011 - val_accuracy: 0.9814\n",
      "Epoch 880/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0541 - accuracy: 0.9801 - val_loss: 0.1136 - val_accuracy: 0.9752\n",
      "Epoch 881/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0528 - accuracy: 0.9801 - val_loss: 0.1197 - val_accuracy: 0.9752\n",
      "Epoch 882/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0547 - accuracy: 0.9770 - val_loss: 0.1136 - val_accuracy: 0.9752\n",
      "Epoch 883/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 0.1005 - val_accuracy: 0.9814\n",
      "Epoch 884/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0558 - accuracy: 0.9816 - val_loss: 0.1004 - val_accuracy: 0.9814\n",
      "Epoch 885/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0537 - accuracy: 0.9847 - val_loss: 0.1177 - val_accuracy: 0.9752\n",
      "Epoch 886/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0535 - accuracy: 0.9801 - val_loss: 0.1284 - val_accuracy: 0.9752\n",
      "Epoch 887/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0570 - accuracy: 0.9816 - val_loss: 0.1099 - val_accuracy: 0.9752\n",
      "Epoch 888/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0548 - accuracy: 0.9832 - val_loss: 0.0987 - val_accuracy: 0.9845\n",
      "Epoch 889/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0569 - accuracy: 0.9832 - val_loss: 0.1084 - val_accuracy: 0.9752\n",
      "Epoch 890/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0525 - accuracy: 0.9816 - val_loss: 0.1319 - val_accuracy: 0.9752\n",
      "Epoch 891/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0579 - accuracy: 0.9816 - val_loss: 0.1274 - val_accuracy: 0.9752\n",
      "Epoch 892/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0570 - accuracy: 0.9786 - val_loss: 0.1128 - val_accuracy: 0.9752\n",
      "Epoch 893/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0527 - accuracy: 0.9816 - val_loss: 0.1088 - val_accuracy: 0.9783\n",
      "Epoch 894/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0516 - accuracy: 0.9816 - val_loss: 0.1101 - val_accuracy: 0.9752\n",
      "Epoch 895/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0515 - accuracy: 0.9816 - val_loss: 0.1093 - val_accuracy: 0.9752\n",
      "Epoch 896/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0514 - accuracy: 0.9816 - val_loss: 0.1074 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 897/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0513 - accuracy: 0.9816 - val_loss: 0.1069 - val_accuracy: 0.9752\n",
      "Epoch 898/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0512 - accuracy: 0.9816 - val_loss: 0.1092 - val_accuracy: 0.9752\n",
      "Epoch 899/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0514 - accuracy: 0.9816 - val_loss: 0.1104 - val_accuracy: 0.9752\n",
      "Epoch 900/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0515 - accuracy: 0.9816 - val_loss: 0.1089 - val_accuracy: 0.9752\n",
      "Epoch 901/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0512 - accuracy: 0.9816 - val_loss: 0.1071 - val_accuracy: 0.9752\n",
      "Epoch 902/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0508 - accuracy: 0.9816 - val_loss: 0.1021 - val_accuracy: 0.9814\n",
      "Epoch 903/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0526 - accuracy: 0.9816 - val_loss: 0.1017 - val_accuracy: 0.9814\n",
      "Epoch 904/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0530 - accuracy: 0.9832 - val_loss: 0.1096 - val_accuracy: 0.9752\n",
      "Epoch 905/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0508 - accuracy: 0.9816 - val_loss: 0.1180 - val_accuracy: 0.9752\n",
      "Epoch 906/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0520 - accuracy: 0.9832 - val_loss: 0.1185 - val_accuracy: 0.9752\n",
      "Epoch 907/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0520 - accuracy: 0.9832 - val_loss: 0.1119 - val_accuracy: 0.9752\n",
      "Epoch 908/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0508 - accuracy: 0.9816 - val_loss: 0.1055 - val_accuracy: 0.9783\n",
      "Epoch 909/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0509 - accuracy: 0.9816 - val_loss: 0.1043 - val_accuracy: 0.9783\n",
      "Epoch 910/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0514 - accuracy: 0.9801 - val_loss: 0.1060 - val_accuracy: 0.9783\n",
      "Epoch 911/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0507 - accuracy: 0.9816 - val_loss: 0.1033 - val_accuracy: 0.9783\n",
      "Epoch 912/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0510 - accuracy: 0.9801 - val_loss: 0.1029 - val_accuracy: 0.9783\n",
      "Epoch 913/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0507 - accuracy: 0.9801 - val_loss: 0.1098 - val_accuracy: 0.9752\n",
      "Epoch 914/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0503 - accuracy: 0.9832 - val_loss: 0.1155 - val_accuracy: 0.9752\n",
      "Epoch 915/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0511 - accuracy: 0.9832 - val_loss: 0.1103 - val_accuracy: 0.9752\n",
      "Epoch 916/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0510 - accuracy: 0.9847 - val_loss: 0.1041 - val_accuracy: 0.9783\n",
      "Epoch 917/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0504 - accuracy: 0.9816 - val_loss: 0.1057 - val_accuracy: 0.9783\n",
      "Epoch 918/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0501 - accuracy: 0.9816 - val_loss: 0.1061 - val_accuracy: 0.9752\n",
      "Epoch 919/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0500 - accuracy: 0.9847 - val_loss: 0.1077 - val_accuracy: 0.9752\n",
      "Epoch 920/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0499 - accuracy: 0.9847 - val_loss: 0.1112 - val_accuracy: 0.9752\n",
      "Epoch 921/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0501 - accuracy: 0.9847 - val_loss: 0.1072 - val_accuracy: 0.9752\n",
      "Epoch 922/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0498 - accuracy: 0.9816 - val_loss: 0.1027 - val_accuracy: 0.9783\n",
      "Epoch 923/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0506 - accuracy: 0.9801 - val_loss: 0.1036 - val_accuracy: 0.9783\n",
      "Epoch 924/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0499 - accuracy: 0.9816 - val_loss: 0.1086 - val_accuracy: 0.9752\n",
      "Epoch 925/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0494 - accuracy: 0.9847 - val_loss: 0.1189 - val_accuracy: 0.9752\n",
      "Epoch 926/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0524 - accuracy: 0.9832 - val_loss: 0.1142 - val_accuracy: 0.9752\n",
      "Epoch 927/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0502 - accuracy: 0.9847 - val_loss: 0.0998 - val_accuracy: 0.9814\n",
      "Epoch 928/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0515 - accuracy: 0.9801 - val_loss: 0.0989 - val_accuracy: 0.9814\n",
      "Epoch 929/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0510 - accuracy: 0.9832 - val_loss: 0.1092 - val_accuracy: 0.9752\n",
      "Epoch 930/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0499 - accuracy: 0.9832 - val_loss: 0.1178 - val_accuracy: 0.9752\n",
      "Epoch 931/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0511 - accuracy: 0.9832 - val_loss: 0.1099 - val_accuracy: 0.9752\n",
      "Epoch 932/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0498 - accuracy: 0.9847 - val_loss: 0.1041 - val_accuracy: 0.9783\n",
      "Epoch 933/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0492 - accuracy: 0.9847 - val_loss: 0.1072 - val_accuracy: 0.9752\n",
      "Epoch 934/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0494 - accuracy: 0.9847 - val_loss: 0.1113 - val_accuracy: 0.9752\n",
      "Epoch 935/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0493 - accuracy: 0.9832 - val_loss: 0.1044 - val_accuracy: 0.9783\n",
      "Epoch 936/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0495 - accuracy: 0.9816 - val_loss: 0.1001 - val_accuracy: 0.9814\n",
      "Epoch 937/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0503 - accuracy: 0.9816 - val_loss: 0.1044 - val_accuracy: 0.9783\n",
      "Epoch 938/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0486 - accuracy: 0.9832 - val_loss: 0.1129 - val_accuracy: 0.9752\n",
      "Epoch 939/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0500 - accuracy: 0.9832 - val_loss: 0.1186 - val_accuracy: 0.9752\n",
      "Epoch 940/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0512 - accuracy: 0.9816 - val_loss: 0.1093 - val_accuracy: 0.9752\n",
      "Epoch 941/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0489 - accuracy: 0.9847 - val_loss: 0.1036 - val_accuracy: 0.9783\n",
      "Epoch 942/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0488 - accuracy: 0.9847 - val_loss: 0.1018 - val_accuracy: 0.9783\n",
      "Epoch 943/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0492 - accuracy: 0.9816 - val_loss: 0.1031 - val_accuracy: 0.9783\n",
      "Epoch 944/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0492 - accuracy: 0.9847 - val_loss: 0.1065 - val_accuracy: 0.9752\n",
      "Epoch 945/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0484 - accuracy: 0.9847 - val_loss: 0.1045 - val_accuracy: 0.9783\n",
      "Epoch 946/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0483 - accuracy: 0.9816 - val_loss: 0.1052 - val_accuracy: 0.9783\n",
      "Epoch 947/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0483 - accuracy: 0.9816 - val_loss: 0.1092 - val_accuracy: 0.9752\n",
      "Epoch 948/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0490 - accuracy: 0.9816 - val_loss: 0.1091 - val_accuracy: 0.9752\n",
      "Epoch 949/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0482 - accuracy: 0.9832 - val_loss: 0.1012 - val_accuracy: 0.9783\n",
      "Epoch 950/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0492 - accuracy: 0.9832 - val_loss: 0.1012 - val_accuracy: 0.9783\n",
      "Epoch 951/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0496 - accuracy: 0.9832 - val_loss: 0.1058 - val_accuracy: 0.9752\n",
      "Epoch 952/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0488 - accuracy: 0.9847 - val_loss: 0.1099 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 953/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0484 - accuracy: 0.9862 - val_loss: 0.1189 - val_accuracy: 0.9752\n",
      "Epoch 954/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0506 - accuracy: 0.9832 - val_loss: 0.1119 - val_accuracy: 0.9752\n",
      "Epoch 955/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0487 - accuracy: 0.9832 - val_loss: 0.1034 - val_accuracy: 0.9783\n",
      "Epoch 956/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0482 - accuracy: 0.9847 - val_loss: 0.1032 - val_accuracy: 0.9783\n",
      "Epoch 957/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0478 - accuracy: 0.9847 - val_loss: 0.1114 - val_accuracy: 0.9752\n",
      "Epoch 958/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0486 - accuracy: 0.9847 - val_loss: 0.1154 - val_accuracy: 0.9752\n",
      "Epoch 959/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0488 - accuracy: 0.9832 - val_loss: 0.1052 - val_accuracy: 0.9752\n",
      "Epoch 960/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0482 - accuracy: 0.9832 - val_loss: 0.1005 - val_accuracy: 0.9814\n",
      "Epoch 961/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0486 - accuracy: 0.9847 - val_loss: 0.1080 - val_accuracy: 0.9752\n",
      "Epoch 962/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0474 - accuracy: 0.9847 - val_loss: 0.1192 - val_accuracy: 0.9752\n",
      "Epoch 963/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0501 - accuracy: 0.9816 - val_loss: 0.1157 - val_accuracy: 0.9752\n",
      "Epoch 964/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0486 - accuracy: 0.9847 - val_loss: 0.1020 - val_accuracy: 0.9814\n",
      "Epoch 965/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0500 - accuracy: 0.9847 - val_loss: 0.1033 - val_accuracy: 0.9783\n",
      "Epoch 966/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0477 - accuracy: 0.9847 - val_loss: 0.1204 - val_accuracy: 0.9752\n",
      "Epoch 967/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0501 - accuracy: 0.9832 - val_loss: 0.1223 - val_accuracy: 0.9752\n",
      "Epoch 968/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0503 - accuracy: 0.9847 - val_loss: 0.1084 - val_accuracy: 0.9752\n",
      "Epoch 969/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0472 - accuracy: 0.9847 - val_loss: 0.0979 - val_accuracy: 0.9814\n",
      "Epoch 970/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0502 - accuracy: 0.9832 - val_loss: 0.1000 - val_accuracy: 0.9814\n",
      "Epoch 971/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0487 - accuracy: 0.9847 - val_loss: 0.1111 - val_accuracy: 0.9752\n",
      "Epoch 972/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0476 - accuracy: 0.9862 - val_loss: 0.1140 - val_accuracy: 0.9752\n",
      "Epoch 973/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0480 - accuracy: 0.9862 - val_loss: 0.1055 - val_accuracy: 0.9783\n",
      "Epoch 974/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0468 - accuracy: 0.9847 - val_loss: 0.0979 - val_accuracy: 0.9845\n",
      "Epoch 975/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0499 - accuracy: 0.9847 - val_loss: 0.1005 - val_accuracy: 0.9814\n",
      "Epoch 976/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0479 - accuracy: 0.9832 - val_loss: 0.1085 - val_accuracy: 0.9783\n",
      "Epoch 977/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0469 - accuracy: 0.9847 - val_loss: 0.1161 - val_accuracy: 0.9752\n",
      "Epoch 978/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0496 - accuracy: 0.9832 - val_loss: 0.1096 - val_accuracy: 0.9752\n",
      "Epoch 979/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0476 - accuracy: 0.9847 - val_loss: 0.0984 - val_accuracy: 0.9845\n",
      "Epoch 980/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0494 - accuracy: 0.9847 - val_loss: 0.1039 - val_accuracy: 0.9783\n",
      "Epoch 981/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0479 - accuracy: 0.9847 - val_loss: 0.1159 - val_accuracy: 0.9752\n",
      "Epoch 982/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0480 - accuracy: 0.9862 - val_loss: 0.1076 - val_accuracy: 0.9752\n",
      "Epoch 983/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0461 - accuracy: 0.9847 - val_loss: 0.0976 - val_accuracy: 0.9845\n",
      "Epoch 984/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0500 - accuracy: 0.9862 - val_loss: 0.0980 - val_accuracy: 0.9845\n",
      "Epoch 985/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0483 - accuracy: 0.9862 - val_loss: 0.1086 - val_accuracy: 0.9752\n",
      "Epoch 986/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0471 - accuracy: 0.9847 - val_loss: 0.1162 - val_accuracy: 0.9752\n",
      "Epoch 987/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0483 - accuracy: 0.9832 - val_loss: 0.1046 - val_accuracy: 0.9752\n",
      "Epoch 988/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0476 - accuracy: 0.9847 - val_loss: 0.0970 - val_accuracy: 0.9845\n",
      "Epoch 989/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0488 - accuracy: 0.9862 - val_loss: 0.1054 - val_accuracy: 0.9752\n",
      "Epoch 990/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0475 - accuracy: 0.9832 - val_loss: 0.1169 - val_accuracy: 0.9752\n",
      "Epoch 991/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0481 - accuracy: 0.9832 - val_loss: 0.1087 - val_accuracy: 0.9752\n",
      "Epoch 992/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0469 - accuracy: 0.9862 - val_loss: 0.1025 - val_accuracy: 0.9783\n",
      "Epoch 993/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0463 - accuracy: 0.9847 - val_loss: 0.1063 - val_accuracy: 0.9783\n",
      "Epoch 994/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0458 - accuracy: 0.9847 - val_loss: 0.1155 - val_accuracy: 0.9752\n",
      "Epoch 995/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0478 - accuracy: 0.9862 - val_loss: 0.1158 - val_accuracy: 0.9752\n",
      "Epoch 996/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0470 - accuracy: 0.9862 - val_loss: 0.1031 - val_accuracy: 0.9783\n",
      "Epoch 997/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0462 - accuracy: 0.9847 - val_loss: 0.0985 - val_accuracy: 0.9845\n",
      "Epoch 998/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0484 - accuracy: 0.9847 - val_loss: 0.1026 - val_accuracy: 0.9783\n",
      "Epoch 999/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0458 - accuracy: 0.9816 - val_loss: 0.1132 - val_accuracy: 0.9752\n",
      "Epoch 1000/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0473 - accuracy: 0.9862 - val_loss: 0.1145 - val_accuracy: 0.9752\n",
      "Epoch 1001/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0470 - accuracy: 0.9862 - val_loss: 0.1043 - val_accuracy: 0.9783\n",
      "Epoch 1002/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0462 - accuracy: 0.9847 - val_loss: 0.1025 - val_accuracy: 0.9783\n",
      "Epoch 1003/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0458 - accuracy: 0.9847 - val_loss: 0.1111 - val_accuracy: 0.9752\n",
      "Epoch 1004/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0469 - accuracy: 0.9862 - val_loss: 0.1143 - val_accuracy: 0.9752\n",
      "Epoch 1005/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0470 - accuracy: 0.9847 - val_loss: 0.1058 - val_accuracy: 0.9783\n",
      "Epoch 1006/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0457 - accuracy: 0.9847 - val_loss: 0.1018 - val_accuracy: 0.9814\n",
      "Epoch 1007/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0463 - accuracy: 0.9847 - val_loss: 0.1010 - val_accuracy: 0.9814\n",
      "Epoch 1008/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0456 - accuracy: 0.9847 - val_loss: 0.1114 - val_accuracy: 0.9752\n",
      "Epoch 1009/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0461 - accuracy: 0.9862 - val_loss: 0.1226 - val_accuracy: 0.9752\n",
      "Epoch 1010/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0493 - accuracy: 0.9832 - val_loss: 0.1066 - val_accuracy: 0.9752\n",
      "Epoch 1011/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0458 - accuracy: 0.9877 - val_loss: 0.0958 - val_accuracy: 0.9876\n",
      "Epoch 1012/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0510 - accuracy: 0.9893 - val_loss: 0.1019 - val_accuracy: 0.9783\n",
      "Epoch 1013/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0448 - accuracy: 0.9847 - val_loss: 0.1245 - val_accuracy: 0.9752\n",
      "Epoch 1014/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0500 - accuracy: 0.9832 - val_loss: 0.1253 - val_accuracy: 0.9752\n",
      "Epoch 1015/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0489 - accuracy: 0.9862 - val_loss: 0.1026 - val_accuracy: 0.9783\n",
      "Epoch 1016/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0466 - accuracy: 0.9847 - val_loss: 0.0971 - val_accuracy: 0.9845\n",
      "Epoch 1017/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0469 - accuracy: 0.9877 - val_loss: 0.1102 - val_accuracy: 0.9752\n",
      "Epoch 1018/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0447 - accuracy: 0.9862 - val_loss: 0.1344 - val_accuracy: 0.9752\n",
      "Epoch 1019/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0547 - accuracy: 0.9847 - val_loss: 0.1205 - val_accuracy: 0.9752\n",
      "Epoch 1020/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0457 - accuracy: 0.9847 - val_loss: 0.0948 - val_accuracy: 0.9845\n",
      "Epoch 1021/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0524 - accuracy: 0.9862 - val_loss: 0.0940 - val_accuracy: 0.9876\n",
      "Epoch 1022/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0535 - accuracy: 0.9801 - val_loss: 0.1105 - val_accuracy: 0.9752\n",
      "Epoch 1023/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0481 - accuracy: 0.9862 - val_loss: 0.1367 - val_accuracy: 0.9752\n",
      "Epoch 1024/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0545 - accuracy: 0.9847 - val_loss: 0.1153 - val_accuracy: 0.9752\n",
      "Epoch 1025/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0464 - accuracy: 0.9862 - val_loss: 0.0964 - val_accuracy: 0.9845\n",
      "Epoch 1026/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0475 - accuracy: 0.9862 - val_loss: 0.0962 - val_accuracy: 0.9845\n",
      "Epoch 1027/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0462 - accuracy: 0.9877 - val_loss: 0.1110 - val_accuracy: 0.9752\n",
      "Epoch 1028/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0460 - accuracy: 0.9847 - val_loss: 0.1262 - val_accuracy: 0.9752\n",
      "Epoch 1029/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0507 - accuracy: 0.9847 - val_loss: 0.1164 - val_accuracy: 0.9752\n",
      "Epoch 1030/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0467 - accuracy: 0.9847 - val_loss: 0.1032 - val_accuracy: 0.9783\n",
      "Epoch 1031/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0456 - accuracy: 0.9847 - val_loss: 0.0995 - val_accuracy: 0.9783\n",
      "Epoch 1032/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0449 - accuracy: 0.9847 - val_loss: 0.1080 - val_accuracy: 0.9752\n",
      "Epoch 1033/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0447 - accuracy: 0.9862 - val_loss: 0.1126 - val_accuracy: 0.9752\n",
      "Epoch 1034/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0458 - accuracy: 0.9862 - val_loss: 0.1017 - val_accuracy: 0.9783\n",
      "Epoch 1035/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0458 - accuracy: 0.9832 - val_loss: 0.0951 - val_accuracy: 0.9845\n",
      "Epoch 1036/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0487 - accuracy: 0.9877 - val_loss: 0.1037 - val_accuracy: 0.9783\n",
      "Epoch 1037/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0449 - accuracy: 0.9847 - val_loss: 0.1151 - val_accuracy: 0.9752\n",
      "Epoch 1038/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0457 - accuracy: 0.9862 - val_loss: 0.1106 - val_accuracy: 0.9783\n",
      "Epoch 1039/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0444 - accuracy: 0.9862 - val_loss: 0.1017 - val_accuracy: 0.9783\n",
      "Epoch 1040/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0453 - accuracy: 0.9847 - val_loss: 0.1011 - val_accuracy: 0.9814\n",
      "Epoch 1041/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0454 - accuracy: 0.9847 - val_loss: 0.1084 - val_accuracy: 0.9783\n",
      "Epoch 1042/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0446 - accuracy: 0.9862 - val_loss: 0.1044 - val_accuracy: 0.9783\n",
      "Epoch 1043/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0446 - accuracy: 0.9847 - val_loss: 0.0993 - val_accuracy: 0.9783\n",
      "Epoch 1044/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0446 - accuracy: 0.9847 - val_loss: 0.1052 - val_accuracy: 0.9783\n",
      "Epoch 1045/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0445 - accuracy: 0.9862 - val_loss: 0.1099 - val_accuracy: 0.9752\n",
      "Epoch 1046/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0446 - accuracy: 0.9862 - val_loss: 0.1024 - val_accuracy: 0.9783\n",
      "Epoch 1047/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0451 - accuracy: 0.9847 - val_loss: 0.0986 - val_accuracy: 0.9783\n",
      "Epoch 1048/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0446 - accuracy: 0.9847 - val_loss: 0.1071 - val_accuracy: 0.9752\n",
      "Epoch 1049/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0454 - accuracy: 0.9862 - val_loss: 0.1075 - val_accuracy: 0.9752\n",
      "Epoch 1050/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0450 - accuracy: 0.9862 - val_loss: 0.0999 - val_accuracy: 0.9783\n",
      "Epoch 1051/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0442 - accuracy: 0.9847 - val_loss: 0.1070 - val_accuracy: 0.9783\n",
      "Epoch 1052/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0435 - accuracy: 0.9862 - val_loss: 0.1186 - val_accuracy: 0.9752\n",
      "Epoch 1053/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0465 - accuracy: 0.9832 - val_loss: 0.1112 - val_accuracy: 0.9752\n",
      "Epoch 1054/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0436 - accuracy: 0.9862 - val_loss: 0.0975 - val_accuracy: 0.9845\n",
      "Epoch 1055/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0475 - accuracy: 0.9862 - val_loss: 0.0984 - val_accuracy: 0.9845\n",
      "Epoch 1056/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0470 - accuracy: 0.9862 - val_loss: 0.1115 - val_accuracy: 0.9752\n",
      "Epoch 1057/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0444 - accuracy: 0.9862 - val_loss: 0.1191 - val_accuracy: 0.9752\n",
      "Epoch 1058/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0456 - accuracy: 0.9847 - val_loss: 0.1091 - val_accuracy: 0.9783\n",
      "Epoch 1059/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0436 - accuracy: 0.9862 - val_loss: 0.0991 - val_accuracy: 0.9814\n",
      "Epoch 1060/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0454 - accuracy: 0.9877 - val_loss: 0.1022 - val_accuracy: 0.9783\n",
      "Epoch 1061/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0434 - accuracy: 0.9847 - val_loss: 0.1176 - val_accuracy: 0.9752\n",
      "Epoch 1062/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0461 - accuracy: 0.9847 - val_loss: 0.1127 - val_accuracy: 0.9752\n",
      "Epoch 1063/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 16us/step - loss: 0.0456 - accuracy: 0.9862 - val_loss: 0.1007 - val_accuracy: 0.9814\n",
      "Epoch 1064/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0439 - accuracy: 0.9877 - val_loss: 0.1062 - val_accuracy: 0.9783\n",
      "Epoch 1065/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0434 - accuracy: 0.9862 - val_loss: 0.1165 - val_accuracy: 0.9752\n",
      "Epoch 1066/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0450 - accuracy: 0.9847 - val_loss: 0.1117 - val_accuracy: 0.9752\n",
      "Epoch 1067/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0442 - accuracy: 0.9862 - val_loss: 0.1035 - val_accuracy: 0.9783\n",
      "Epoch 1068/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.1024 - val_accuracy: 0.9783\n",
      "Epoch 1069/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0433 - accuracy: 0.9847 - val_loss: 0.1047 - val_accuracy: 0.9783\n",
      "Epoch 1070/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.1066 - val_accuracy: 0.9783\n",
      "Epoch 1071/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0431 - accuracy: 0.9862 - val_loss: 0.1028 - val_accuracy: 0.9783\n",
      "Epoch 1072/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0435 - accuracy: 0.9847 - val_loss: 0.1022 - val_accuracy: 0.9783\n",
      "Epoch 1073/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0432 - accuracy: 0.9847 - val_loss: 0.1100 - val_accuracy: 0.9783\n",
      "Epoch 1074/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.1159 - val_accuracy: 0.9752\n",
      "Epoch 1075/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0445 - accuracy: 0.9847 - val_loss: 0.1175 - val_accuracy: 0.9752\n",
      "Epoch 1076/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0446 - accuracy: 0.9847 - val_loss: 0.1131 - val_accuracy: 0.9752\n",
      "Epoch 1077/3500\n",
      "653/653 [==============================] - 0s 35us/step - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.1016 - val_accuracy: 0.9783\n",
      "Epoch 1078/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0426 - accuracy: 0.9893 - val_loss: 0.0960 - val_accuracy: 0.9845\n",
      "Epoch 1079/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0461 - accuracy: 0.9862 - val_loss: 0.0989 - val_accuracy: 0.9845\n",
      "Epoch 1080/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0451 - accuracy: 0.9862 - val_loss: 0.1050 - val_accuracy: 0.9783\n",
      "Epoch 1081/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0428 - accuracy: 0.9862 - val_loss: 0.1030 - val_accuracy: 0.9783\n",
      "Epoch 1082/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0425 - accuracy: 0.9862 - val_loss: 0.1062 - val_accuracy: 0.9752\n",
      "Epoch 1083/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0426 - accuracy: 0.9862 - val_loss: 0.1102 - val_accuracy: 0.9752\n",
      "Epoch 1084/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0431 - accuracy: 0.9862 - val_loss: 0.1102 - val_accuracy: 0.9752\n",
      "Epoch 1085/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0431 - accuracy: 0.9862 - val_loss: 0.1053 - val_accuracy: 0.9752\n",
      "Epoch 1086/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0426 - accuracy: 0.9862 - val_loss: 0.1034 - val_accuracy: 0.9783\n",
      "Epoch 1087/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0423 - accuracy: 0.9862 - val_loss: 0.1064 - val_accuracy: 0.9752\n",
      "Epoch 1088/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0427 - accuracy: 0.9862 - val_loss: 0.1055 - val_accuracy: 0.9752\n",
      "Epoch 1089/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0421 - accuracy: 0.9862 - val_loss: 0.1003 - val_accuracy: 0.9845\n",
      "Epoch 1090/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0432 - accuracy: 0.9893 - val_loss: 0.1026 - val_accuracy: 0.9814\n",
      "Epoch 1091/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0424 - accuracy: 0.9862 - val_loss: 0.1110 - val_accuracy: 0.9752\n",
      "Epoch 1092/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0428 - accuracy: 0.9847 - val_loss: 0.1128 - val_accuracy: 0.9752\n",
      "Epoch 1093/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0434 - accuracy: 0.9847 - val_loss: 0.1012 - val_accuracy: 0.9845\n",
      "Epoch 1094/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0434 - accuracy: 0.9893 - val_loss: 0.0958 - val_accuracy: 0.9876\n",
      "Epoch 1095/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0468 - accuracy: 0.9877 - val_loss: 0.1076 - val_accuracy: 0.9752\n",
      "Epoch 1096/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0419 - accuracy: 0.9862 - val_loss: 0.1302 - val_accuracy: 0.9720\n",
      "Epoch 1097/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0490 - accuracy: 0.9847 - val_loss: 0.1149 - val_accuracy: 0.9752\n",
      "Epoch 1098/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0415 - accuracy: 0.9862 - val_loss: 0.0941 - val_accuracy: 0.9876\n",
      "Epoch 1099/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0493 - accuracy: 0.9847 - val_loss: 0.0946 - val_accuracy: 0.9845\n",
      "Epoch 1100/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0451 - accuracy: 0.9877 - val_loss: 0.1214 - val_accuracy: 0.9752\n",
      "Epoch 1101/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0452 - accuracy: 0.9862 - val_loss: 0.1397 - val_accuracy: 0.9720\n",
      "Epoch 1102/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0538 - accuracy: 0.9816 - val_loss: 0.1204 - val_accuracy: 0.9752\n",
      "Epoch 1103/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0450 - accuracy: 0.9877 - val_loss: 0.0963 - val_accuracy: 0.9814\n",
      "Epoch 1104/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0449 - accuracy: 0.9877 - val_loss: 0.0958 - val_accuracy: 0.9814\n",
      "Epoch 1105/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0448 - accuracy: 0.9847 - val_loss: 0.1065 - val_accuracy: 0.9783\n",
      "Epoch 1106/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0422 - accuracy: 0.9862 - val_loss: 0.1116 - val_accuracy: 0.9752\n",
      "Epoch 1107/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0427 - accuracy: 0.9862 - val_loss: 0.1047 - val_accuracy: 0.9783\n",
      "Epoch 1108/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0419 - accuracy: 0.9862 - val_loss: 0.0990 - val_accuracy: 0.9783\n",
      "Epoch 1109/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0424 - accuracy: 0.9847 - val_loss: 0.1003 - val_accuracy: 0.9783\n",
      "Epoch 1110/3500\n",
      "653/653 [==============================] - 0s 80us/step - loss: 0.0422 - accuracy: 0.9862 - val_loss: 0.0995 - val_accuracy: 0.9845\n",
      "Epoch 1111/3500\n",
      "653/653 [==============================] - 0s 59us/step - loss: 0.0423 - accuracy: 0.9908 - val_loss: 0.0995 - val_accuracy: 0.9845\n",
      "Epoch 1112/3500\n",
      "653/653 [==============================] - 0s 48us/step - loss: 0.0423 - accuracy: 0.9893 - val_loss: 0.1054 - val_accuracy: 0.9783\n",
      "Epoch 1113/3500\n",
      "653/653 [==============================] - 0s 37us/step - loss: 0.0416 - accuracy: 0.9862 - val_loss: 0.1120 - val_accuracy: 0.9783\n",
      "Epoch 1114/3500\n",
      "653/653 [==============================] - 0s 39us/step - loss: 0.0423 - accuracy: 0.9862 - val_loss: 0.1105 - val_accuracy: 0.9783\n",
      "Epoch 1115/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0419 - accuracy: 0.9862 - val_loss: 0.1041 - val_accuracy: 0.9783\n",
      "Epoch 1116/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0419 - accuracy: 0.9862 - val_loss: 0.1038 - val_accuracy: 0.9783\n",
      "Epoch 1117/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0410 - accuracy: 0.9862 - val_loss: 0.1143 - val_accuracy: 0.9752\n",
      "Epoch 1118/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0424 - accuracy: 0.9847 - val_loss: 0.1213 - val_accuracy: 0.9752\n",
      "Epoch 1119/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0443 - accuracy: 0.9862 - val_loss: 0.1143 - val_accuracy: 0.9752\n",
      "Epoch 1120/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0426 - accuracy: 0.9862 - val_loss: 0.1051 - val_accuracy: 0.9783\n",
      "Epoch 1121/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0411 - accuracy: 0.9862 - val_loss: 0.1010 - val_accuracy: 0.9814\n",
      "Epoch 1122/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0418 - accuracy: 0.9877 - val_loss: 0.1016 - val_accuracy: 0.9814\n",
      "Epoch 1123/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0412 - accuracy: 0.9877 - val_loss: 0.1095 - val_accuracy: 0.9783\n",
      "Epoch 1124/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0414 - accuracy: 0.9862 - val_loss: 0.1090 - val_accuracy: 0.9783\n",
      "Epoch 1125/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0415 - accuracy: 0.9862 - val_loss: 0.1043 - val_accuracy: 0.9783\n",
      "Epoch 1126/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0412 - accuracy: 0.9862 - val_loss: 0.1017 - val_accuracy: 0.9783\n",
      "Epoch 1127/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0413 - accuracy: 0.9877 - val_loss: 0.1004 - val_accuracy: 0.9814\n",
      "Epoch 1128/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0417 - accuracy: 0.9893 - val_loss: 0.1089 - val_accuracy: 0.9783\n",
      "Epoch 1129/3500\n",
      "653/653 [==============================] - 0s 52us/step - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.1110 - val_accuracy: 0.9783\n",
      "Epoch 1130/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 0.0969 - val_accuracy: 0.9845\n",
      "Epoch 1131/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0439 - accuracy: 0.9862 - val_loss: 0.0960 - val_accuracy: 0.9845\n",
      "Epoch 1132/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0434 - accuracy: 0.9877 - val_loss: 0.1099 - val_accuracy: 0.9752\n",
      "Epoch 1133/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0411 - accuracy: 0.9877 - val_loss: 0.1195 - val_accuracy: 0.9752\n",
      "Epoch 1134/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0441 - accuracy: 0.9877 - val_loss: 0.1147 - val_accuracy: 0.9752\n",
      "Epoch 1135/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0426 - accuracy: 0.9862 - val_loss: 0.1033 - val_accuracy: 0.9814\n",
      "Epoch 1136/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0410 - accuracy: 0.9862 - val_loss: 0.0985 - val_accuracy: 0.9876\n",
      "Epoch 1137/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0422 - accuracy: 0.9877 - val_loss: 0.1003 - val_accuracy: 0.9876\n",
      "Epoch 1138/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0412 - accuracy: 0.9877 - val_loss: 0.1041 - val_accuracy: 0.9814\n",
      "Epoch 1139/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 0.1089 - val_accuracy: 0.9752\n",
      "Epoch 1140/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0409 - accuracy: 0.9862 - val_loss: 0.1081 - val_accuracy: 0.9783\n",
      "Epoch 1141/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 0.1041 - val_accuracy: 0.9814\n",
      "Epoch 1142/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0409 - accuracy: 0.9893 - val_loss: 0.1032 - val_accuracy: 0.9845\n",
      "Epoch 1143/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0408 - accuracy: 0.9893 - val_loss: 0.1098 - val_accuracy: 0.9783\n",
      "Epoch 1144/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0409 - accuracy: 0.9847 - val_loss: 0.1131 - val_accuracy: 0.9752\n",
      "Epoch 1145/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0412 - accuracy: 0.9862 - val_loss: 0.1079 - val_accuracy: 0.9783\n",
      "Epoch 1146/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0411 - accuracy: 0.9877 - val_loss: 0.1065 - val_accuracy: 0.9783\n",
      "Epoch 1147/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0409 - accuracy: 0.9877 - val_loss: 0.1087 - val_accuracy: 0.9783\n",
      "Epoch 1148/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0405 - accuracy: 0.9862 - val_loss: 0.1044 - val_accuracy: 0.9814\n",
      "Epoch 1149/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0406 - accuracy: 0.9893 - val_loss: 0.1031 - val_accuracy: 0.9814\n",
      "Epoch 1150/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0405 - accuracy: 0.9877 - val_loss: 0.1053 - val_accuracy: 0.9783\n",
      "Epoch 1151/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0401 - accuracy: 0.9862 - val_loss: 0.1055 - val_accuracy: 0.9783\n",
      "Epoch 1152/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0400 - accuracy: 0.9862 - val_loss: 0.1051 - val_accuracy: 0.9783\n",
      "Epoch 1153/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0401 - accuracy: 0.9862 - val_loss: 0.1038 - val_accuracy: 0.9783\n",
      "Epoch 1154/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0406 - accuracy: 0.9862 - val_loss: 0.1064 - val_accuracy: 0.9783\n",
      "Epoch 1155/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0405 - accuracy: 0.9877 - val_loss: 0.1119 - val_accuracy: 0.9752\n",
      "Epoch 1156/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0410 - accuracy: 0.9877 - val_loss: 0.1048 - val_accuracy: 0.9783\n",
      "Epoch 1157/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0400 - accuracy: 0.9862 - val_loss: 0.1008 - val_accuracy: 0.9783\n",
      "Epoch 1158/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0401 - accuracy: 0.9893 - val_loss: 0.1042 - val_accuracy: 0.9783\n",
      "Epoch 1159/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0399 - accuracy: 0.9862 - val_loss: 0.1089 - val_accuracy: 0.9783\n",
      "Epoch 1160/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0401 - accuracy: 0.9862 - val_loss: 0.1059 - val_accuracy: 0.9783\n",
      "Epoch 1161/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0402 - accuracy: 0.9862 - val_loss: 0.1039 - val_accuracy: 0.9783\n",
      "Epoch 1162/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0403 - accuracy: 0.9893 - val_loss: 0.1057 - val_accuracy: 0.9783\n",
      "Epoch 1163/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0400 - accuracy: 0.9862 - val_loss: 0.1029 - val_accuracy: 0.9783\n",
      "Epoch 1164/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0397 - accuracy: 0.9877 - val_loss: 0.0986 - val_accuracy: 0.9845\n",
      "Epoch 1165/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0409 - accuracy: 0.9908 - val_loss: 0.1016 - val_accuracy: 0.9783\n",
      "Epoch 1166/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0400 - accuracy: 0.9862 - val_loss: 0.1114 - val_accuracy: 0.9752\n",
      "Epoch 1167/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0414 - accuracy: 0.9877 - val_loss: 0.1004 - val_accuracy: 0.9814\n",
      "Epoch 1168/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0410 - accuracy: 0.9862 - val_loss: 0.0933 - val_accuracy: 0.9876\n",
      "Epoch 1169/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0447 - accuracy: 0.9893 - val_loss: 0.1035 - val_accuracy: 0.9752\n",
      "Epoch 1170/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0411 - accuracy: 0.9877 - val_loss: 0.1149 - val_accuracy: 0.9752\n",
      "Epoch 1171/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0416 - accuracy: 0.9877 - val_loss: 0.1028 - val_accuracy: 0.9783\n",
      "Epoch 1172/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0410 - accuracy: 0.9877 - val_loss: 0.0993 - val_accuracy: 0.9845\n",
      "Epoch 1173/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 15us/step - loss: 0.0400 - accuracy: 0.9862 - val_loss: 0.1080 - val_accuracy: 0.9752\n",
      "Epoch 1174/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0401 - accuracy: 0.9877 - val_loss: 0.1080 - val_accuracy: 0.9752\n",
      "Epoch 1175/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0395 - accuracy: 0.9877 - val_loss: 0.0991 - val_accuracy: 0.9845\n",
      "Epoch 1176/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0408 - accuracy: 0.9877 - val_loss: 0.1006 - val_accuracy: 0.9845\n",
      "Epoch 1177/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0404 - accuracy: 0.9862 - val_loss: 0.1114 - val_accuracy: 0.9752\n",
      "Epoch 1178/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0402 - accuracy: 0.9877 - val_loss: 0.1086 - val_accuracy: 0.9783\n",
      "Epoch 1179/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0393 - accuracy: 0.9877 - val_loss: 0.1038 - val_accuracy: 0.9783\n",
      "Epoch 1180/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0392 - accuracy: 0.9862 - val_loss: 0.0990 - val_accuracy: 0.9845\n",
      "Epoch 1181/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0402 - accuracy: 0.9908 - val_loss: 0.1031 - val_accuracy: 0.9814\n",
      "Epoch 1182/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0397 - accuracy: 0.9862 - val_loss: 0.1094 - val_accuracy: 0.9752\n",
      "Epoch 1183/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0396 - accuracy: 0.9877 - val_loss: 0.0999 - val_accuracy: 0.9814\n",
      "Epoch 1184/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0396 - accuracy: 0.9893 - val_loss: 0.0950 - val_accuracy: 0.9876\n",
      "Epoch 1185/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0422 - accuracy: 0.9893 - val_loss: 0.1027 - val_accuracy: 0.9814\n",
      "Epoch 1186/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0387 - accuracy: 0.9877 - val_loss: 0.1171 - val_accuracy: 0.9752\n",
      "Epoch 1187/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0420 - accuracy: 0.9877 - val_loss: 0.1132 - val_accuracy: 0.9752\n",
      "Epoch 1188/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0400 - accuracy: 0.9877 - val_loss: 0.0979 - val_accuracy: 0.9845\n",
      "Epoch 1189/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0405 - accuracy: 0.9923 - val_loss: 0.1009 - val_accuracy: 0.9845\n",
      "Epoch 1190/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0384 - accuracy: 0.9893 - val_loss: 0.1202 - val_accuracy: 0.9752\n",
      "Epoch 1191/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0436 - accuracy: 0.9862 - val_loss: 0.1176 - val_accuracy: 0.9752\n",
      "Epoch 1192/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0425 - accuracy: 0.9877 - val_loss: 0.0997 - val_accuracy: 0.9814\n",
      "Epoch 1193/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0421 - accuracy: 0.9877 - val_loss: 0.1028 - val_accuracy: 0.9814\n",
      "Epoch 1194/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0404 - accuracy: 0.9877 - val_loss: 0.1118 - val_accuracy: 0.9783\n",
      "Epoch 1195/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0397 - accuracy: 0.9877 - val_loss: 0.1217 - val_accuracy: 0.9752\n",
      "Epoch 1196/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0413 - accuracy: 0.9862 - val_loss: 0.1132 - val_accuracy: 0.9783\n",
      "Epoch 1197/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0389 - accuracy: 0.9877 - val_loss: 0.0996 - val_accuracy: 0.9845\n",
      "Epoch 1198/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0399 - accuracy: 0.9893 - val_loss: 0.0975 - val_accuracy: 0.9845\n",
      "Epoch 1199/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0404 - accuracy: 0.9893 - val_loss: 0.1080 - val_accuracy: 0.9783\n",
      "Epoch 1200/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0387 - accuracy: 0.9877 - val_loss: 0.1209 - val_accuracy: 0.9752\n",
      "Epoch 1201/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0412 - accuracy: 0.9877 - val_loss: 0.1137 - val_accuracy: 0.9783\n",
      "Epoch 1202/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0392 - accuracy: 0.9877 - val_loss: 0.1031 - val_accuracy: 0.9845\n",
      "Epoch 1203/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0387 - accuracy: 0.9893 - val_loss: 0.1012 - val_accuracy: 0.9845\n",
      "Epoch 1204/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0396 - accuracy: 0.9893 - val_loss: 0.1044 - val_accuracy: 0.9814\n",
      "Epoch 1205/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0388 - accuracy: 0.9877 - val_loss: 0.1031 - val_accuracy: 0.9845\n",
      "Epoch 1206/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0385 - accuracy: 0.9908 - val_loss: 0.0995 - val_accuracy: 0.9845\n",
      "Epoch 1207/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0393 - accuracy: 0.9908 - val_loss: 0.1021 - val_accuracy: 0.9814\n",
      "Epoch 1208/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0382 - accuracy: 0.9893 - val_loss: 0.1146 - val_accuracy: 0.9752\n",
      "Epoch 1209/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0400 - accuracy: 0.9862 - val_loss: 0.1206 - val_accuracy: 0.9720\n",
      "Epoch 1210/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0405 - accuracy: 0.9877 - val_loss: 0.1016 - val_accuracy: 0.9845\n",
      "Epoch 1211/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0386 - accuracy: 0.9908 - val_loss: 0.0947 - val_accuracy: 0.9876\n",
      "Epoch 1212/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0450 - accuracy: 0.9893 - val_loss: 0.1040 - val_accuracy: 0.9783\n",
      "Epoch 1213/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0384 - accuracy: 0.9893 - val_loss: 0.1230 - val_accuracy: 0.9752\n",
      "Epoch 1214/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0413 - accuracy: 0.9877 - val_loss: 0.1125 - val_accuracy: 0.9783\n",
      "Epoch 1215/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0389 - accuracy: 0.9877 - val_loss: 0.0994 - val_accuracy: 0.9845\n",
      "Epoch 1216/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0397 - accuracy: 0.9893 - val_loss: 0.1020 - val_accuracy: 0.9845\n",
      "Epoch 1217/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.1108 - val_accuracy: 0.9783\n",
      "Epoch 1218/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0386 - accuracy: 0.9877 - val_loss: 0.1126 - val_accuracy: 0.9752\n",
      "Epoch 1219/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0391 - accuracy: 0.9877 - val_loss: 0.1050 - val_accuracy: 0.9783\n",
      "Epoch 1220/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0381 - accuracy: 0.9893 - val_loss: 0.1011 - val_accuracy: 0.9814\n",
      "Epoch 1221/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0382 - accuracy: 0.9877 - val_loss: 0.1015 - val_accuracy: 0.9814\n",
      "Epoch 1222/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0383 - accuracy: 0.9877 - val_loss: 0.1015 - val_accuracy: 0.9814\n",
      "Epoch 1223/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.0998 - val_accuracy: 0.9845\n",
      "Epoch 1224/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0388 - accuracy: 0.9893 - val_loss: 0.1057 - val_accuracy: 0.9783\n",
      "Epoch 1225/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0393 - accuracy: 0.9877 - val_loss: 0.1150 - val_accuracy: 0.9752\n",
      "Epoch 1226/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0387 - accuracy: 0.9877 - val_loss: 0.1042 - val_accuracy: 0.9783\n",
      "Epoch 1227/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0375 - accuracy: 0.9877 - val_loss: 0.0984 - val_accuracy: 0.9876\n",
      "Epoch 1228/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0388 - accuracy: 0.9923 - val_loss: 0.1009 - val_accuracy: 0.9814\n",
      "Epoch 1229/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0385 - accuracy: 0.9908 - val_loss: 0.1075 - val_accuracy: 0.9752\n",
      "Epoch 1230/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.1028 - val_accuracy: 0.9814\n",
      "Epoch 1231/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0382 - accuracy: 0.9893 - val_loss: 0.0989 - val_accuracy: 0.9876\n",
      "Epoch 1232/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0387 - accuracy: 0.9908 - val_loss: 0.1047 - val_accuracy: 0.9783\n",
      "Epoch 1233/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0405 - accuracy: 0.9877 - val_loss: 0.1025 - val_accuracy: 0.9814\n",
      "Epoch 1234/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0379 - accuracy: 0.9908 - val_loss: 0.0942 - val_accuracy: 0.9876\n",
      "Epoch 1235/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0424 - accuracy: 0.9908 - val_loss: 0.1026 - val_accuracy: 0.9814\n",
      "Epoch 1236/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0371 - accuracy: 0.9877 - val_loss: 0.1253 - val_accuracy: 0.9720\n",
      "Epoch 1237/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0431 - accuracy: 0.9877 - val_loss: 0.1122 - val_accuracy: 0.9752\n",
      "Epoch 1238/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0397 - accuracy: 0.9877 - val_loss: 0.0950 - val_accuracy: 0.9876\n",
      "Epoch 1239/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0445 - accuracy: 0.9893 - val_loss: 0.1007 - val_accuracy: 0.9845\n",
      "Epoch 1240/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0371 - accuracy: 0.9908 - val_loss: 0.1260 - val_accuracy: 0.9752\n",
      "Epoch 1241/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0421 - accuracy: 0.9862 - val_loss: 0.1312 - val_accuracy: 0.9720\n",
      "Epoch 1242/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0449 - accuracy: 0.9862 - val_loss: 0.1108 - val_accuracy: 0.9783\n",
      "Epoch 1243/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0380 - accuracy: 0.9877 - val_loss: 0.0977 - val_accuracy: 0.9845\n",
      "Epoch 1244/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0411 - accuracy: 0.9893 - val_loss: 0.0945 - val_accuracy: 0.9876\n",
      "Epoch 1245/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0392 - accuracy: 0.9923 - val_loss: 0.1180 - val_accuracy: 0.9752\n",
      "Epoch 1246/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0437 - accuracy: 0.9862 - val_loss: 0.1204 - val_accuracy: 0.9752\n",
      "Epoch 1247/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0410 - accuracy: 0.9877 - val_loss: 0.0941 - val_accuracy: 0.9876\n",
      "Epoch 1248/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0406 - accuracy: 0.9923 - val_loss: 0.0949 - val_accuracy: 0.9845\n",
      "Epoch 1249/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0388 - accuracy: 0.9923 - val_loss: 0.1163 - val_accuracy: 0.9752\n",
      "Epoch 1250/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0401 - accuracy: 0.9893 - val_loss: 0.1239 - val_accuracy: 0.9752\n",
      "Epoch 1251/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0413 - accuracy: 0.9877 - val_loss: 0.1035 - val_accuracy: 0.9783\n",
      "Epoch 1252/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0381 - accuracy: 0.9893 - val_loss: 0.0955 - val_accuracy: 0.9845\n",
      "Epoch 1253/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0388 - accuracy: 0.9908 - val_loss: 0.1007 - val_accuracy: 0.9783\n",
      "Epoch 1254/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0374 - accuracy: 0.9908 - val_loss: 0.1053 - val_accuracy: 0.9783\n",
      "Epoch 1255/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0373 - accuracy: 0.9893 - val_loss: 0.1028 - val_accuracy: 0.9783\n",
      "Epoch 1256/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0377 - accuracy: 0.9893 - val_loss: 0.1012 - val_accuracy: 0.9814\n",
      "Epoch 1257/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0374 - accuracy: 0.9893 - val_loss: 0.1057 - val_accuracy: 0.9783\n",
      "Epoch 1258/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0373 - accuracy: 0.9893 - val_loss: 0.1056 - val_accuracy: 0.9814\n",
      "Epoch 1259/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0369 - accuracy: 0.9893 - val_loss: 0.1082 - val_accuracy: 0.9783\n",
      "Epoch 1260/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0369 - accuracy: 0.9893 - val_loss: 0.1047 - val_accuracy: 0.9814\n",
      "Epoch 1261/3500\n",
      "653/653 [==============================] - 0s 30us/step - loss: 0.0370 - accuracy: 0.9908 - val_loss: 0.1037 - val_accuracy: 0.9814\n",
      "Epoch 1262/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0370 - accuracy: 0.9908 - val_loss: 0.1079 - val_accuracy: 0.9814\n",
      "Epoch 1263/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0373 - accuracy: 0.9893 - val_loss: 0.1049 - val_accuracy: 0.9814\n",
      "Epoch 1264/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0379 - accuracy: 0.9893 - val_loss: 0.1017 - val_accuracy: 0.9845\n",
      "Epoch 1265/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0370 - accuracy: 0.9908 - val_loss: 0.1090 - val_accuracy: 0.9814\n",
      "Epoch 1266/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0372 - accuracy: 0.9877 - val_loss: 0.1030 - val_accuracy: 0.9845\n",
      "Epoch 1267/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0363 - accuracy: 0.9893 - val_loss: 0.0946 - val_accuracy: 0.9876\n",
      "Epoch 1268/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0403 - accuracy: 0.9923 - val_loss: 0.0987 - val_accuracy: 0.9876\n",
      "Epoch 1269/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0377 - accuracy: 0.9908 - val_loss: 0.1128 - val_accuracy: 0.9752\n",
      "Epoch 1270/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0380 - accuracy: 0.9862 - val_loss: 0.1068 - val_accuracy: 0.9814\n",
      "Epoch 1271/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.1024 - val_accuracy: 0.9845\n",
      "Epoch 1272/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0363 - accuracy: 0.9908 - val_loss: 0.1150 - val_accuracy: 0.9752\n",
      "Epoch 1273/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0384 - accuracy: 0.9862 - val_loss: 0.1183 - val_accuracy: 0.9752\n",
      "Epoch 1274/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0380 - accuracy: 0.9862 - val_loss: 0.1020 - val_accuracy: 0.9876\n",
      "Epoch 1275/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0374 - accuracy: 0.9908 - val_loss: 0.0998 - val_accuracy: 0.9876\n",
      "Epoch 1276/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0379 - accuracy: 0.9923 - val_loss: 0.1094 - val_accuracy: 0.9814\n",
      "Epoch 1277/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0363 - accuracy: 0.9877 - val_loss: 0.1206 - val_accuracy: 0.9752\n",
      "Epoch 1278/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0387 - accuracy: 0.9877 - val_loss: 0.1148 - val_accuracy: 0.9783\n",
      "Epoch 1279/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0370 - accuracy: 0.9893 - val_loss: 0.1010 - val_accuracy: 0.9876\n",
      "Epoch 1280/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0367 - accuracy: 0.9923 - val_loss: 0.0980 - val_accuracy: 0.9876\n",
      "Epoch 1281/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0374 - accuracy: 0.9923 - val_loss: 0.1080 - val_accuracy: 0.9814\n",
      "Epoch 1282/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0362 - accuracy: 0.9893 - val_loss: 0.1224 - val_accuracy: 0.9752\n",
      "Epoch 1283/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 15us/step - loss: 0.0393 - accuracy: 0.9877 - val_loss: 0.1125 - val_accuracy: 0.9783\n",
      "Epoch 1284/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0357 - accuracy: 0.9877 - val_loss: 0.0956 - val_accuracy: 0.9876\n",
      "Epoch 1285/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0403 - accuracy: 0.9908 - val_loss: 0.0965 - val_accuracy: 0.9876\n",
      "Epoch 1286/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0407 - accuracy: 0.9893 - val_loss: 0.1064 - val_accuracy: 0.9814\n",
      "Epoch 1287/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0362 - accuracy: 0.9893 - val_loss: 0.1054 - val_accuracy: 0.9814\n",
      "Epoch 1288/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0359 - accuracy: 0.9908 - val_loss: 0.1081 - val_accuracy: 0.9814\n",
      "Epoch 1289/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0360 - accuracy: 0.9893 - val_loss: 0.1049 - val_accuracy: 0.9814\n",
      "Epoch 1290/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0360 - accuracy: 0.9923 - val_loss: 0.1026 - val_accuracy: 0.9845\n",
      "Epoch 1291/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0361 - accuracy: 0.9923 - val_loss: 0.1075 - val_accuracy: 0.9814\n",
      "Epoch 1292/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0358 - accuracy: 0.9877 - val_loss: 0.1081 - val_accuracy: 0.9783\n",
      "Epoch 1293/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0355 - accuracy: 0.9877 - val_loss: 0.1024 - val_accuracy: 0.9845\n",
      "Epoch 1294/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0368 - accuracy: 0.9923 - val_loss: 0.1059 - val_accuracy: 0.9814\n",
      "Epoch 1295/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0365 - accuracy: 0.9877 - val_loss: 0.1149 - val_accuracy: 0.9752\n",
      "Epoch 1296/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0365 - accuracy: 0.9862 - val_loss: 0.1055 - val_accuracy: 0.9814\n",
      "Epoch 1297/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0354 - accuracy: 0.9893 - val_loss: 0.1011 - val_accuracy: 0.9845\n",
      "Epoch 1298/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0361 - accuracy: 0.9908 - val_loss: 0.1032 - val_accuracy: 0.9814\n",
      "Epoch 1299/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0354 - accuracy: 0.9908 - val_loss: 0.1112 - val_accuracy: 0.9783\n",
      "Epoch 1300/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0360 - accuracy: 0.9877 - val_loss: 0.1156 - val_accuracy: 0.9752\n",
      "Epoch 1301/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0366 - accuracy: 0.9877 - val_loss: 0.1077 - val_accuracy: 0.9783\n",
      "Epoch 1302/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0353 - accuracy: 0.9908 - val_loss: 0.1053 - val_accuracy: 0.9814\n",
      "Epoch 1303/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0355 - accuracy: 0.9923 - val_loss: 0.1060 - val_accuracy: 0.9814\n",
      "Epoch 1304/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0353 - accuracy: 0.9908 - val_loss: 0.1054 - val_accuracy: 0.9814\n",
      "Epoch 1305/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0354 - accuracy: 0.9908 - val_loss: 0.1027 - val_accuracy: 0.9845\n",
      "Epoch 1306/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0353 - accuracy: 0.9908 - val_loss: 0.0987 - val_accuracy: 0.9876\n",
      "Epoch 1307/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0364 - accuracy: 0.9923 - val_loss: 0.1019 - val_accuracy: 0.9814\n",
      "Epoch 1308/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0359 - accuracy: 0.9908 - val_loss: 0.1058 - val_accuracy: 0.9783\n",
      "Epoch 1309/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0355 - accuracy: 0.9893 - val_loss: 0.1039 - val_accuracy: 0.9814\n",
      "Epoch 1310/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0353 - accuracy: 0.9893 - val_loss: 0.1079 - val_accuracy: 0.9752\n",
      "Epoch 1311/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0359 - accuracy: 0.9893 - val_loss: 0.1057 - val_accuracy: 0.9814\n",
      "Epoch 1312/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0355 - accuracy: 0.9893 - val_loss: 0.0998 - val_accuracy: 0.9876\n",
      "Epoch 1313/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0356 - accuracy: 0.9923 - val_loss: 0.1147 - val_accuracy: 0.9752\n",
      "Epoch 1314/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0365 - accuracy: 0.9877 - val_loss: 0.1316 - val_accuracy: 0.9720\n",
      "Epoch 1315/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0398 - accuracy: 0.9862 - val_loss: 0.1087 - val_accuracy: 0.9783\n",
      "Epoch 1316/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0359 - accuracy: 0.9893 - val_loss: 0.0970 - val_accuracy: 0.9876\n",
      "Epoch 1317/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0400 - accuracy: 0.9908 - val_loss: 0.1099 - val_accuracy: 0.9783\n",
      "Epoch 1318/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0358 - accuracy: 0.9877 - val_loss: 0.1341 - val_accuracy: 0.9720\n",
      "Epoch 1319/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0413 - accuracy: 0.9862 - val_loss: 0.1094 - val_accuracy: 0.9783\n",
      "Epoch 1320/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0370 - accuracy: 0.9893 - val_loss: 0.0958 - val_accuracy: 0.9876\n",
      "Epoch 1321/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0445 - accuracy: 0.9877 - val_loss: 0.1060 - val_accuracy: 0.9783\n",
      "Epoch 1322/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0339 - accuracy: 0.9908 - val_loss: 0.1353 - val_accuracy: 0.9720\n",
      "Epoch 1323/3500\n",
      "653/653 [==============================] - 0s 46us/step - loss: 0.0420 - accuracy: 0.9847 - val_loss: 0.1303 - val_accuracy: 0.9720\n",
      "Epoch 1324/3500\n",
      "653/653 [==============================] - 0s 74us/step - loss: 0.0400 - accuracy: 0.9877 - val_loss: 0.1029 - val_accuracy: 0.9845\n",
      "Epoch 1325/3500\n",
      "653/653 [==============================] - 0s 33us/step - loss: 0.0357 - accuracy: 0.9923 - val_loss: 0.0998 - val_accuracy: 0.9845\n",
      "Epoch 1326/3500\n",
      "653/653 [==============================] - 0s 32us/step - loss: 0.0352 - accuracy: 0.9923 - val_loss: 0.1138 - val_accuracy: 0.9752\n",
      "Epoch 1327/3500\n",
      "653/653 [==============================] - 0s 45us/step - loss: 0.0354 - accuracy: 0.9877 - val_loss: 0.1234 - val_accuracy: 0.9720\n",
      "Epoch 1328/3500\n",
      "653/653 [==============================] - 0s 78us/step - loss: 0.0379 - accuracy: 0.9893 - val_loss: 0.1118 - val_accuracy: 0.9783\n",
      "Epoch 1329/3500\n",
      "653/653 [==============================] - 0s 42us/step - loss: 0.0356 - accuracy: 0.9877 - val_loss: 0.1023 - val_accuracy: 0.9845\n",
      "Epoch 1330/3500\n",
      "653/653 [==============================] - 0s 36us/step - loss: 0.0353 - accuracy: 0.9923 - val_loss: 0.1004 - val_accuracy: 0.9876\n",
      "Epoch 1331/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0355 - accuracy: 0.9923 - val_loss: 0.0986 - val_accuracy: 0.9876\n",
      "Epoch 1332/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0360 - accuracy: 0.9923 - val_loss: 0.1054 - val_accuracy: 0.9814\n",
      "Epoch 1333/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0343 - accuracy: 0.9908 - val_loss: 0.1158 - val_accuracy: 0.9720\n",
      "Epoch 1334/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0360 - accuracy: 0.9877 - val_loss: 0.1083 - val_accuracy: 0.9814\n",
      "Epoch 1335/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0349 - accuracy: 0.9877 - val_loss: 0.0970 - val_accuracy: 0.9876\n",
      "Epoch 1336/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0373 - accuracy: 0.9923 - val_loss: 0.1006 - val_accuracy: 0.9876\n",
      "Epoch 1337/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0346 - accuracy: 0.9923 - val_loss: 0.1120 - val_accuracy: 0.9720\n",
      "Epoch 1338/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0349 - accuracy: 0.9893 - val_loss: 0.1243 - val_accuracy: 0.9720\n",
      "Epoch 1339/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.1115 - val_accuracy: 0.9720\n",
      "Epoch 1340/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0349 - accuracy: 0.9877 - val_loss: 0.0976 - val_accuracy: 0.9876\n",
      "Epoch 1341/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0366 - accuracy: 0.9923 - val_loss: 0.1051 - val_accuracy: 0.9814\n",
      "Epoch 1342/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0345 - accuracy: 0.9908 - val_loss: 0.1250 - val_accuracy: 0.9720\n",
      "Epoch 1343/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.1187 - val_accuracy: 0.9720\n",
      "Epoch 1344/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0362 - accuracy: 0.9877 - val_loss: 0.1071 - val_accuracy: 0.9783\n",
      "Epoch 1345/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0344 - accuracy: 0.9908 - val_loss: 0.1036 - val_accuracy: 0.9845\n",
      "Epoch 1346/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0345 - accuracy: 0.9908 - val_loss: 0.1069 - val_accuracy: 0.9783\n",
      "Epoch 1347/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0343 - accuracy: 0.9893 - val_loss: 0.1108 - val_accuracy: 0.9783\n",
      "Epoch 1348/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0345 - accuracy: 0.9877 - val_loss: 0.1062 - val_accuracy: 0.9814\n",
      "Epoch 1349/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0345 - accuracy: 0.9893 - val_loss: 0.1051 - val_accuracy: 0.9814\n",
      "Epoch 1350/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0342 - accuracy: 0.9893 - val_loss: 0.1074 - val_accuracy: 0.9783\n",
      "Epoch 1351/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0342 - accuracy: 0.9893 - val_loss: 0.1008 - val_accuracy: 0.9814\n",
      "Epoch 1352/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0341 - accuracy: 0.9923 - val_loss: 0.0960 - val_accuracy: 0.9876\n",
      "Epoch 1353/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0359 - accuracy: 0.9923 - val_loss: 0.1020 - val_accuracy: 0.9814\n",
      "Epoch 1354/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0345 - accuracy: 0.9908 - val_loss: 0.1142 - val_accuracy: 0.9752\n",
      "Epoch 1355/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0357 - accuracy: 0.9893 - val_loss: 0.1092 - val_accuracy: 0.9783\n",
      "Epoch 1356/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0346 - accuracy: 0.9893 - val_loss: 0.1009 - val_accuracy: 0.9814\n",
      "Epoch 1357/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0340 - accuracy: 0.9908 - val_loss: 0.0995 - val_accuracy: 0.9814\n",
      "Epoch 1358/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0346 - accuracy: 0.9908 - val_loss: 0.1087 - val_accuracy: 0.9783\n",
      "Epoch 1359/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0339 - accuracy: 0.9908 - val_loss: 0.1264 - val_accuracy: 0.9720\n",
      "Epoch 1360/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0390 - accuracy: 0.9893 - val_loss: 0.1149 - val_accuracy: 0.9752\n",
      "Epoch 1361/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0351 - accuracy: 0.9893 - val_loss: 0.0956 - val_accuracy: 0.9876\n",
      "Epoch 1362/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0371 - accuracy: 0.9923 - val_loss: 0.0999 - val_accuracy: 0.9814\n",
      "Epoch 1363/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0332 - accuracy: 0.9893 - val_loss: 0.1269 - val_accuracy: 0.9720\n",
      "Epoch 1364/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0395 - accuracy: 0.9893 - val_loss: 0.1242 - val_accuracy: 0.9720\n",
      "Epoch 1365/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0366 - accuracy: 0.9893 - val_loss: 0.0976 - val_accuracy: 0.9845\n",
      "Epoch 1366/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0373 - accuracy: 0.9908 - val_loss: 0.0947 - val_accuracy: 0.9876\n",
      "Epoch 1367/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0358 - accuracy: 0.9939 - val_loss: 0.1203 - val_accuracy: 0.9752\n",
      "Epoch 1368/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0368 - accuracy: 0.9893 - val_loss: 0.1345 - val_accuracy: 0.9720\n",
      "Epoch 1369/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0401 - accuracy: 0.9862 - val_loss: 0.1046 - val_accuracy: 0.9783\n",
      "Epoch 1370/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0368 - accuracy: 0.9908 - val_loss: 0.0924 - val_accuracy: 0.9876\n",
      "Epoch 1371/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0396 - accuracy: 0.9908 - val_loss: 0.1044 - val_accuracy: 0.9783\n",
      "Epoch 1372/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0357 - accuracy: 0.9893 - val_loss: 0.1146 - val_accuracy: 0.9752\n",
      "Epoch 1373/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0351 - accuracy: 0.9877 - val_loss: 0.1001 - val_accuracy: 0.9814\n",
      "Epoch 1374/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0338 - accuracy: 0.9908 - val_loss: 0.0950 - val_accuracy: 0.9876\n",
      "Epoch 1375/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0355 - accuracy: 0.9923 - val_loss: 0.0976 - val_accuracy: 0.9814\n",
      "Epoch 1376/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0341 - accuracy: 0.9923 - val_loss: 0.1071 - val_accuracy: 0.9783\n",
      "Epoch 1377/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0345 - accuracy: 0.9893 - val_loss: 0.1071 - val_accuracy: 0.9783\n",
      "Epoch 1378/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0338 - accuracy: 0.9893 - val_loss: 0.0966 - val_accuracy: 0.9845\n",
      "Epoch 1379/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0346 - accuracy: 0.9939 - val_loss: 0.1002 - val_accuracy: 0.9814\n",
      "Epoch 1380/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0342 - accuracy: 0.9908 - val_loss: 0.1110 - val_accuracy: 0.9783\n",
      "Epoch 1381/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0344 - accuracy: 0.9877 - val_loss: 0.1061 - val_accuracy: 0.9814\n",
      "Epoch 1382/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0333 - accuracy: 0.9877 - val_loss: 0.1006 - val_accuracy: 0.9814\n",
      "Epoch 1383/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0334 - accuracy: 0.9908 - val_loss: 0.0962 - val_accuracy: 0.9876\n",
      "Epoch 1384/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0356 - accuracy: 0.9923 - val_loss: 0.1062 - val_accuracy: 0.9814\n",
      "Epoch 1385/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0333 - accuracy: 0.9908 - val_loss: 0.1292 - val_accuracy: 0.9720\n",
      "Epoch 1386/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0386 - accuracy: 0.9862 - val_loss: 0.1138 - val_accuracy: 0.9783\n",
      "Epoch 1387/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0347 - accuracy: 0.9893 - val_loss: 0.0950 - val_accuracy: 0.9876\n",
      "Epoch 1388/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0364 - accuracy: 0.9923 - val_loss: 0.1006 - val_accuracy: 0.9814\n",
      "Epoch 1389/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0342 - accuracy: 0.9908 - val_loss: 0.1128 - val_accuracy: 0.9783\n",
      "Epoch 1390/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0338 - accuracy: 0.9877 - val_loss: 0.1054 - val_accuracy: 0.9814\n",
      "Epoch 1391/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0327 - accuracy: 0.9908 - val_loss: 0.0974 - val_accuracy: 0.9876\n",
      "Epoch 1392/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0344 - accuracy: 0.9923 - val_loss: 0.1004 - val_accuracy: 0.9814\n",
      "Epoch 1393/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 14us/step - loss: 0.0324 - accuracy: 0.9908 - val_loss: 0.1190 - val_accuracy: 0.9720\n",
      "Epoch 1394/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0365 - accuracy: 0.9893 - val_loss: 0.1193 - val_accuracy: 0.9720\n",
      "Epoch 1395/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0367 - accuracy: 0.9893 - val_loss: 0.0992 - val_accuracy: 0.9845\n",
      "Epoch 1396/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0341 - accuracy: 0.9923 - val_loss: 0.1022 - val_accuracy: 0.9814\n",
      "Epoch 1397/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0327 - accuracy: 0.9923 - val_loss: 0.1187 - val_accuracy: 0.9720\n",
      "Epoch 1398/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0371 - accuracy: 0.9893 - val_loss: 0.1065 - val_accuracy: 0.9783\n",
      "Epoch 1399/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0358 - accuracy: 0.9893 - val_loss: 0.0951 - val_accuracy: 0.9876\n",
      "Epoch 1400/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0354 - accuracy: 0.9923 - val_loss: 0.1142 - val_accuracy: 0.9720\n",
      "Epoch 1401/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0340 - accuracy: 0.9877 - val_loss: 0.1293 - val_accuracy: 0.9720\n",
      "Epoch 1402/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0379 - accuracy: 0.9893 - val_loss: 0.1067 - val_accuracy: 0.9814\n",
      "Epoch 1403/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0326 - accuracy: 0.9893 - val_loss: 0.0927 - val_accuracy: 0.9876\n",
      "Epoch 1404/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0402 - accuracy: 0.9908 - val_loss: 0.1044 - val_accuracy: 0.9814\n",
      "Epoch 1405/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0325 - accuracy: 0.9923 - val_loss: 0.1390 - val_accuracy: 0.9720\n",
      "Epoch 1406/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0419 - accuracy: 0.9862 - val_loss: 0.1241 - val_accuracy: 0.9720\n",
      "Epoch 1407/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0348 - accuracy: 0.9908 - val_loss: 0.0958 - val_accuracy: 0.9876\n",
      "Epoch 1408/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0347 - accuracy: 0.9923 - val_loss: 0.0923 - val_accuracy: 0.9876\n",
      "Epoch 1409/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0420 - accuracy: 0.9877 - val_loss: 0.1054 - val_accuracy: 0.9814\n",
      "Epoch 1410/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0320 - accuracy: 0.9877 - val_loss: 0.1302 - val_accuracy: 0.9720\n",
      "Epoch 1411/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0380 - accuracy: 0.9893 - val_loss: 0.1257 - val_accuracy: 0.9720\n",
      "Epoch 1412/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0354 - accuracy: 0.9893 - val_loss: 0.1014 - val_accuracy: 0.9845\n",
      "Epoch 1413/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0347 - accuracy: 0.9939 - val_loss: 0.0960 - val_accuracy: 0.9876\n",
      "Epoch 1414/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0340 - accuracy: 0.9923 - val_loss: 0.1198 - val_accuracy: 0.9720\n",
      "Epoch 1415/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0359 - accuracy: 0.9877 - val_loss: 0.1383 - val_accuracy: 0.9720\n",
      "Epoch 1416/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0395 - accuracy: 0.9862 - val_loss: 0.1020 - val_accuracy: 0.9814\n",
      "Epoch 1417/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0339 - accuracy: 0.9923 - val_loss: 0.0922 - val_accuracy: 0.9876\n",
      "Epoch 1418/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0383 - accuracy: 0.9893 - val_loss: 0.1102 - val_accuracy: 0.9752\n",
      "Epoch 1419/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0380 - accuracy: 0.9877 - val_loss: 0.1224 - val_accuracy: 0.9720\n",
      "Epoch 1420/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0367 - accuracy: 0.9893 - val_loss: 0.0969 - val_accuracy: 0.9876\n",
      "Epoch 1421/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0340 - accuracy: 0.9923 - val_loss: 0.0974 - val_accuracy: 0.9876\n",
      "Epoch 1422/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0333 - accuracy: 0.9908 - val_loss: 0.1136 - val_accuracy: 0.9720\n",
      "Epoch 1423/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0330 - accuracy: 0.9877 - val_loss: 0.1147 - val_accuracy: 0.9752\n",
      "Epoch 1424/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0330 - accuracy: 0.9877 - val_loss: 0.1040 - val_accuracy: 0.9814\n",
      "Epoch 1425/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0330 - accuracy: 0.9908 - val_loss: 0.1033 - val_accuracy: 0.9814\n",
      "Epoch 1426/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0325 - accuracy: 0.9908 - val_loss: 0.1109 - val_accuracy: 0.9783\n",
      "Epoch 1427/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0326 - accuracy: 0.9893 - val_loss: 0.1064 - val_accuracy: 0.9814\n",
      "Epoch 1428/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0320 - accuracy: 0.9908 - val_loss: 0.0975 - val_accuracy: 0.9876\n",
      "Epoch 1429/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0327 - accuracy: 0.9923 - val_loss: 0.0984 - val_accuracy: 0.9845\n",
      "Epoch 1430/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0322 - accuracy: 0.9923 - val_loss: 0.1077 - val_accuracy: 0.9814\n",
      "Epoch 1431/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0324 - accuracy: 0.9893 - val_loss: 0.1100 - val_accuracy: 0.9783\n",
      "Epoch 1432/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0331 - accuracy: 0.9877 - val_loss: 0.1060 - val_accuracy: 0.9814\n",
      "Epoch 1433/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0319 - accuracy: 0.9908 - val_loss: 0.1106 - val_accuracy: 0.9783\n",
      "Epoch 1434/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0324 - accuracy: 0.9893 - val_loss: 0.1130 - val_accuracy: 0.9783\n",
      "Epoch 1435/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0327 - accuracy: 0.9877 - val_loss: 0.1057 - val_accuracy: 0.9814\n",
      "Epoch 1436/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0322 - accuracy: 0.9908 - val_loss: 0.0986 - val_accuracy: 0.9876\n",
      "Epoch 1437/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0325 - accuracy: 0.9908 - val_loss: 0.1077 - val_accuracy: 0.9814\n",
      "Epoch 1438/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0347 - accuracy: 0.9893 - val_loss: 0.1071 - val_accuracy: 0.9814\n",
      "Epoch 1439/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0337 - accuracy: 0.9908 - val_loss: 0.0946 - val_accuracy: 0.9876\n",
      "Epoch 1440/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0341 - accuracy: 0.9923 - val_loss: 0.1136 - val_accuracy: 0.9783\n",
      "Epoch 1441/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0351 - accuracy: 0.9862 - val_loss: 0.1358 - val_accuracy: 0.9720\n",
      "Epoch 1442/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0381 - accuracy: 0.9862 - val_loss: 0.0986 - val_accuracy: 0.9876\n",
      "Epoch 1443/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0348 - accuracy: 0.9908 - val_loss: 0.0946 - val_accuracy: 0.9814\n",
      "Epoch 1444/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0478 - accuracy: 0.9832 - val_loss: 0.1095 - val_accuracy: 0.9783\n",
      "Epoch 1445/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0334 - accuracy: 0.9877 - val_loss: 0.1417 - val_accuracy: 0.9720\n",
      "Epoch 1446/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0416 - accuracy: 0.9847 - val_loss: 0.1139 - val_accuracy: 0.9752\n",
      "Epoch 1447/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0337 - accuracy: 0.9877 - val_loss: 0.0933 - val_accuracy: 0.9876\n",
      "Epoch 1448/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0387 - accuracy: 0.9923 - val_loss: 0.1017 - val_accuracy: 0.9814\n",
      "Epoch 1449/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0328 - accuracy: 0.9908 - val_loss: 0.1277 - val_accuracy: 0.9720\n",
      "Epoch 1450/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0371 - accuracy: 0.9893 - val_loss: 0.1152 - val_accuracy: 0.9720\n",
      "Epoch 1451/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0325 - accuracy: 0.9893 - val_loss: 0.0957 - val_accuracy: 0.9876\n",
      "Epoch 1452/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0342 - accuracy: 0.9923 - val_loss: 0.0929 - val_accuracy: 0.9876\n",
      "Epoch 1453/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0352 - accuracy: 0.9923 - val_loss: 0.1092 - val_accuracy: 0.9783\n",
      "Epoch 1454/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0318 - accuracy: 0.9893 - val_loss: 0.1191 - val_accuracy: 0.9720\n",
      "Epoch 1455/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0338 - accuracy: 0.9893 - val_loss: 0.1135 - val_accuracy: 0.9783\n",
      "Epoch 1456/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0328 - accuracy: 0.9893 - val_loss: 0.1018 - val_accuracy: 0.9814\n",
      "Epoch 1457/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0320 - accuracy: 0.9908 - val_loss: 0.1029 - val_accuracy: 0.9814\n",
      "Epoch 1458/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0317 - accuracy: 0.9908 - val_loss: 0.1106 - val_accuracy: 0.9783\n",
      "Epoch 1459/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0321 - accuracy: 0.9893 - val_loss: 0.1101 - val_accuracy: 0.9783\n",
      "Epoch 1460/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0318 - accuracy: 0.9908 - val_loss: 0.1042 - val_accuracy: 0.9814\n",
      "Epoch 1461/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0315 - accuracy: 0.9908 - val_loss: 0.1024 - val_accuracy: 0.9814\n",
      "Epoch 1462/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0318 - accuracy: 0.9908 - val_loss: 0.1021 - val_accuracy: 0.9814\n",
      "Epoch 1463/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0317 - accuracy: 0.9908 - val_loss: 0.1006 - val_accuracy: 0.9814\n",
      "Epoch 1464/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0322 - accuracy: 0.9908 - val_loss: 0.1024 - val_accuracy: 0.9814\n",
      "Epoch 1465/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0315 - accuracy: 0.9908 - val_loss: 0.1013 - val_accuracy: 0.9814\n",
      "Epoch 1466/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0311 - accuracy: 0.9908 - val_loss: 0.1109 - val_accuracy: 0.9752\n",
      "Epoch 1467/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0316 - accuracy: 0.9893 - val_loss: 0.1174 - val_accuracy: 0.9720\n",
      "Epoch 1468/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0329 - accuracy: 0.9908 - val_loss: 0.1089 - val_accuracy: 0.9783\n",
      "Epoch 1469/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0311 - accuracy: 0.9893 - val_loss: 0.0990 - val_accuracy: 0.9876\n",
      "Epoch 1470/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0323 - accuracy: 0.9923 - val_loss: 0.1052 - val_accuracy: 0.9814\n",
      "Epoch 1471/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0312 - accuracy: 0.9908 - val_loss: 0.1201 - val_accuracy: 0.9720\n",
      "Epoch 1472/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0336 - accuracy: 0.9893 - val_loss: 0.1101 - val_accuracy: 0.9783\n",
      "Epoch 1473/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0316 - accuracy: 0.9908 - val_loss: 0.0993 - val_accuracy: 0.9845\n",
      "Epoch 1474/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0316 - accuracy: 0.9923 - val_loss: 0.1014 - val_accuracy: 0.9845\n",
      "Epoch 1475/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0308 - accuracy: 0.9923 - val_loss: 0.1141 - val_accuracy: 0.9720\n",
      "Epoch 1476/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0323 - accuracy: 0.9877 - val_loss: 0.1202 - val_accuracy: 0.9720\n",
      "Epoch 1477/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0336 - accuracy: 0.9893 - val_loss: 0.1060 - val_accuracy: 0.9783\n",
      "Epoch 1478/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0325 - accuracy: 0.9877 - val_loss: 0.0978 - val_accuracy: 0.9876\n",
      "Epoch 1479/3500\n",
      "653/653 [==============================] - 0s 54us/step - loss: 0.0318 - accuracy: 0.9923 - val_loss: 0.1128 - val_accuracy: 0.9752\n",
      "Epoch 1480/3500\n",
      "653/653 [==============================] - 0s 58us/step - loss: 0.0345 - accuracy: 0.9893 - val_loss: 0.1112 - val_accuracy: 0.9752\n",
      "Epoch 1481/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0320 - accuracy: 0.9877 - val_loss: 0.0966 - val_accuracy: 0.9876\n",
      "Epoch 1482/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0323 - accuracy: 0.9923 - val_loss: 0.1055 - val_accuracy: 0.9783\n",
      "Epoch 1483/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0305 - accuracy: 0.9893 - val_loss: 0.1204 - val_accuracy: 0.9720\n",
      "Epoch 1484/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0340 - accuracy: 0.9893 - val_loss: 0.1135 - val_accuracy: 0.9720\n",
      "Epoch 1485/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0321 - accuracy: 0.9893 - val_loss: 0.0974 - val_accuracy: 0.9876\n",
      "Epoch 1486/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0327 - accuracy: 0.9923 - val_loss: 0.0965 - val_accuracy: 0.9876\n",
      "Epoch 1487/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0333 - accuracy: 0.9908 - val_loss: 0.1039 - val_accuracy: 0.9814\n",
      "Epoch 1488/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0308 - accuracy: 0.9908 - val_loss: 0.1102 - val_accuracy: 0.9783\n",
      "Epoch 1489/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0311 - accuracy: 0.9893 - val_loss: 0.1171 - val_accuracy: 0.9752\n",
      "Epoch 1490/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0324 - accuracy: 0.9893 - val_loss: 0.1099 - val_accuracy: 0.9783\n",
      "Epoch 1491/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0315 - accuracy: 0.9893 - val_loss: 0.1022 - val_accuracy: 0.9814\n",
      "Epoch 1492/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0314 - accuracy: 0.9908 - val_loss: 0.1016 - val_accuracy: 0.9814\n",
      "Epoch 1493/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0309 - accuracy: 0.9923 - val_loss: 0.0988 - val_accuracy: 0.9845\n",
      "Epoch 1494/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0312 - accuracy: 0.9923 - val_loss: 0.1040 - val_accuracy: 0.9814\n",
      "Epoch 1495/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0303 - accuracy: 0.9908 - val_loss: 0.1155 - val_accuracy: 0.9720\n",
      "Epoch 1496/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0322 - accuracy: 0.9893 - val_loss: 0.1193 - val_accuracy: 0.9720\n",
      "Epoch 1497/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0328 - accuracy: 0.9893 - val_loss: 0.0995 - val_accuracy: 0.9845\n",
      "Epoch 1498/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0362 - accuracy: 0.9908 - val_loss: 0.0978 - val_accuracy: 0.9876\n",
      "Epoch 1499/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0331 - accuracy: 0.9908 - val_loss: 0.1298 - val_accuracy: 0.9720\n",
      "Epoch 1500/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0365 - accuracy: 0.9893 - val_loss: 0.1185 - val_accuracy: 0.9720\n",
      "Epoch 1501/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0324 - accuracy: 0.9893 - val_loss: 0.0953 - val_accuracy: 0.9876\n",
      "Epoch 1502/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0338 - accuracy: 0.9923 - val_loss: 0.0962 - val_accuracy: 0.9876\n",
      "Epoch 1503/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 16us/step - loss: 0.0334 - accuracy: 0.9908 - val_loss: 0.1097 - val_accuracy: 0.9783\n",
      "Epoch 1504/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0311 - accuracy: 0.9893 - val_loss: 0.1110 - val_accuracy: 0.9783\n",
      "Epoch 1505/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0304 - accuracy: 0.9908 - val_loss: 0.0995 - val_accuracy: 0.9845\n",
      "Epoch 1506/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0318 - accuracy: 0.9923 - val_loss: 0.0991 - val_accuracy: 0.9876\n",
      "Epoch 1507/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0319 - accuracy: 0.9908 - val_loss: 0.1070 - val_accuracy: 0.9814\n",
      "Epoch 1508/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0302 - accuracy: 0.9893 - val_loss: 0.1021 - val_accuracy: 0.9845\n",
      "Epoch 1509/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0306 - accuracy: 0.9908 - val_loss: 0.1000 - val_accuracy: 0.9845\n",
      "Epoch 1510/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0312 - accuracy: 0.9923 - val_loss: 0.1066 - val_accuracy: 0.9783\n",
      "Epoch 1511/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0309 - accuracy: 0.9908 - val_loss: 0.1095 - val_accuracy: 0.9783\n",
      "Epoch 1512/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0315 - accuracy: 0.9893 - val_loss: 0.1107 - val_accuracy: 0.9752\n",
      "Epoch 1513/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0314 - accuracy: 0.9877 - val_loss: 0.1075 - val_accuracy: 0.9783\n",
      "Epoch 1514/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0298 - accuracy: 0.9908 - val_loss: 0.0954 - val_accuracy: 0.9876\n",
      "Epoch 1515/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0335 - accuracy: 0.9923 - val_loss: 0.0980 - val_accuracy: 0.9876\n",
      "Epoch 1516/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0309 - accuracy: 0.9908 - val_loss: 0.1160 - val_accuracy: 0.9720\n",
      "Epoch 1517/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0315 - accuracy: 0.9893 - val_loss: 0.1191 - val_accuracy: 0.9720\n",
      "Epoch 1518/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0315 - accuracy: 0.9893 - val_loss: 0.1033 - val_accuracy: 0.9814\n",
      "Epoch 1519/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0293 - accuracy: 0.9939 - val_loss: 0.0934 - val_accuracy: 0.9876\n",
      "Epoch 1520/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0393 - accuracy: 0.9893 - val_loss: 0.1035 - val_accuracy: 0.9814\n",
      "Epoch 1521/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0310 - accuracy: 0.9939 - val_loss: 0.1456 - val_accuracy: 0.9658\n",
      "Epoch 1522/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0414 - accuracy: 0.9847 - val_loss: 0.1203 - val_accuracy: 0.9720\n",
      "Epoch 1523/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0320 - accuracy: 0.9908 - val_loss: 0.0953 - val_accuracy: 0.9876\n",
      "Epoch 1524/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0332 - accuracy: 0.9923 - val_loss: 0.1024 - val_accuracy: 0.9814\n",
      "Epoch 1525/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0317 - accuracy: 0.9923 - val_loss: 0.1196 - val_accuracy: 0.9752\n",
      "Epoch 1526/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0318 - accuracy: 0.9908 - val_loss: 0.1036 - val_accuracy: 0.9814\n",
      "Epoch 1527/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0306 - accuracy: 0.9908 - val_loss: 0.0947 - val_accuracy: 0.9876\n",
      "Epoch 1528/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0331 - accuracy: 0.9923 - val_loss: 0.1003 - val_accuracy: 0.9814\n",
      "Epoch 1529/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0299 - accuracy: 0.9923 - val_loss: 0.1082 - val_accuracy: 0.9783\n",
      "Epoch 1530/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0299 - accuracy: 0.9893 - val_loss: 0.1134 - val_accuracy: 0.9752\n",
      "Epoch 1531/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0306 - accuracy: 0.9908 - val_loss: 0.1045 - val_accuracy: 0.9783\n",
      "Epoch 1532/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0302 - accuracy: 0.9908 - val_loss: 0.0987 - val_accuracy: 0.9845\n",
      "Epoch 1533/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0300 - accuracy: 0.9923 - val_loss: 0.1079 - val_accuracy: 0.9783\n",
      "Epoch 1534/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0295 - accuracy: 0.9893 - val_loss: 0.1173 - val_accuracy: 0.9720\n",
      "Epoch 1535/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0313 - accuracy: 0.9893 - val_loss: 0.1081 - val_accuracy: 0.9783\n",
      "Epoch 1536/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.0963 - val_accuracy: 0.9876\n",
      "Epoch 1537/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0318 - accuracy: 0.9923 - val_loss: 0.0972 - val_accuracy: 0.9876\n",
      "Epoch 1538/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0312 - accuracy: 0.9923 - val_loss: 0.1065 - val_accuracy: 0.9783\n",
      "Epoch 1539/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.1145 - val_accuracy: 0.9752\n",
      "Epoch 1540/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0305 - accuracy: 0.9893 - val_loss: 0.1114 - val_accuracy: 0.9752\n",
      "Epoch 1541/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0299 - accuracy: 0.9908 - val_loss: 0.1016 - val_accuracy: 0.9814\n",
      "Epoch 1542/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.1034 - val_accuracy: 0.9814\n",
      "Epoch 1543/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0292 - accuracy: 0.9923 - val_loss: 0.1083 - val_accuracy: 0.9783\n",
      "Epoch 1544/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0294 - accuracy: 0.9893 - val_loss: 0.1050 - val_accuracy: 0.9783\n",
      "Epoch 1545/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0297 - accuracy: 0.9893 - val_loss: 0.0946 - val_accuracy: 0.9876\n",
      "Epoch 1546/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0316 - accuracy: 0.9923 - val_loss: 0.0915 - val_accuracy: 0.9876\n",
      "Epoch 1547/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0347 - accuracy: 0.9908 - val_loss: 0.1048 - val_accuracy: 0.9783\n",
      "Epoch 1548/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0296 - accuracy: 0.9923 - val_loss: 0.1189 - val_accuracy: 0.9720\n",
      "Epoch 1549/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0318 - accuracy: 0.9893 - val_loss: 0.1075 - val_accuracy: 0.9783\n",
      "Epoch 1550/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0300 - accuracy: 0.9893 - val_loss: 0.0990 - val_accuracy: 0.9845\n",
      "Epoch 1551/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0293 - accuracy: 0.9908 - val_loss: 0.1041 - val_accuracy: 0.9783\n",
      "Epoch 1552/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0296 - accuracy: 0.9893 - val_loss: 0.1028 - val_accuracy: 0.9783\n",
      "Epoch 1553/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0301 - accuracy: 0.9893 - val_loss: 0.1011 - val_accuracy: 0.9814\n",
      "Epoch 1554/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0290 - accuracy: 0.9893 - val_loss: 0.1143 - val_accuracy: 0.9720\n",
      "Epoch 1555/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0303 - accuracy: 0.9893 - val_loss: 0.1119 - val_accuracy: 0.9752\n",
      "Epoch 1556/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0295 - accuracy: 0.9893 - val_loss: 0.0978 - val_accuracy: 0.9876\n",
      "Epoch 1557/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0299 - accuracy: 0.9923 - val_loss: 0.0927 - val_accuracy: 0.9876\n",
      "Epoch 1558/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0335 - accuracy: 0.9923 - val_loss: 0.1064 - val_accuracy: 0.9783\n",
      "Epoch 1559/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0290 - accuracy: 0.9908 - val_loss: 0.1265 - val_accuracy: 0.9720\n",
      "Epoch 1560/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0333 - accuracy: 0.9893 - val_loss: 0.1072 - val_accuracy: 0.9783\n",
      "Epoch 1561/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0290 - accuracy: 0.9923 - val_loss: 0.0937 - val_accuracy: 0.9876\n",
      "Epoch 1562/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0320 - accuracy: 0.9923 - val_loss: 0.1042 - val_accuracy: 0.9814\n",
      "Epoch 1563/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.1199 - val_accuracy: 0.9720\n",
      "Epoch 1564/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0315 - accuracy: 0.9893 - val_loss: 0.1110 - val_accuracy: 0.9752\n",
      "Epoch 1565/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0290 - accuracy: 0.9893 - val_loss: 0.1016 - val_accuracy: 0.9845\n",
      "Epoch 1566/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0288 - accuracy: 0.9923 - val_loss: 0.0979 - val_accuracy: 0.9845\n",
      "Epoch 1567/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.1040 - val_accuracy: 0.9814\n",
      "Epoch 1568/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0286 - accuracy: 0.9893 - val_loss: 0.1111 - val_accuracy: 0.9752\n",
      "Epoch 1569/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0290 - accuracy: 0.9893 - val_loss: 0.1077 - val_accuracy: 0.9814\n",
      "Epoch 1570/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0286 - accuracy: 0.9893 - val_loss: 0.1034 - val_accuracy: 0.9814\n",
      "Epoch 1571/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0287 - accuracy: 0.9923 - val_loss: 0.1065 - val_accuracy: 0.9814\n",
      "Epoch 1572/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0288 - accuracy: 0.9893 - val_loss: 0.1097 - val_accuracy: 0.9783\n",
      "Epoch 1573/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0293 - accuracy: 0.9893 - val_loss: 0.1052 - val_accuracy: 0.9783\n",
      "Epoch 1574/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.1094 - val_accuracy: 0.9783\n",
      "Epoch 1575/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0288 - accuracy: 0.9893 - val_loss: 0.1038 - val_accuracy: 0.9783\n",
      "Epoch 1576/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0282 - accuracy: 0.9923 - val_loss: 0.0969 - val_accuracy: 0.9876\n",
      "Epoch 1577/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0297 - accuracy: 0.9923 - val_loss: 0.1033 - val_accuracy: 0.9814\n",
      "Epoch 1578/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0277 - accuracy: 0.9923 - val_loss: 0.1249 - val_accuracy: 0.9720\n",
      "Epoch 1579/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0329 - accuracy: 0.9893 - val_loss: 0.1295 - val_accuracy: 0.9720\n",
      "Epoch 1580/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0331 - accuracy: 0.9877 - val_loss: 0.1006 - val_accuracy: 0.9845\n",
      "Epoch 1581/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0280 - accuracy: 0.9923 - val_loss: 0.0930 - val_accuracy: 0.9876\n",
      "Epoch 1582/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0342 - accuracy: 0.9923 - val_loss: 0.1003 - val_accuracy: 0.9845\n",
      "Epoch 1583/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0295 - accuracy: 0.9923 - val_loss: 0.1222 - val_accuracy: 0.9720\n",
      "Epoch 1584/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0314 - accuracy: 0.9893 - val_loss: 0.1167 - val_accuracy: 0.9752\n",
      "Epoch 1585/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0294 - accuracy: 0.9908 - val_loss: 0.1016 - val_accuracy: 0.9814\n",
      "Epoch 1586/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0284 - accuracy: 0.9923 - val_loss: 0.0959 - val_accuracy: 0.9876\n",
      "Epoch 1587/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0297 - accuracy: 0.9923 - val_loss: 0.1062 - val_accuracy: 0.9814\n",
      "Epoch 1588/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0283 - accuracy: 0.9923 - val_loss: 0.1189 - val_accuracy: 0.9752\n",
      "Epoch 1589/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0299 - accuracy: 0.9908 - val_loss: 0.1076 - val_accuracy: 0.9783\n",
      "Epoch 1590/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0275 - accuracy: 0.9908 - val_loss: 0.0945 - val_accuracy: 0.9876\n",
      "Epoch 1591/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0322 - accuracy: 0.9923 - val_loss: 0.0978 - val_accuracy: 0.9876\n",
      "Epoch 1592/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0287 - accuracy: 0.9908 - val_loss: 0.1197 - val_accuracy: 0.9752\n",
      "Epoch 1593/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0301 - accuracy: 0.9893 - val_loss: 0.1203 - val_accuracy: 0.9752\n",
      "Epoch 1594/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0310 - accuracy: 0.9862 - val_loss: 0.1065 - val_accuracy: 0.9814\n",
      "Epoch 1595/3500\n",
      "653/653 [==============================] - 0s 45us/step - loss: 0.0283 - accuracy: 0.9893 - val_loss: 0.1002 - val_accuracy: 0.9845\n",
      "Epoch 1596/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0291 - accuracy: 0.9923 - val_loss: 0.0975 - val_accuracy: 0.9876\n",
      "Epoch 1597/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0296 - accuracy: 0.9923 - val_loss: 0.1102 - val_accuracy: 0.9752\n",
      "Epoch 1598/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0277 - accuracy: 0.9893 - val_loss: 0.1237 - val_accuracy: 0.9720\n",
      "Epoch 1599/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0308 - accuracy: 0.9893 - val_loss: 0.1123 - val_accuracy: 0.9752\n",
      "Epoch 1600/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0274 - accuracy: 0.9893 - val_loss: 0.0965 - val_accuracy: 0.9876\n",
      "Epoch 1601/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0321 - accuracy: 0.9923 - val_loss: 0.1051 - val_accuracy: 0.9783\n",
      "Epoch 1602/3500\n",
      "653/653 [==============================] - 0s 32us/step - loss: 0.0305 - accuracy: 0.9893 - val_loss: 0.1264 - val_accuracy: 0.9720\n",
      "Epoch 1603/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0311 - accuracy: 0.9893 - val_loss: 0.1025 - val_accuracy: 0.9814\n",
      "Epoch 1604/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0309 - accuracy: 0.9923 - val_loss: 0.0982 - val_accuracy: 0.9876\n",
      "Epoch 1605/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0299 - accuracy: 0.9908 - val_loss: 0.1194 - val_accuracy: 0.9720\n",
      "Epoch 1606/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0295 - accuracy: 0.9877 - val_loss: 0.1257 - val_accuracy: 0.9720\n",
      "Epoch 1607/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0312 - accuracy: 0.9893 - val_loss: 0.1071 - val_accuracy: 0.9783\n",
      "Epoch 1608/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0289 - accuracy: 0.9923 - val_loss: 0.0965 - val_accuracy: 0.9876\n",
      "Epoch 1609/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0306 - accuracy: 0.9923 - val_loss: 0.1097 - val_accuracy: 0.9783\n",
      "Epoch 1610/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0273 - accuracy: 0.9923 - val_loss: 0.1258 - val_accuracy: 0.9720\n",
      "Epoch 1611/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0308 - accuracy: 0.9893 - val_loss: 0.1190 - val_accuracy: 0.9720\n",
      "Epoch 1612/3500\n",
      "653/653 [==============================] - 0s 61us/step - loss: 0.0284 - accuracy: 0.9893 - val_loss: 0.1020 - val_accuracy: 0.9845\n",
      "Epoch 1613/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 74us/step - loss: 0.0284 - accuracy: 0.9923 - val_loss: 0.0969 - val_accuracy: 0.9876\n",
      "Epoch 1614/3500\n",
      "653/653 [==============================] - 0s 124us/step - loss: 0.0302 - accuracy: 0.9923 - val_loss: 0.1093 - val_accuracy: 0.9783\n",
      "Epoch 1615/3500\n",
      "653/653 [==============================] - 0s 48us/step - loss: 0.0274 - accuracy: 0.9893 - val_loss: 0.1169 - val_accuracy: 0.9752\n",
      "Epoch 1616/3500\n",
      "653/653 [==============================] - 0s 69us/step - loss: 0.0287 - accuracy: 0.9908 - val_loss: 0.1114 - val_accuracy: 0.9783\n",
      "Epoch 1617/3500\n",
      "653/653 [==============================] - 0s 55us/step - loss: 0.0275 - accuracy: 0.9893 - val_loss: 0.1023 - val_accuracy: 0.9845\n",
      "Epoch 1618/3500\n",
      "653/653 [==============================] - 0s 37us/step - loss: 0.0279 - accuracy: 0.9923 - val_loss: 0.0990 - val_accuracy: 0.9876\n",
      "Epoch 1619/3500\n",
      "653/653 [==============================] - 0s 41us/step - loss: 0.0280 - accuracy: 0.9923 - val_loss: 0.1098 - val_accuracy: 0.9783\n",
      "Epoch 1620/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0276 - accuracy: 0.9893 - val_loss: 0.1150 - val_accuracy: 0.9752\n",
      "Epoch 1621/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0281 - accuracy: 0.9893 - val_loss: 0.1069 - val_accuracy: 0.9783\n",
      "Epoch 1622/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0273 - accuracy: 0.9923 - val_loss: 0.1037 - val_accuracy: 0.9814\n",
      "Epoch 1623/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0272 - accuracy: 0.9923 - val_loss: 0.1071 - val_accuracy: 0.9783\n",
      "Epoch 1624/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0271 - accuracy: 0.9923 - val_loss: 0.1113 - val_accuracy: 0.9783\n",
      "Epoch 1625/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0275 - accuracy: 0.9893 - val_loss: 0.1107 - val_accuracy: 0.9783\n",
      "Epoch 1626/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0279 - accuracy: 0.9893 - val_loss: 0.1070 - val_accuracy: 0.9814\n",
      "Epoch 1627/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0271 - accuracy: 0.9908 - val_loss: 0.1080 - val_accuracy: 0.9814\n",
      "Epoch 1628/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0270 - accuracy: 0.9923 - val_loss: 0.1031 - val_accuracy: 0.9814\n",
      "Epoch 1629/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0272 - accuracy: 0.9923 - val_loss: 0.1015 - val_accuracy: 0.9814\n",
      "Epoch 1630/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0274 - accuracy: 0.9908 - val_loss: 0.1059 - val_accuracy: 0.9814\n",
      "Epoch 1631/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 0.1137 - val_accuracy: 0.9752\n",
      "Epoch 1632/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0278 - accuracy: 0.9908 - val_loss: 0.1088 - val_accuracy: 0.9814\n",
      "Epoch 1633/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 0.1009 - val_accuracy: 0.9845\n",
      "Epoch 1634/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0273 - accuracy: 0.9923 - val_loss: 0.0995 - val_accuracy: 0.9845\n",
      "Epoch 1635/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0272 - accuracy: 0.9923 - val_loss: 0.1117 - val_accuracy: 0.9783\n",
      "Epoch 1636/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0282 - accuracy: 0.9893 - val_loss: 0.1175 - val_accuracy: 0.9752\n",
      "Epoch 1637/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0288 - accuracy: 0.9923 - val_loss: 0.1031 - val_accuracy: 0.9814\n",
      "Epoch 1638/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0272 - accuracy: 0.9908 - val_loss: 0.1002 - val_accuracy: 0.9845\n",
      "Epoch 1639/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0284 - accuracy: 0.9893 - val_loss: 0.0998 - val_accuracy: 0.9845\n",
      "Epoch 1640/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0271 - accuracy: 0.9908 - val_loss: 0.0949 - val_accuracy: 0.9876\n",
      "Epoch 1641/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0299 - accuracy: 0.9923 - val_loss: 0.0980 - val_accuracy: 0.9845\n",
      "Epoch 1642/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0275 - accuracy: 0.9923 - val_loss: 0.1061 - val_accuracy: 0.9814\n",
      "Epoch 1643/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0274 - accuracy: 0.9908 - val_loss: 0.1108 - val_accuracy: 0.9752\n",
      "Epoch 1644/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0269 - accuracy: 0.9893 - val_loss: 0.1010 - val_accuracy: 0.9845\n",
      "Epoch 1645/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0271 - accuracy: 0.9923 - val_loss: 0.1020 - val_accuracy: 0.9814\n",
      "Epoch 1646/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0277 - accuracy: 0.9923 - val_loss: 0.1049 - val_accuracy: 0.9814\n",
      "Epoch 1647/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 0.1030 - val_accuracy: 0.9814\n",
      "Epoch 1648/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0269 - accuracy: 0.9923 - val_loss: 0.1079 - val_accuracy: 0.9783\n",
      "Epoch 1649/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0267 - accuracy: 0.9893 - val_loss: 0.1001 - val_accuracy: 0.9845\n",
      "Epoch 1650/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0278 - accuracy: 0.9908 - val_loss: 0.0993 - val_accuracy: 0.9845\n",
      "Epoch 1651/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0286 - accuracy: 0.9893 - val_loss: 0.1055 - val_accuracy: 0.9814\n",
      "Epoch 1652/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 0.0963 - val_accuracy: 0.9876\n",
      "Epoch 1653/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0290 - accuracy: 0.9923 - val_loss: 0.0985 - val_accuracy: 0.9845\n",
      "Epoch 1654/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0280 - accuracy: 0.9893 - val_loss: 0.1155 - val_accuracy: 0.9752\n",
      "Epoch 1655/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0278 - accuracy: 0.9908 - val_loss: 0.1136 - val_accuracy: 0.9752\n",
      "Epoch 1656/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0274 - accuracy: 0.9908 - val_loss: 0.1069 - val_accuracy: 0.9783\n",
      "Epoch 1657/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0268 - accuracy: 0.9908 - val_loss: 0.1060 - val_accuracy: 0.9783\n",
      "Epoch 1658/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0266 - accuracy: 0.9908 - val_loss: 0.1075 - val_accuracy: 0.9783\n",
      "Epoch 1659/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0266 - accuracy: 0.9908 - val_loss: 0.1042 - val_accuracy: 0.9814\n",
      "Epoch 1660/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0267 - accuracy: 0.9908 - val_loss: 0.0999 - val_accuracy: 0.9845\n",
      "Epoch 1661/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0276 - accuracy: 0.9908 - val_loss: 0.1027 - val_accuracy: 0.9814\n",
      "Epoch 1662/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0257 - accuracy: 0.9923 - val_loss: 0.1218 - val_accuracy: 0.9752\n",
      "Epoch 1663/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0294 - accuracy: 0.9923 - val_loss: 0.1147 - val_accuracy: 0.9752\n",
      "Epoch 1664/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0268 - accuracy: 0.9908 - val_loss: 0.0958 - val_accuracy: 0.9876\n",
      "Epoch 1665/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0297 - accuracy: 0.9908 - val_loss: 0.0990 - val_accuracy: 0.9845\n",
      "Epoch 1666/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0283 - accuracy: 0.9923 - val_loss: 0.1177 - val_accuracy: 0.9752\n",
      "Epoch 1667/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0274 - accuracy: 0.9908 - val_loss: 0.1138 - val_accuracy: 0.9752\n",
      "Epoch 1668/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 0.1026 - val_accuracy: 0.9845\n",
      "Epoch 1669/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0264 - accuracy: 0.9923 - val_loss: 0.1016 - val_accuracy: 0.9845\n",
      "Epoch 1670/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0267 - accuracy: 0.9908 - val_loss: 0.1072 - val_accuracy: 0.9814\n",
      "Epoch 1671/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0263 - accuracy: 0.9908 - val_loss: 0.1165 - val_accuracy: 0.9752\n",
      "Epoch 1672/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0274 - accuracy: 0.9893 - val_loss: 0.1095 - val_accuracy: 0.9783\n",
      "Epoch 1673/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0261 - accuracy: 0.9908 - val_loss: 0.1014 - val_accuracy: 0.9845\n",
      "Epoch 1674/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0269 - accuracy: 0.9908 - val_loss: 0.0975 - val_accuracy: 0.9876\n",
      "Epoch 1675/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0283 - accuracy: 0.9923 - val_loss: 0.1013 - val_accuracy: 0.9845\n",
      "Epoch 1676/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0260 - accuracy: 0.9923 - val_loss: 0.1162 - val_accuracy: 0.9752\n",
      "Epoch 1677/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0267 - accuracy: 0.9908 - val_loss: 0.1176 - val_accuracy: 0.9752\n",
      "Epoch 1678/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0266 - accuracy: 0.9908 - val_loss: 0.1037 - val_accuracy: 0.9845\n",
      "Epoch 1679/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0269 - accuracy: 0.9908 - val_loss: 0.0997 - val_accuracy: 0.9845\n",
      "Epoch 1680/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0275 - accuracy: 0.9923 - val_loss: 0.1155 - val_accuracy: 0.9752\n",
      "Epoch 1681/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0269 - accuracy: 0.9923 - val_loss: 0.1258 - val_accuracy: 0.9720\n",
      "Epoch 1682/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0288 - accuracy: 0.9893 - val_loss: 0.1059 - val_accuracy: 0.9783\n",
      "Epoch 1683/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0258 - accuracy: 0.9923 - val_loss: 0.0971 - val_accuracy: 0.9876\n",
      "Epoch 1684/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0284 - accuracy: 0.9923 - val_loss: 0.1058 - val_accuracy: 0.9783\n",
      "Epoch 1685/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.1248 - val_accuracy: 0.9720\n",
      "Epoch 1686/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.1234 - val_accuracy: 0.9720\n",
      "Epoch 1687/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0282 - accuracy: 0.9908 - val_loss: 0.1035 - val_accuracy: 0.9814\n",
      "Epoch 1688/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0258 - accuracy: 0.9908 - val_loss: 0.0952 - val_accuracy: 0.9876\n",
      "Epoch 1689/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0290 - accuracy: 0.9923 - val_loss: 0.1049 - val_accuracy: 0.9814\n",
      "Epoch 1690/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0268 - accuracy: 0.9877 - val_loss: 0.1127 - val_accuracy: 0.9783\n",
      "Epoch 1691/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0268 - accuracy: 0.9877 - val_loss: 0.1083 - val_accuracy: 0.9814\n",
      "Epoch 1692/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0261 - accuracy: 0.9908 - val_loss: 0.1073 - val_accuracy: 0.9814\n",
      "Epoch 1693/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 0.1041 - val_accuracy: 0.9814\n",
      "Epoch 1694/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0256 - accuracy: 0.9908 - val_loss: 0.1113 - val_accuracy: 0.9783\n",
      "Epoch 1695/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0269 - accuracy: 0.9908 - val_loss: 0.1070 - val_accuracy: 0.9814\n",
      "Epoch 1696/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0267 - accuracy: 0.9923 - val_loss: 0.0989 - val_accuracy: 0.9845\n",
      "Epoch 1697/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0271 - accuracy: 0.9923 - val_loss: 0.1153 - val_accuracy: 0.9752\n",
      "Epoch 1698/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.1204 - val_accuracy: 0.9752\n",
      "Epoch 1699/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0261 - accuracy: 0.9923 - val_loss: 0.0977 - val_accuracy: 0.9876\n",
      "Epoch 1700/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0304 - accuracy: 0.9908 - val_loss: 0.0980 - val_accuracy: 0.9876\n",
      "Epoch 1701/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0265 - accuracy: 0.9939 - val_loss: 0.1328 - val_accuracy: 0.9720\n",
      "Epoch 1702/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0316 - accuracy: 0.9908 - val_loss: 0.1405 - val_accuracy: 0.9720\n",
      "Epoch 1703/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0327 - accuracy: 0.9862 - val_loss: 0.1030 - val_accuracy: 0.9814\n",
      "Epoch 1704/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0275 - accuracy: 0.9908 - val_loss: 0.0960 - val_accuracy: 0.9876\n",
      "Epoch 1705/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0297 - accuracy: 0.9893 - val_loss: 0.1092 - val_accuracy: 0.9783\n",
      "Epoch 1706/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0265 - accuracy: 0.9908 - val_loss: 0.1080 - val_accuracy: 0.9783\n",
      "Epoch 1707/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.0965 - val_accuracy: 0.9876\n",
      "Epoch 1708/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0288 - accuracy: 0.9923 - val_loss: 0.1060 - val_accuracy: 0.9814\n",
      "Epoch 1709/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0285 - accuracy: 0.9862 - val_loss: 0.1184 - val_accuracy: 0.9752\n",
      "Epoch 1710/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0267 - accuracy: 0.9908 - val_loss: 0.1006 - val_accuracy: 0.9845\n",
      "Epoch 1711/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 0.0986 - val_accuracy: 0.9845\n",
      "Epoch 1712/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0269 - accuracy: 0.9923 - val_loss: 0.1066 - val_accuracy: 0.9814\n",
      "Epoch 1713/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.1157 - val_accuracy: 0.9752\n",
      "Epoch 1714/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0262 - accuracy: 0.9908 - val_loss: 0.1122 - val_accuracy: 0.9752\n",
      "Epoch 1715/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0255 - accuracy: 0.9893 - val_loss: 0.1058 - val_accuracy: 0.9814\n",
      "Epoch 1716/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0256 - accuracy: 0.9923 - val_loss: 0.1062 - val_accuracy: 0.9814\n",
      "Epoch 1717/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0254 - accuracy: 0.9908 - val_loss: 0.1119 - val_accuracy: 0.9783\n",
      "Epoch 1718/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0255 - accuracy: 0.9908 - val_loss: 0.1091 - val_accuracy: 0.9814\n",
      "Epoch 1719/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.1081 - val_accuracy: 0.9814\n",
      "Epoch 1720/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.1100 - val_accuracy: 0.9783\n",
      "Epoch 1721/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.1095 - val_accuracy: 0.9783\n",
      "Epoch 1722/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.1059 - val_accuracy: 0.9814\n",
      "Epoch 1723/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 18us/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.1044 - val_accuracy: 0.9845\n",
      "Epoch 1724/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0250 - accuracy: 0.9908 - val_loss: 0.1128 - val_accuracy: 0.9783\n",
      "Epoch 1725/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0276 - accuracy: 0.9908 - val_loss: 0.1057 - val_accuracy: 0.9814\n",
      "Epoch 1726/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0273 - accuracy: 0.9923 - val_loss: 0.0965 - val_accuracy: 0.9876\n",
      "Epoch 1727/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.1164 - val_accuracy: 0.9752\n",
      "Epoch 1728/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0256 - accuracy: 0.9923 - val_loss: 0.1339 - val_accuracy: 0.9720\n",
      "Epoch 1729/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0308 - accuracy: 0.9908 - val_loss: 0.1198 - val_accuracy: 0.9752\n",
      "Epoch 1730/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0249 - accuracy: 0.9939 - val_loss: 0.0975 - val_accuracy: 0.9876\n",
      "Epoch 1731/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0302 - accuracy: 0.9908 - val_loss: 0.0996 - val_accuracy: 0.9845\n",
      "Epoch 1732/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0260 - accuracy: 0.9939 - val_loss: 0.1360 - val_accuracy: 0.9720\n",
      "Epoch 1733/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0317 - accuracy: 0.9908 - val_loss: 0.1401 - val_accuracy: 0.9720\n",
      "Epoch 1734/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0311 - accuracy: 0.9923 - val_loss: 0.1019 - val_accuracy: 0.9814\n",
      "Epoch 1735/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0267 - accuracy: 0.9923 - val_loss: 0.0944 - val_accuracy: 0.9814\n",
      "Epoch 1736/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0410 - accuracy: 0.9877 - val_loss: 0.1058 - val_accuracy: 0.9814\n",
      "Epoch 1737/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0249 - accuracy: 0.9908 - val_loss: 0.1384 - val_accuracy: 0.9720\n",
      "Epoch 1738/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0325 - accuracy: 0.9908 - val_loss: 0.1246 - val_accuracy: 0.9752\n",
      "Epoch 1739/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0271 - accuracy: 0.9923 - val_loss: 0.0959 - val_accuracy: 0.9876\n",
      "Epoch 1740/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0310 - accuracy: 0.9893 - val_loss: 0.1020 - val_accuracy: 0.9845\n",
      "Epoch 1741/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 0.1344 - val_accuracy: 0.9720\n",
      "Epoch 1742/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0305 - accuracy: 0.9908 - val_loss: 0.1239 - val_accuracy: 0.9752\n",
      "Epoch 1743/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0265 - accuracy: 0.9939 - val_loss: 0.1000 - val_accuracy: 0.9845\n",
      "Epoch 1744/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0289 - accuracy: 0.9923 - val_loss: 0.1066 - val_accuracy: 0.9814\n",
      "Epoch 1745/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0249 - accuracy: 0.9908 - val_loss: 0.1399 - val_accuracy: 0.9720\n",
      "Epoch 1746/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0325 - accuracy: 0.9893 - val_loss: 0.1258 - val_accuracy: 0.9752\n",
      "Epoch 1747/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 0.1009 - val_accuracy: 0.9845\n",
      "Epoch 1748/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0258 - accuracy: 0.9923 - val_loss: 0.0958 - val_accuracy: 0.9876\n",
      "Epoch 1749/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.1051 - val_accuracy: 0.9814\n",
      "Epoch 1750/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0262 - accuracy: 0.9893 - val_loss: 0.1194 - val_accuracy: 0.9752\n",
      "Epoch 1751/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0260 - accuracy: 0.9923 - val_loss: 0.1074 - val_accuracy: 0.9814\n",
      "Epoch 1752/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.1036 - val_accuracy: 0.9845\n",
      "Epoch 1753/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0248 - accuracy: 0.9908 - val_loss: 0.1129 - val_accuracy: 0.9752\n",
      "Epoch 1754/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.1170 - val_accuracy: 0.9752\n",
      "Epoch 1755/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0259 - accuracy: 0.9923 - val_loss: 0.1067 - val_accuracy: 0.9814\n",
      "Epoch 1756/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.1074 - val_accuracy: 0.9814\n",
      "Epoch 1757/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 0.1143 - val_accuracy: 0.9752\n",
      "Epoch 1758/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0251 - accuracy: 0.9908 - val_loss: 0.1164 - val_accuracy: 0.9752\n",
      "Epoch 1759/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.1077 - val_accuracy: 0.9814\n",
      "Epoch 1760/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.1046 - val_accuracy: 0.9845\n",
      "Epoch 1761/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.1116 - val_accuracy: 0.9814\n",
      "Epoch 1762/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0250 - accuracy: 0.9908 - val_loss: 0.1075 - val_accuracy: 0.9814\n",
      "Epoch 1763/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.1036 - val_accuracy: 0.9845\n",
      "Epoch 1764/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.1077 - val_accuracy: 0.9814\n",
      "Epoch 1765/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0250 - accuracy: 0.9908 - val_loss: 0.1046 - val_accuracy: 0.9814\n",
      "Epoch 1766/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 0.0993 - val_accuracy: 0.9845\n",
      "Epoch 1767/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0259 - accuracy: 0.9923 - val_loss: 0.1061 - val_accuracy: 0.9814\n",
      "Epoch 1768/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.1129 - val_accuracy: 0.9783\n",
      "Epoch 1769/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0246 - accuracy: 0.9939 - val_loss: 0.1158 - val_accuracy: 0.9752\n",
      "Epoch 1770/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0249 - accuracy: 0.9939 - val_loss: 0.1074 - val_accuracy: 0.9814\n",
      "Epoch 1771/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0245 - accuracy: 0.9923 - val_loss: 0.1015 - val_accuracy: 0.9814\n",
      "Epoch 1772/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0258 - accuracy: 0.9923 - val_loss: 0.1172 - val_accuracy: 0.9783\n",
      "Epoch 1773/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0263 - accuracy: 0.9923 - val_loss: 0.1358 - val_accuracy: 0.9720\n",
      "Epoch 1774/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0297 - accuracy: 0.9908 - val_loss: 0.1030 - val_accuracy: 0.9845\n",
      "Epoch 1775/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0276 - accuracy: 0.9877 - val_loss: 0.0947 - val_accuracy: 0.9814\n",
      "Epoch 1776/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0317 - accuracy: 0.9908 - val_loss: 0.1165 - val_accuracy: 0.9752\n",
      "Epoch 1777/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.1462 - val_accuracy: 0.9689\n",
      "Epoch 1778/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0345 - accuracy: 0.9862 - val_loss: 0.1160 - val_accuracy: 0.9783\n",
      "Epoch 1779/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0262 - accuracy: 0.9939 - val_loss: 0.0944 - val_accuracy: 0.9814\n",
      "Epoch 1780/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0360 - accuracy: 0.9877 - val_loss: 0.1036 - val_accuracy: 0.9814\n",
      "Epoch 1781/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0271 - accuracy: 0.9893 - val_loss: 0.1314 - val_accuracy: 0.9752\n",
      "Epoch 1782/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0291 - accuracy: 0.9908 - val_loss: 0.1055 - val_accuracy: 0.9814\n",
      "Epoch 1783/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0240 - accuracy: 0.9908 - val_loss: 0.0940 - val_accuracy: 0.9814\n",
      "Epoch 1784/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0370 - accuracy: 0.9877 - val_loss: 0.1164 - val_accuracy: 0.9783\n",
      "Epoch 1785/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0299 - accuracy: 0.9893 - val_loss: 0.1722 - val_accuracy: 0.9596\n",
      "Epoch 1786/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0449 - accuracy: 0.9847 - val_loss: 0.1141 - val_accuracy: 0.9783\n",
      "Epoch 1787/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.1055 - val_accuracy: 0.9689\n",
      "Epoch 1788/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0650 - accuracy: 0.9755 - val_loss: 0.1061 - val_accuracy: 0.9783\n",
      "Epoch 1789/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0289 - accuracy: 0.9893 - val_loss: 0.1731 - val_accuracy: 0.9596\n",
      "Epoch 1790/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0468 - accuracy: 0.9862 - val_loss: 0.1483 - val_accuracy: 0.9720\n",
      "Epoch 1791/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0347 - accuracy: 0.9908 - val_loss: 0.0974 - val_accuracy: 0.9814\n",
      "Epoch 1792/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0301 - accuracy: 0.9893 - val_loss: 0.0935 - val_accuracy: 0.9876\n",
      "Epoch 1793/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0318 - accuracy: 0.9908 - val_loss: 0.1216 - val_accuracy: 0.9783\n",
      "Epoch 1794/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0314 - accuracy: 0.9908 - val_loss: 0.1346 - val_accuracy: 0.9783\n",
      "Epoch 1795/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0321 - accuracy: 0.9893 - val_loss: 0.1074 - val_accuracy: 0.9783\n",
      "Epoch 1796/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0253 - accuracy: 0.9908 - val_loss: 0.1005 - val_accuracy: 0.9814\n",
      "Epoch 1797/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0255 - accuracy: 0.9908 - val_loss: 0.1090 - val_accuracy: 0.9783\n",
      "Epoch 1798/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0254 - accuracy: 0.9923 - val_loss: 0.1146 - val_accuracy: 0.9783\n",
      "Epoch 1799/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0258 - accuracy: 0.9923 - val_loss: 0.1072 - val_accuracy: 0.9783\n",
      "Epoch 1800/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0245 - accuracy: 0.9908 - val_loss: 0.1066 - val_accuracy: 0.9783\n",
      "Epoch 1801/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0245 - accuracy: 0.9908 - val_loss: 0.1017 - val_accuracy: 0.9814\n",
      "Epoch 1802/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0248 - accuracy: 0.9908 - val_loss: 0.0975 - val_accuracy: 0.9845\n",
      "Epoch 1803/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0271 - accuracy: 0.9923 - val_loss: 0.1118 - val_accuracy: 0.9783\n",
      "Epoch 1804/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0269 - accuracy: 0.9923 - val_loss: 0.1318 - val_accuracy: 0.9752\n",
      "Epoch 1805/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.1075 - val_accuracy: 0.9814\n",
      "Epoch 1806/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.0952 - val_accuracy: 0.9876\n",
      "Epoch 1807/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0291 - accuracy: 0.9893 - val_loss: 0.0963 - val_accuracy: 0.9876\n",
      "Epoch 1808/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0273 - accuracy: 0.9923 - val_loss: 0.1205 - val_accuracy: 0.9752\n",
      "Epoch 1809/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0257 - accuracy: 0.9908 - val_loss: 0.1351 - val_accuracy: 0.9720\n",
      "Epoch 1810/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.1154 - val_accuracy: 0.9752\n",
      "Epoch 1811/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.1064 - val_accuracy: 0.9814\n",
      "Epoch 1812/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0242 - accuracy: 0.9908 - val_loss: 0.1183 - val_accuracy: 0.9752\n",
      "Epoch 1813/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0274 - accuracy: 0.9908 - val_loss: 0.1168 - val_accuracy: 0.9752\n",
      "Epoch 1814/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.0985 - val_accuracy: 0.9876\n",
      "Epoch 1815/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.1066 - val_accuracy: 0.9814\n",
      "Epoch 1816/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0245 - accuracy: 0.9923 - val_loss: 0.1302 - val_accuracy: 0.9720\n",
      "Epoch 1817/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0277 - accuracy: 0.9908 - val_loss: 0.1260 - val_accuracy: 0.9752\n",
      "Epoch 1818/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0263 - accuracy: 0.9923 - val_loss: 0.1134 - val_accuracy: 0.9783\n",
      "Epoch 1819/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.1068 - val_accuracy: 0.9814\n",
      "Epoch 1820/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0242 - accuracy: 0.9908 - val_loss: 0.1105 - val_accuracy: 0.9814\n",
      "Epoch 1821/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0240 - accuracy: 0.9923 - val_loss: 0.1128 - val_accuracy: 0.9783\n",
      "Epoch 1822/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.1103 - val_accuracy: 0.9814\n",
      "Epoch 1823/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.1074 - val_accuracy: 0.9814\n",
      "Epoch 1824/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0247 - accuracy: 0.9908 - val_loss: 0.1044 - val_accuracy: 0.9845\n",
      "Epoch 1825/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0254 - accuracy: 0.9908 - val_loss: 0.1055 - val_accuracy: 0.9845\n",
      "Epoch 1826/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0239 - accuracy: 0.9923 - val_loss: 0.1182 - val_accuracy: 0.9783\n",
      "Epoch 1827/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 0.1151 - val_accuracy: 0.9783\n",
      "Epoch 1828/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 0.1076 - val_accuracy: 0.9845\n",
      "Epoch 1829/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0239 - accuracy: 0.9923 - val_loss: 0.1095 - val_accuracy: 0.9814\n",
      "Epoch 1830/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0240 - accuracy: 0.9923 - val_loss: 0.1167 - val_accuracy: 0.9783\n",
      "Epoch 1831/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.1251 - val_accuracy: 0.9752\n",
      "Epoch 1832/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0258 - accuracy: 0.9923 - val_loss: 0.1105 - val_accuracy: 0.9814\n",
      "Epoch 1833/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 16us/step - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.0976 - val_accuracy: 0.9876\n",
      "Epoch 1834/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0280 - accuracy: 0.9908 - val_loss: 0.1035 - val_accuracy: 0.9845\n",
      "Epoch 1835/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.1283 - val_accuracy: 0.9752\n",
      "Epoch 1836/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0273 - accuracy: 0.9923 - val_loss: 0.1239 - val_accuracy: 0.9752\n",
      "Epoch 1837/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0248 - accuracy: 0.9923 - val_loss: 0.1001 - val_accuracy: 0.9845\n",
      "Epoch 1838/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0270 - accuracy: 0.9923 - val_loss: 0.0994 - val_accuracy: 0.9876\n",
      "Epoch 1839/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0264 - accuracy: 0.9923 - val_loss: 0.1191 - val_accuracy: 0.9752\n",
      "Epoch 1840/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0257 - accuracy: 0.9908 - val_loss: 0.1191 - val_accuracy: 0.9752\n",
      "Epoch 1841/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.1014 - val_accuracy: 0.9845\n",
      "Epoch 1842/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 0.1090 - val_accuracy: 0.9814\n",
      "Epoch 1843/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0232 - accuracy: 0.9939 - val_loss: 0.1381 - val_accuracy: 0.9720\n",
      "Epoch 1844/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0306 - accuracy: 0.9908 - val_loss: 0.1243 - val_accuracy: 0.9752\n",
      "Epoch 1845/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0250 - accuracy: 0.9939 - val_loss: 0.0971 - val_accuracy: 0.9876\n",
      "Epoch 1846/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0280 - accuracy: 0.9893 - val_loss: 0.0962 - val_accuracy: 0.9876\n",
      "Epoch 1847/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0286 - accuracy: 0.9893 - val_loss: 0.1122 - val_accuracy: 0.9814\n",
      "Epoch 1848/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0240 - accuracy: 0.9908 - val_loss: 0.1241 - val_accuracy: 0.9783\n",
      "Epoch 1849/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0258 - accuracy: 0.9923 - val_loss: 0.1120 - val_accuracy: 0.9814\n",
      "Epoch 1850/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0236 - accuracy: 0.9908 - val_loss: 0.1001 - val_accuracy: 0.9845\n",
      "Epoch 1851/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0258 - accuracy: 0.9923 - val_loss: 0.1046 - val_accuracy: 0.9845\n",
      "Epoch 1852/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0241 - accuracy: 0.9908 - val_loss: 0.1180 - val_accuracy: 0.9783\n",
      "Epoch 1853/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 0.1128 - val_accuracy: 0.9814\n",
      "Epoch 1854/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 0.1016 - val_accuracy: 0.9845\n",
      "Epoch 1855/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0245 - accuracy: 0.9908 - val_loss: 0.1092 - val_accuracy: 0.9814\n",
      "Epoch 1856/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0232 - accuracy: 0.9939 - val_loss: 0.1227 - val_accuracy: 0.9752\n",
      "Epoch 1857/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0257 - accuracy: 0.9923 - val_loss: 0.1209 - val_accuracy: 0.9752\n",
      "Epoch 1858/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0248 - accuracy: 0.9923 - val_loss: 0.1037 - val_accuracy: 0.9845\n",
      "Epoch 1859/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.0979 - val_accuracy: 0.9845\n",
      "Epoch 1860/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 0.1080 - val_accuracy: 0.9814\n",
      "Epoch 1861/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0233 - accuracy: 0.9939 - val_loss: 0.1234 - val_accuracy: 0.9752\n",
      "Epoch 1862/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0257 - accuracy: 0.9923 - val_loss: 0.1137 - val_accuracy: 0.9783\n",
      "Epoch 1863/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0229 - accuracy: 0.9939 - val_loss: 0.0984 - val_accuracy: 0.9876\n",
      "Epoch 1864/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0268 - accuracy: 0.9908 - val_loss: 0.1006 - val_accuracy: 0.9845\n",
      "Epoch 1865/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0266 - accuracy: 0.9908 - val_loss: 0.1149 - val_accuracy: 0.9783\n",
      "Epoch 1866/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0235 - accuracy: 0.9954 - val_loss: 0.1106 - val_accuracy: 0.9814\n",
      "Epoch 1867/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1078 - val_accuracy: 0.9814\n",
      "Epoch 1868/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.1153 - val_accuracy: 0.9783\n",
      "Epoch 1869/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0237 - accuracy: 0.9954 - val_loss: 0.1157 - val_accuracy: 0.9783\n",
      "Epoch 1870/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0237 - accuracy: 0.9939 - val_loss: 0.1112 - val_accuracy: 0.9814\n",
      "Epoch 1871/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1042 - val_accuracy: 0.9845\n",
      "Epoch 1872/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.0988 - val_accuracy: 0.9845\n",
      "Epoch 1873/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0261 - accuracy: 0.9908 - val_loss: 0.1097 - val_accuracy: 0.9845\n",
      "Epoch 1874/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.1289 - val_accuracy: 0.9752\n",
      "Epoch 1875/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0272 - accuracy: 0.9908 - val_loss: 0.1163 - val_accuracy: 0.9783\n",
      "Epoch 1876/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0241 - accuracy: 0.9939 - val_loss: 0.0982 - val_accuracy: 0.9876\n",
      "Epoch 1877/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0271 - accuracy: 0.9908 - val_loss: 0.1036 - val_accuracy: 0.9845\n",
      "Epoch 1878/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0240 - accuracy: 0.9923 - val_loss: 0.1210 - val_accuracy: 0.9752\n",
      "Epoch 1879/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0252 - accuracy: 0.9923 - val_loss: 0.1238 - val_accuracy: 0.9752\n",
      "Epoch 1880/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0256 - accuracy: 0.9923 - val_loss: 0.1101 - val_accuracy: 0.9814\n",
      "Epoch 1881/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0232 - accuracy: 0.9908 - val_loss: 0.1029 - val_accuracy: 0.9845\n",
      "Epoch 1882/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0245 - accuracy: 0.9923 - val_loss: 0.1041 - val_accuracy: 0.9845\n",
      "Epoch 1883/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.1157 - val_accuracy: 0.9814\n",
      "Epoch 1884/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0252 - accuracy: 0.9954 - val_loss: 0.1165 - val_accuracy: 0.9783\n",
      "Epoch 1885/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0228 - accuracy: 0.9939 - val_loss: 0.1011 - val_accuracy: 0.9845\n",
      "Epoch 1886/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0261 - accuracy: 0.9908 - val_loss: 0.1041 - val_accuracy: 0.9845\n",
      "Epoch 1887/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.1243 - val_accuracy: 0.9752\n",
      "Epoch 1888/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.1161 - val_accuracy: 0.9783\n",
      "Epoch 1889/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0235 - accuracy: 0.9939 - val_loss: 0.1000 - val_accuracy: 0.9845\n",
      "Epoch 1890/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 0.1018 - val_accuracy: 0.9845\n",
      "Epoch 1891/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.1124 - val_accuracy: 0.9814\n",
      "Epoch 1892/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0233 - accuracy: 0.9939 - val_loss: 0.1190 - val_accuracy: 0.9783\n",
      "Epoch 1893/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0239 - accuracy: 0.9939 - val_loss: 0.1135 - val_accuracy: 0.9814\n",
      "Epoch 1894/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0232 - accuracy: 0.9939 - val_loss: 0.1062 - val_accuracy: 0.9845\n",
      "Epoch 1895/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.1026 - val_accuracy: 0.9845\n",
      "Epoch 1896/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0245 - accuracy: 0.9908 - val_loss: 0.1090 - val_accuracy: 0.9845\n",
      "Epoch 1897/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.1178 - val_accuracy: 0.9814\n",
      "Epoch 1898/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1174 - val_accuracy: 0.9783\n",
      "Epoch 1899/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1118 - val_accuracy: 0.9814\n",
      "Epoch 1900/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0230 - accuracy: 0.9939 - val_loss: 0.1089 - val_accuracy: 0.9845\n",
      "Epoch 1901/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0231 - accuracy: 0.9908 - val_loss: 0.1069 - val_accuracy: 0.9845\n",
      "Epoch 1902/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0232 - accuracy: 0.9908 - val_loss: 0.1077 - val_accuracy: 0.9845\n",
      "Epoch 1903/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0234 - accuracy: 0.9908 - val_loss: 0.1161 - val_accuracy: 0.9783\n",
      "Epoch 1904/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0237 - accuracy: 0.9939 - val_loss: 0.1210 - val_accuracy: 0.9783\n",
      "Epoch 1905/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 0.1068 - val_accuracy: 0.9845\n",
      "Epoch 1906/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0236 - accuracy: 0.9908 - val_loss: 0.0983 - val_accuracy: 0.9876\n",
      "Epoch 1907/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0273 - accuracy: 0.9908 - val_loss: 0.1150 - val_accuracy: 0.9783\n",
      "Epoch 1908/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0245 - accuracy: 0.9954 - val_loss: 0.1401 - val_accuracy: 0.9720\n",
      "Epoch 1909/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0291 - accuracy: 0.9908 - val_loss: 0.1136 - val_accuracy: 0.9783\n",
      "Epoch 1910/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0224 - accuracy: 0.9939 - val_loss: 0.0984 - val_accuracy: 0.9845\n",
      "Epoch 1911/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0297 - accuracy: 0.9893 - val_loss: 0.1136 - val_accuracy: 0.9783\n",
      "Epoch 1912/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0241 - accuracy: 0.9908 - val_loss: 0.1546 - val_accuracy: 0.9689\n",
      "Epoch 1913/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0345 - accuracy: 0.9877 - val_loss: 0.1218 - val_accuracy: 0.9752\n",
      "Epoch 1914/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0224 - accuracy: 0.9939 - val_loss: 0.0984 - val_accuracy: 0.9814\n",
      "Epoch 1915/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0383 - accuracy: 0.9877 - val_loss: 0.1006 - val_accuracy: 0.9845\n",
      "Epoch 1916/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0246 - accuracy: 0.9923 - val_loss: 0.1454 - val_accuracy: 0.9720\n",
      "Epoch 1917/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0343 - accuracy: 0.9893 - val_loss: 0.1539 - val_accuracy: 0.9658\n",
      "Epoch 1918/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0330 - accuracy: 0.9877 - val_loss: 0.1022 - val_accuracy: 0.9845\n",
      "Epoch 1919/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0259 - accuracy: 0.9923 - val_loss: 0.0957 - val_accuracy: 0.9814\n",
      "Epoch 1920/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0370 - accuracy: 0.9893 - val_loss: 0.1121 - val_accuracy: 0.9783\n",
      "Epoch 1921/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0232 - accuracy: 0.9939 - val_loss: 0.1486 - val_accuracy: 0.9689\n",
      "Epoch 1922/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0341 - accuracy: 0.9877 - val_loss: 0.1269 - val_accuracy: 0.9783\n",
      "Epoch 1923/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0232 - accuracy: 0.9954 - val_loss: 0.0949 - val_accuracy: 0.9845\n",
      "Epoch 1924/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0325 - accuracy: 0.9877 - val_loss: 0.0952 - val_accuracy: 0.9845\n",
      "Epoch 1925/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0301 - accuracy: 0.9908 - val_loss: 0.1170 - val_accuracy: 0.9783\n",
      "Epoch 1926/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0238 - accuracy: 0.9939 - val_loss: 0.1281 - val_accuracy: 0.9783\n",
      "Epoch 1927/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 0.1130 - val_accuracy: 0.9783\n",
      "Epoch 1928/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0225 - accuracy: 0.9923 - val_loss: 0.1006 - val_accuracy: 0.9845\n",
      "Epoch 1929/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.1019 - val_accuracy: 0.9845\n",
      "Epoch 1930/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.1187 - val_accuracy: 0.9783\n",
      "Epoch 1931/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0236 - accuracy: 0.9939 - val_loss: 0.1216 - val_accuracy: 0.9752\n",
      "Epoch 1932/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.1098 - val_accuracy: 0.9814\n",
      "Epoch 1933/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0229 - accuracy: 0.9908 - val_loss: 0.1060 - val_accuracy: 0.9814\n",
      "Epoch 1934/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0236 - accuracy: 0.9908 - val_loss: 0.1061 - val_accuracy: 0.9845\n",
      "Epoch 1935/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0234 - accuracy: 0.9908 - val_loss: 0.1072 - val_accuracy: 0.9845\n",
      "Epoch 1936/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0232 - accuracy: 0.9908 - val_loss: 0.1181 - val_accuracy: 0.9752\n",
      "Epoch 1937/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.1132 - val_accuracy: 0.9814\n",
      "Epoch 1938/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0230 - accuracy: 0.9939 - val_loss: 0.1055 - val_accuracy: 0.9845\n",
      "Epoch 1939/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0228 - accuracy: 0.9908 - val_loss: 0.1109 - val_accuracy: 0.9814\n",
      "Epoch 1940/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0228 - accuracy: 0.9939 - val_loss: 0.1164 - val_accuracy: 0.9752\n",
      "Epoch 1941/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0232 - accuracy: 0.9939 - val_loss: 0.1071 - val_accuracy: 0.9814\n",
      "Epoch 1942/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0226 - accuracy: 0.9908 - val_loss: 0.1037 - val_accuracy: 0.9845\n",
      "Epoch 1943/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 18us/step - loss: 0.0231 - accuracy: 0.9908 - val_loss: 0.1074 - val_accuracy: 0.9814\n",
      "Epoch 1944/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0225 - accuracy: 0.9923 - val_loss: 0.1162 - val_accuracy: 0.9814\n",
      "Epoch 1945/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0233 - accuracy: 0.9954 - val_loss: 0.1205 - val_accuracy: 0.9752\n",
      "Epoch 1946/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0239 - accuracy: 0.9923 - val_loss: 0.1070 - val_accuracy: 0.9845\n",
      "Epoch 1947/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0266 - accuracy: 0.9893 - val_loss: 0.1121 - val_accuracy: 0.9814\n",
      "Epoch 1948/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0266 - accuracy: 0.9939 - val_loss: 0.1310 - val_accuracy: 0.9752\n",
      "Epoch 1949/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0264 - accuracy: 0.9923 - val_loss: 0.1068 - val_accuracy: 0.9814\n",
      "Epoch 1950/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0223 - accuracy: 0.9908 - val_loss: 0.0987 - val_accuracy: 0.9845\n",
      "Epoch 1951/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0259 - accuracy: 0.9893 - val_loss: 0.1069 - val_accuracy: 0.9814\n",
      "Epoch 1952/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0240 - accuracy: 0.9893 - val_loss: 0.1231 - val_accuracy: 0.9783\n",
      "Epoch 1953/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0241 - accuracy: 0.9923 - val_loss: 0.1085 - val_accuracy: 0.9814\n",
      "Epoch 1954/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0227 - accuracy: 0.9923 - val_loss: 0.0988 - val_accuracy: 0.9845\n",
      "Epoch 1955/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 0.1109 - val_accuracy: 0.9814\n",
      "Epoch 1956/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1193 - val_accuracy: 0.9783\n",
      "Epoch 1957/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.1068 - val_accuracy: 0.9845\n",
      "Epoch 1958/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0232 - accuracy: 0.9908 - val_loss: 0.1050 - val_accuracy: 0.9845\n",
      "Epoch 1959/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0230 - accuracy: 0.9908 - val_loss: 0.1157 - val_accuracy: 0.9783\n",
      "Epoch 1960/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0225 - accuracy: 0.9954 - val_loss: 0.1268 - val_accuracy: 0.9752\n",
      "Epoch 1961/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.1181 - val_accuracy: 0.9783\n",
      "Epoch 1962/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0230 - accuracy: 0.9939 - val_loss: 0.1015 - val_accuracy: 0.9845\n",
      "Epoch 1963/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 0.1017 - val_accuracy: 0.9845\n",
      "Epoch 1964/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0249 - accuracy: 0.9908 - val_loss: 0.1093 - val_accuracy: 0.9814\n",
      "Epoch 1965/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0224 - accuracy: 0.9908 - val_loss: 0.1112 - val_accuracy: 0.9814\n",
      "Epoch 1966/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0225 - accuracy: 0.9939 - val_loss: 0.1095 - val_accuracy: 0.9814\n",
      "Epoch 1967/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0225 - accuracy: 0.9923 - val_loss: 0.1069 - val_accuracy: 0.9845\n",
      "Epoch 1968/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0223 - accuracy: 0.9908 - val_loss: 0.1142 - val_accuracy: 0.9814\n",
      "Epoch 1969/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0225 - accuracy: 0.9954 - val_loss: 0.1203 - val_accuracy: 0.9783\n",
      "Epoch 1970/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.1136 - val_accuracy: 0.9814\n",
      "Epoch 1971/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0233 - accuracy: 0.9939 - val_loss: 0.1066 - val_accuracy: 0.9845\n",
      "Epoch 1972/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0223 - accuracy: 0.9908 - val_loss: 0.1179 - val_accuracy: 0.9783\n",
      "Epoch 1973/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.1204 - val_accuracy: 0.9752\n",
      "Epoch 1974/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0230 - accuracy: 0.9939 - val_loss: 0.1042 - val_accuracy: 0.9845\n",
      "Epoch 1975/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0233 - accuracy: 0.9908 - val_loss: 0.1022 - val_accuracy: 0.9845\n",
      "Epoch 1976/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.1153 - val_accuracy: 0.9814\n",
      "Epoch 1977/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0225 - accuracy: 0.9954 - val_loss: 0.1246 - val_accuracy: 0.9752\n",
      "Epoch 1978/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.1090 - val_accuracy: 0.9814\n",
      "Epoch 1979/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0244 - accuracy: 0.9908 - val_loss: 0.1003 - val_accuracy: 0.9845\n",
      "Epoch 1980/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0247 - accuracy: 0.9923 - val_loss: 0.1203 - val_accuracy: 0.9752\n",
      "Epoch 1981/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0239 - accuracy: 0.9939 - val_loss: 0.1247 - val_accuracy: 0.9752\n",
      "Epoch 1982/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0250 - accuracy: 0.9908 - val_loss: 0.1107 - val_accuracy: 0.9814\n",
      "Epoch 1983/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0223 - accuracy: 0.9908 - val_loss: 0.1123 - val_accuracy: 0.9814\n",
      "Epoch 1984/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.1164 - val_accuracy: 0.9783\n",
      "Epoch 1985/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0223 - accuracy: 0.9954 - val_loss: 0.1193 - val_accuracy: 0.9783\n",
      "Epoch 1986/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0227 - accuracy: 0.9954 - val_loss: 0.1144 - val_accuracy: 0.9814\n",
      "Epoch 1987/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0223 - accuracy: 0.9954 - val_loss: 0.1094 - val_accuracy: 0.9845\n",
      "Epoch 1988/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0222 - accuracy: 0.9923 - val_loss: 0.1122 - val_accuracy: 0.9814\n",
      "Epoch 1989/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0220 - accuracy: 0.9954 - val_loss: 0.1180 - val_accuracy: 0.9783\n",
      "Epoch 1990/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0227 - accuracy: 0.9954 - val_loss: 0.1147 - val_accuracy: 0.9783\n",
      "Epoch 1991/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0220 - accuracy: 0.9954 - val_loss: 0.1034 - val_accuracy: 0.9845\n",
      "Epoch 1992/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0253 - accuracy: 0.9893 - val_loss: 0.1120 - val_accuracy: 0.9814\n",
      "Epoch 1993/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1400 - val_accuracy: 0.9720\n",
      "Epoch 1994/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0285 - accuracy: 0.9908 - val_loss: 0.1205 - val_accuracy: 0.9752\n",
      "Epoch 1995/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0224 - accuracy: 0.9939 - val_loss: 0.1000 - val_accuracy: 0.9845\n",
      "Epoch 1996/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0255 - accuracy: 0.9893 - val_loss: 0.1045 - val_accuracy: 0.9845\n",
      "Epoch 1997/3500\n",
      "653/653 [==============================] - 0s 43us/step - loss: 0.0223 - accuracy: 0.9908 - val_loss: 0.1275 - val_accuracy: 0.9783\n",
      "Epoch 1998/3500\n",
      "653/653 [==============================] - 0s 45us/step - loss: 0.0264 - accuracy: 0.9908 - val_loss: 0.1186 - val_accuracy: 0.9783\n",
      "Epoch 1999/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0224 - accuracy: 0.9939 - val_loss: 0.0988 - val_accuracy: 0.9876\n",
      "Epoch 2000/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0270 - accuracy: 0.9893 - val_loss: 0.1104 - val_accuracy: 0.9814\n",
      "Epoch 2001/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.1390 - val_accuracy: 0.9752\n",
      "Epoch 2002/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.1210 - val_accuracy: 0.9783\n",
      "Epoch 2003/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.1000 - val_accuracy: 0.9845\n",
      "Epoch 2004/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0256 - accuracy: 0.9908 - val_loss: 0.1062 - val_accuracy: 0.9814\n",
      "Epoch 2005/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.1232 - val_accuracy: 0.9783\n",
      "Epoch 2006/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.1187 - val_accuracy: 0.9783\n",
      "Epoch 2007/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0218 - accuracy: 0.9939 - val_loss: 0.1007 - val_accuracy: 0.9845\n",
      "Epoch 2008/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0272 - accuracy: 0.9893 - val_loss: 0.1021 - val_accuracy: 0.9845\n",
      "Epoch 2009/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0227 - accuracy: 0.9939 - val_loss: 0.1376 - val_accuracy: 0.9752\n",
      "Epoch 2010/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0276 - accuracy: 0.9908 - val_loss: 0.1473 - val_accuracy: 0.9689\n",
      "Epoch 2011/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0304 - accuracy: 0.9908 - val_loss: 0.1053 - val_accuracy: 0.9845\n",
      "Epoch 2012/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0256 - accuracy: 0.9893 - val_loss: 0.0962 - val_accuracy: 0.9814\n",
      "Epoch 2013/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0374 - accuracy: 0.9877 - val_loss: 0.1184 - val_accuracy: 0.9814\n",
      "Epoch 2014/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0241 - accuracy: 0.9939 - val_loss: 0.1549 - val_accuracy: 0.9658\n",
      "Epoch 2015/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0340 - accuracy: 0.9862 - val_loss: 0.1212 - val_accuracy: 0.9783\n",
      "Epoch 2016/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0212 - accuracy: 0.9939 - val_loss: 0.0956 - val_accuracy: 0.9814\n",
      "Epoch 2017/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0350 - accuracy: 0.9877 - val_loss: 0.1000 - val_accuracy: 0.9845\n",
      "Epoch 2018/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0257 - accuracy: 0.9908 - val_loss: 0.1312 - val_accuracy: 0.9752\n",
      "Epoch 2019/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.1329 - val_accuracy: 0.9752\n",
      "Epoch 2020/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0254 - accuracy: 0.9923 - val_loss: 0.1134 - val_accuracy: 0.9814\n",
      "Epoch 2021/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.1038 - val_accuracy: 0.9845\n",
      "Epoch 2022/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0231 - accuracy: 0.9923 - val_loss: 0.1166 - val_accuracy: 0.9783\n",
      "Epoch 2023/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0225 - accuracy: 0.9939 - val_loss: 0.1197 - val_accuracy: 0.9783\n",
      "Epoch 2024/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1057 - val_accuracy: 0.9814\n",
      "Epoch 2025/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0230 - accuracy: 0.9908 - val_loss: 0.1034 - val_accuracy: 0.9845\n",
      "Epoch 2026/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0244 - accuracy: 0.9908 - val_loss: 0.1122 - val_accuracy: 0.9814\n",
      "Epoch 2027/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.1128 - val_accuracy: 0.9814\n",
      "Epoch 2028/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.1093 - val_accuracy: 0.9814\n",
      "Epoch 2029/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0228 - accuracy: 0.9908 - val_loss: 0.1120 - val_accuracy: 0.9814\n",
      "Epoch 2030/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.1243 - val_accuracy: 0.9752\n",
      "Epoch 2031/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0234 - accuracy: 0.9939 - val_loss: 0.1162 - val_accuracy: 0.9814\n",
      "Epoch 2032/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0225 - accuracy: 0.9939 - val_loss: 0.1114 - val_accuracy: 0.9845\n",
      "Epoch 2033/3500\n",
      "653/653 [==============================] - 0s 45us/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.1099 - val_accuracy: 0.9845\n",
      "Epoch 2034/3500\n",
      "653/653 [==============================] - 0s 43us/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.1047 - val_accuracy: 0.9845\n",
      "Epoch 2035/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.1105 - val_accuracy: 0.9814\n",
      "Epoch 2036/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.1310 - val_accuracy: 0.9752\n",
      "Epoch 2037/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.1269 - val_accuracy: 0.9752\n",
      "Epoch 2038/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1064 - val_accuracy: 0.9845\n",
      "Epoch 2039/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 0.1048 - val_accuracy: 0.9845\n",
      "Epoch 2040/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.1173 - val_accuracy: 0.9814\n",
      "Epoch 2041/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0221 - accuracy: 0.9954 - val_loss: 0.1209 - val_accuracy: 0.9752\n",
      "Epoch 2042/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0222 - accuracy: 0.9954 - val_loss: 0.1121 - val_accuracy: 0.9814\n",
      "Epoch 2043/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.1090 - val_accuracy: 0.9845\n",
      "Epoch 2044/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0219 - accuracy: 0.9908 - val_loss: 0.1152 - val_accuracy: 0.9814\n",
      "Epoch 2045/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0215 - accuracy: 0.9954 - val_loss: 0.1226 - val_accuracy: 0.9783\n",
      "Epoch 2046/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.1231 - val_accuracy: 0.9783\n",
      "Epoch 2047/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0229 - accuracy: 0.9939 - val_loss: 0.1149 - val_accuracy: 0.9814\n",
      "Epoch 2048/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.1122 - val_accuracy: 0.9845\n",
      "Epoch 2049/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0219 - accuracy: 0.9954 - val_loss: 0.1197 - val_accuracy: 0.9783\n",
      "Epoch 2050/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0226 - accuracy: 0.9939 - val_loss: 0.1139 - val_accuracy: 0.9845\n",
      "Epoch 2051/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0219 - accuracy: 0.9954 - val_loss: 0.1114 - val_accuracy: 0.9845\n",
      "Epoch 2052/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0217 - accuracy: 0.9939 - val_loss: 0.1128 - val_accuracy: 0.9845\n",
      "Epoch 2053/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 15us/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.1094 - val_accuracy: 0.9845\n",
      "Epoch 2054/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0221 - accuracy: 0.9908 - val_loss: 0.1117 - val_accuracy: 0.9845\n",
      "Epoch 2055/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.1222 - val_accuracy: 0.9783\n",
      "Epoch 2056/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.1190 - val_accuracy: 0.9814\n",
      "Epoch 2057/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.1096 - val_accuracy: 0.9845\n",
      "Epoch 2058/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0226 - accuracy: 0.9908 - val_loss: 0.1108 - val_accuracy: 0.9845\n",
      "Epoch 2059/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.1184 - val_accuracy: 0.9814\n",
      "Epoch 2060/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.1127 - val_accuracy: 0.9814\n",
      "Epoch 2061/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.1098 - val_accuracy: 0.9845\n",
      "Epoch 2062/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0218 - accuracy: 0.9908 - val_loss: 0.1100 - val_accuracy: 0.9845\n",
      "Epoch 2063/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.1192 - val_accuracy: 0.9783\n",
      "Epoch 2064/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0218 - accuracy: 0.9954 - val_loss: 0.1209 - val_accuracy: 0.9783\n",
      "Epoch 2065/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0222 - accuracy: 0.9954 - val_loss: 0.1159 - val_accuracy: 0.9814\n",
      "Epoch 2066/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0216 - accuracy: 0.9954 - val_loss: 0.1064 - val_accuracy: 0.9845\n",
      "Epoch 2067/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0230 - accuracy: 0.9908 - val_loss: 0.1058 - val_accuracy: 0.9845\n",
      "Epoch 2068/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0223 - accuracy: 0.9923 - val_loss: 0.1205 - val_accuracy: 0.9783\n",
      "Epoch 2069/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0222 - accuracy: 0.9954 - val_loss: 0.1248 - val_accuracy: 0.9752\n",
      "Epoch 2070/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0228 - accuracy: 0.9939 - val_loss: 0.1099 - val_accuracy: 0.9845\n",
      "Epoch 2071/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0224 - accuracy: 0.9908 - val_loss: 0.1024 - val_accuracy: 0.9845\n",
      "Epoch 2072/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0251 - accuracy: 0.9908 - val_loss: 0.1136 - val_accuracy: 0.9814\n",
      "Epoch 2073/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1123 - val_accuracy: 0.9814\n",
      "Epoch 2074/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0223 - accuracy: 0.9923 - val_loss: 0.1097 - val_accuracy: 0.9814\n",
      "Epoch 2075/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0231 - accuracy: 0.9908 - val_loss: 0.1163 - val_accuracy: 0.9814\n",
      "Epoch 2076/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.1167 - val_accuracy: 0.9814\n",
      "Epoch 2077/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0219 - accuracy: 0.9954 - val_loss: 0.1241 - val_accuracy: 0.9752\n",
      "Epoch 2078/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0224 - accuracy: 0.9939 - val_loss: 0.1100 - val_accuracy: 0.9845\n",
      "Epoch 2079/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0218 - accuracy: 0.9908 - val_loss: 0.1033 - val_accuracy: 0.9845\n",
      "Epoch 2080/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0240 - accuracy: 0.9908 - val_loss: 0.1117 - val_accuracy: 0.9845\n",
      "Epoch 2081/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0218 - accuracy: 0.9923 - val_loss: 0.1166 - val_accuracy: 0.9814\n",
      "Epoch 2082/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0214 - accuracy: 0.9954 - val_loss: 0.1082 - val_accuracy: 0.9845\n",
      "Epoch 2083/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.1056 - val_accuracy: 0.9845\n",
      "Epoch 2084/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.1141 - val_accuracy: 0.9845\n",
      "Epoch 2085/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.1274 - val_accuracy: 0.9752\n",
      "Epoch 2086/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.1237 - val_accuracy: 0.9783\n",
      "Epoch 2087/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 0.1027 - val_accuracy: 0.9845\n",
      "Epoch 2088/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0252 - accuracy: 0.9908 - val_loss: 0.1002 - val_accuracy: 0.9845\n",
      "Epoch 2089/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0282 - accuracy: 0.9877 - val_loss: 0.1169 - val_accuracy: 0.9814\n",
      "Epoch 2090/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0214 - accuracy: 0.9923 - val_loss: 0.1314 - val_accuracy: 0.9752\n",
      "Epoch 2091/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1209 - val_accuracy: 0.9814\n",
      "Epoch 2092/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0212 - accuracy: 0.9954 - val_loss: 0.1063 - val_accuracy: 0.9845\n",
      "Epoch 2093/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0233 - accuracy: 0.9908 - val_loss: 0.1093 - val_accuracy: 0.9845\n",
      "Epoch 2094/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0241 - accuracy: 0.9908 - val_loss: 0.1194 - val_accuracy: 0.9814\n",
      "Epoch 2095/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.1129 - val_accuracy: 0.9814\n",
      "Epoch 2096/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0218 - accuracy: 0.9908 - val_loss: 0.1144 - val_accuracy: 0.9814\n",
      "Epoch 2097/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.1123 - val_accuracy: 0.9814\n",
      "Epoch 2098/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0216 - accuracy: 0.9908 - val_loss: 0.1164 - val_accuracy: 0.9814\n",
      "Epoch 2099/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.1157 - val_accuracy: 0.9814\n",
      "Epoch 2100/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 0.1081 - val_accuracy: 0.9845\n",
      "Epoch 2101/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0224 - accuracy: 0.9923 - val_loss: 0.1156 - val_accuracy: 0.9845\n",
      "Epoch 2102/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0212 - accuracy: 0.9939 - val_loss: 0.1338 - val_accuracy: 0.9720\n",
      "Epoch 2103/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0245 - accuracy: 0.9908 - val_loss: 0.1254 - val_accuracy: 0.9783\n",
      "Epoch 2104/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.1157 - val_accuracy: 0.9845\n",
      "Epoch 2105/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0212 - accuracy: 0.9954 - val_loss: 0.1189 - val_accuracy: 0.9814\n",
      "Epoch 2106/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0216 - accuracy: 0.9954 - val_loss: 0.1177 - val_accuracy: 0.9845\n",
      "Epoch 2107/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0215 - accuracy: 0.9954 - val_loss: 0.1126 - val_accuracy: 0.9845\n",
      "Epoch 2108/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0214 - accuracy: 0.9939 - val_loss: 0.1125 - val_accuracy: 0.9845\n",
      "Epoch 2109/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0218 - accuracy: 0.9923 - val_loss: 0.1243 - val_accuracy: 0.9783\n",
      "Epoch 2110/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.1251 - val_accuracy: 0.9814\n",
      "Epoch 2111/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 0.1066 - val_accuracy: 0.9845\n",
      "Epoch 2112/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0225 - accuracy: 0.9908 - val_loss: 0.1093 - val_accuracy: 0.9845\n",
      "Epoch 2113/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0221 - accuracy: 0.9908 - val_loss: 0.1114 - val_accuracy: 0.9845\n",
      "Epoch 2114/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0214 - accuracy: 0.9908 - val_loss: 0.1117 - val_accuracy: 0.9845\n",
      "Epoch 2115/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0213 - accuracy: 0.9908 - val_loss: 0.1212 - val_accuracy: 0.9814\n",
      "Epoch 2116/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1197 - val_accuracy: 0.9814\n",
      "Epoch 2117/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0216 - accuracy: 0.9954 - val_loss: 0.1095 - val_accuracy: 0.9845\n",
      "Epoch 2118/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0221 - accuracy: 0.9908 - val_loss: 0.1194 - val_accuracy: 0.9814\n",
      "Epoch 2119/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 0.1368 - val_accuracy: 0.9752\n",
      "Epoch 2120/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0255 - accuracy: 0.9908 - val_loss: 0.1209 - val_accuracy: 0.9783\n",
      "Epoch 2121/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0212 - accuracy: 0.9939 - val_loss: 0.1029 - val_accuracy: 0.9845\n",
      "Epoch 2122/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0253 - accuracy: 0.9908 - val_loss: 0.1137 - val_accuracy: 0.9814\n",
      "Epoch 2123/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0237 - accuracy: 0.9954 - val_loss: 0.1413 - val_accuracy: 0.9752\n",
      "Epoch 2124/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0261 - accuracy: 0.9908 - val_loss: 0.1139 - val_accuracy: 0.9814\n",
      "Epoch 2125/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0222 - accuracy: 0.9923 - val_loss: 0.1015 - val_accuracy: 0.9845\n",
      "Epoch 2126/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0247 - accuracy: 0.9908 - val_loss: 0.1108 - val_accuracy: 0.9814\n",
      "Epoch 2127/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0212 - accuracy: 0.9923 - val_loss: 0.1301 - val_accuracy: 0.9783\n",
      "Epoch 2128/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0256 - accuracy: 0.9908 - val_loss: 0.1307 - val_accuracy: 0.9752\n",
      "Epoch 2129/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.1016 - val_accuracy: 0.9845\n",
      "Epoch 2130/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0258 - accuracy: 0.9908 - val_loss: 0.1015 - val_accuracy: 0.9845\n",
      "Epoch 2131/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0244 - accuracy: 0.9893 - val_loss: 0.1246 - val_accuracy: 0.9783\n",
      "Epoch 2132/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0229 - accuracy: 0.9939 - val_loss: 0.1327 - val_accuracy: 0.9752\n",
      "Epoch 2133/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.1128 - val_accuracy: 0.9814\n",
      "Epoch 2134/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 0.1044 - val_accuracy: 0.9845\n",
      "Epoch 2135/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.1137 - val_accuracy: 0.9814\n",
      "Epoch 2136/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0224 - accuracy: 0.9923 - val_loss: 0.1160 - val_accuracy: 0.9814\n",
      "Epoch 2137/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.1109 - val_accuracy: 0.9845\n",
      "Epoch 2138/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0212 - accuracy: 0.9939 - val_loss: 0.1227 - val_accuracy: 0.9783\n",
      "Epoch 2139/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0223 - accuracy: 0.9939 - val_loss: 0.1183 - val_accuracy: 0.9845\n",
      "Epoch 2140/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.1081 - val_accuracy: 0.9845\n",
      "Epoch 2141/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0218 - accuracy: 0.9923 - val_loss: 0.1171 - val_accuracy: 0.9845\n",
      "Epoch 2142/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0209 - accuracy: 0.9954 - val_loss: 0.1245 - val_accuracy: 0.9783\n",
      "Epoch 2143/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.1170 - val_accuracy: 0.9814\n",
      "Epoch 2144/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.1194 - val_accuracy: 0.9814\n",
      "Epoch 2145/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0214 - accuracy: 0.9954 - val_loss: 0.1264 - val_accuracy: 0.9752\n",
      "Epoch 2146/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1118 - val_accuracy: 0.9814\n",
      "Epoch 2147/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0227 - accuracy: 0.9908 - val_loss: 0.1119 - val_accuracy: 0.9814\n",
      "Epoch 2148/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0210 - accuracy: 0.9939 - val_loss: 0.1292 - val_accuracy: 0.9752\n",
      "Epoch 2149/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.1228 - val_accuracy: 0.9814\n",
      "Epoch 2150/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0218 - accuracy: 0.9954 - val_loss: 0.1100 - val_accuracy: 0.9845\n",
      "Epoch 2151/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0215 - accuracy: 0.9908 - val_loss: 0.1077 - val_accuracy: 0.9845\n",
      "Epoch 2152/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0220 - accuracy: 0.9908 - val_loss: 0.1102 - val_accuracy: 0.9845\n",
      "Epoch 2153/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0210 - accuracy: 0.9923 - val_loss: 0.1176 - val_accuracy: 0.9814\n",
      "Epoch 2154/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0208 - accuracy: 0.9954 - val_loss: 0.1209 - val_accuracy: 0.9783\n",
      "Epoch 2155/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0212 - accuracy: 0.9954 - val_loss: 0.1129 - val_accuracy: 0.9814\n",
      "Epoch 2156/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0209 - accuracy: 0.9923 - val_loss: 0.1024 - val_accuracy: 0.9845\n",
      "Epoch 2157/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0247 - accuracy: 0.9893 - val_loss: 0.1026 - val_accuracy: 0.9845\n",
      "Epoch 2158/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0238 - accuracy: 0.9908 - val_loss: 0.1247 - val_accuracy: 0.9814\n",
      "Epoch 2159/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0239 - accuracy: 0.9923 - val_loss: 0.1269 - val_accuracy: 0.9783\n",
      "Epoch 2160/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0227 - accuracy: 0.9923 - val_loss: 0.1044 - val_accuracy: 0.9845\n",
      "Epoch 2161/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0238 - accuracy: 0.9908 - val_loss: 0.1065 - val_accuracy: 0.9845\n",
      "Epoch 2162/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0223 - accuracy: 0.9923 - val_loss: 0.1204 - val_accuracy: 0.9814\n",
      "Epoch 2163/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 15us/step - loss: 0.0214 - accuracy: 0.9954 - val_loss: 0.1231 - val_accuracy: 0.9783\n",
      "Epoch 2164/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.1092 - val_accuracy: 0.9845\n",
      "Epoch 2165/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0214 - accuracy: 0.9923 - val_loss: 0.1048 - val_accuracy: 0.9845\n",
      "Epoch 2166/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0237 - accuracy: 0.9908 - val_loss: 0.1118 - val_accuracy: 0.9814\n",
      "Epoch 2167/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0209 - accuracy: 0.9923 - val_loss: 0.1231 - val_accuracy: 0.9783\n",
      "Epoch 2168/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0215 - accuracy: 0.9939 - val_loss: 0.1241 - val_accuracy: 0.9783\n",
      "Epoch 2169/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.1169 - val_accuracy: 0.9814\n",
      "Epoch 2170/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1192 - val_accuracy: 0.9814\n",
      "Epoch 2171/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0209 - accuracy: 0.9954 - val_loss: 0.1205 - val_accuracy: 0.9814\n",
      "Epoch 2172/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0210 - accuracy: 0.9954 - val_loss: 0.1156 - val_accuracy: 0.9814\n",
      "Epoch 2173/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0219 - accuracy: 0.9923 - val_loss: 0.1191 - val_accuracy: 0.9814\n",
      "Epoch 2174/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0210 - accuracy: 0.9954 - val_loss: 0.1324 - val_accuracy: 0.9752\n",
      "Epoch 2175/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0228 - accuracy: 0.9939 - val_loss: 0.1186 - val_accuracy: 0.9814\n",
      "Epoch 2176/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.1092 - val_accuracy: 0.9845\n",
      "Epoch 2177/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0219 - accuracy: 0.9908 - val_loss: 0.1143 - val_accuracy: 0.9845\n",
      "Epoch 2178/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0215 - accuracy: 0.9939 - val_loss: 0.1150 - val_accuracy: 0.9845\n",
      "Epoch 2179/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1090 - val_accuracy: 0.9845\n",
      "Epoch 2180/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.1169 - val_accuracy: 0.9845\n",
      "Epoch 2181/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0207 - accuracy: 0.9954 - val_loss: 0.1232 - val_accuracy: 0.9814\n",
      "Epoch 2182/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0210 - accuracy: 0.9954 - val_loss: 0.1153 - val_accuracy: 0.9845\n",
      "Epoch 2183/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0208 - accuracy: 0.9939 - val_loss: 0.1124 - val_accuracy: 0.9845\n",
      "Epoch 2184/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0213 - accuracy: 0.9908 - val_loss: 0.1186 - val_accuracy: 0.9845\n",
      "Epoch 2185/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0209 - accuracy: 0.9954 - val_loss: 0.1247 - val_accuracy: 0.9814\n",
      "Epoch 2186/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0214 - accuracy: 0.9939 - val_loss: 0.1250 - val_accuracy: 0.9814\n",
      "Epoch 2187/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.1148 - val_accuracy: 0.9845\n",
      "Epoch 2188/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.1115 - val_accuracy: 0.9845\n",
      "Epoch 2189/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0212 - accuracy: 0.9908 - val_loss: 0.1179 - val_accuracy: 0.9814\n",
      "Epoch 2190/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.1307 - val_accuracy: 0.9783\n",
      "Epoch 2191/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.1378 - val_accuracy: 0.9783\n",
      "Epoch 2192/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0248 - accuracy: 0.9923 - val_loss: 0.1098 - val_accuracy: 0.9845\n",
      "Epoch 2193/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0250 - accuracy: 0.9877 - val_loss: 0.1014 - val_accuracy: 0.9845\n",
      "Epoch 2194/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0272 - accuracy: 0.9893 - val_loss: 0.1349 - val_accuracy: 0.9752\n",
      "Epoch 2195/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 0.1453 - val_accuracy: 0.9720\n",
      "Epoch 2196/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0241 - accuracy: 0.9908 - val_loss: 0.1054 - val_accuracy: 0.9814\n",
      "Epoch 2197/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0259 - accuracy: 0.9908 - val_loss: 0.1020 - val_accuracy: 0.9845\n",
      "Epoch 2198/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0304 - accuracy: 0.9877 - val_loss: 0.1273 - val_accuracy: 0.9752\n",
      "Epoch 2199/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0216 - accuracy: 0.9954 - val_loss: 0.1438 - val_accuracy: 0.9720\n",
      "Epoch 2200/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0261 - accuracy: 0.9923 - val_loss: 0.1275 - val_accuracy: 0.9783\n",
      "Epoch 2201/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0223 - accuracy: 0.9939 - val_loss: 0.1097 - val_accuracy: 0.9814\n",
      "Epoch 2202/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.1115 - val_accuracy: 0.9814\n",
      "Epoch 2203/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0223 - accuracy: 0.9908 - val_loss: 0.1240 - val_accuracy: 0.9783\n",
      "Epoch 2204/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1165 - val_accuracy: 0.9814\n",
      "Epoch 2205/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0218 - accuracy: 0.9939 - val_loss: 0.1101 - val_accuracy: 0.9845\n",
      "Epoch 2206/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.1208 - val_accuracy: 0.9783\n",
      "Epoch 2207/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 0.1201 - val_accuracy: 0.9814\n",
      "Epoch 2208/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0204 - accuracy: 0.9954 - val_loss: 0.1094 - val_accuracy: 0.9845\n",
      "Epoch 2209/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.1103 - val_accuracy: 0.9845\n",
      "Epoch 2210/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.1195 - val_accuracy: 0.9814\n",
      "Epoch 2211/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0206 - accuracy: 0.9954 - val_loss: 0.1235 - val_accuracy: 0.9814\n",
      "Epoch 2212/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0211 - accuracy: 0.9954 - val_loss: 0.1188 - val_accuracy: 0.9814\n",
      "Epoch 2213/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.1067 - val_accuracy: 0.9845\n",
      "Epoch 2214/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0229 - accuracy: 0.9908 - val_loss: 0.1111 - val_accuracy: 0.9845\n",
      "Epoch 2215/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1314 - val_accuracy: 0.9752\n",
      "Epoch 2216/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0224 - accuracy: 0.9923 - val_loss: 0.1328 - val_accuracy: 0.9752\n",
      "Epoch 2217/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.1171 - val_accuracy: 0.9845\n",
      "Epoch 2218/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0210 - accuracy: 0.9939 - val_loss: 0.1158 - val_accuracy: 0.9845\n",
      "Epoch 2219/3500\n",
      "653/653 [==============================] - 0s 32us/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.1319 - val_accuracy: 0.9752\n",
      "Epoch 2220/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.1226 - val_accuracy: 0.9814\n",
      "Epoch 2221/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0209 - accuracy: 0.9969 - val_loss: 0.1063 - val_accuracy: 0.9845\n",
      "Epoch 2222/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0237 - accuracy: 0.9893 - val_loss: 0.1102 - val_accuracy: 0.9845\n",
      "Epoch 2223/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0212 - accuracy: 0.9923 - val_loss: 0.1193 - val_accuracy: 0.9814\n",
      "Epoch 2224/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0202 - accuracy: 0.9939 - val_loss: 0.1300 - val_accuracy: 0.9783\n",
      "Epoch 2225/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.1168 - val_accuracy: 0.9814\n",
      "Epoch 2226/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0215 - accuracy: 0.9908 - val_loss: 0.1049 - val_accuracy: 0.9845\n",
      "Epoch 2227/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0251 - accuracy: 0.9908 - val_loss: 0.1156 - val_accuracy: 0.9814\n",
      "Epoch 2228/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0206 - accuracy: 0.9923 - val_loss: 0.1229 - val_accuracy: 0.9814\n",
      "Epoch 2229/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0209 - accuracy: 0.9954 - val_loss: 0.1135 - val_accuracy: 0.9845\n",
      "Epoch 2230/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0209 - accuracy: 0.9908 - val_loss: 0.1084 - val_accuracy: 0.9845\n",
      "Epoch 2231/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0214 - accuracy: 0.9923 - val_loss: 0.1266 - val_accuracy: 0.9783\n",
      "Epoch 2232/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0214 - accuracy: 0.9939 - val_loss: 0.1412 - val_accuracy: 0.9752\n",
      "Epoch 2233/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0245 - accuracy: 0.9908 - val_loss: 0.1167 - val_accuracy: 0.9814\n",
      "Epoch 2234/3500\n",
      "653/653 [==============================] - 0s 35us/step - loss: 0.0223 - accuracy: 0.9893 - val_loss: 0.1069 - val_accuracy: 0.9845\n",
      "Epoch 2235/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0233 - accuracy: 0.9908 - val_loss: 0.1229 - val_accuracy: 0.9814\n",
      "Epoch 2236/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1127 - val_accuracy: 0.9845\n",
      "Epoch 2237/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0220 - accuracy: 0.9893 - val_loss: 0.1060 - val_accuracy: 0.9845\n",
      "Epoch 2238/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0227 - accuracy: 0.9908 - val_loss: 0.1261 - val_accuracy: 0.9814\n",
      "Epoch 2239/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.1396 - val_accuracy: 0.9783\n",
      "Epoch 2240/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0243 - accuracy: 0.9908 - val_loss: 0.1206 - val_accuracy: 0.9814\n",
      "Epoch 2241/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.1056 - val_accuracy: 0.9845\n",
      "Epoch 2242/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0243 - accuracy: 0.9908 - val_loss: 0.1231 - val_accuracy: 0.9814\n",
      "Epoch 2243/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1394 - val_accuracy: 0.9783\n",
      "Epoch 2244/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0227 - accuracy: 0.9923 - val_loss: 0.1051 - val_accuracy: 0.9845\n",
      "Epoch 2245/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0271 - accuracy: 0.9877 - val_loss: 0.1073 - val_accuracy: 0.9845\n",
      "Epoch 2246/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0224 - accuracy: 0.9923 - val_loss: 0.1472 - val_accuracy: 0.9720\n",
      "Epoch 2247/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0272 - accuracy: 0.9908 - val_loss: 0.1297 - val_accuracy: 0.9783\n",
      "Epoch 2248/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.1029 - val_accuracy: 0.9814\n",
      "Epoch 2249/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0355 - accuracy: 0.9877 - val_loss: 0.1057 - val_accuracy: 0.9845\n",
      "Epoch 2250/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0252 - accuracy: 0.9893 - val_loss: 0.1550 - val_accuracy: 0.9720\n",
      "Epoch 2251/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0317 - accuracy: 0.9908 - val_loss: 0.1355 - val_accuracy: 0.9752\n",
      "Epoch 2252/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.1034 - val_accuracy: 0.9814\n",
      "Epoch 2253/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0391 - accuracy: 0.9877 - val_loss: 0.1067 - val_accuracy: 0.9845\n",
      "Epoch 2254/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0251 - accuracy: 0.9893 - val_loss: 0.1485 - val_accuracy: 0.9720\n",
      "Epoch 2255/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0266 - accuracy: 0.9908 - val_loss: 0.1268 - val_accuracy: 0.9814\n",
      "Epoch 2256/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0232 - accuracy: 0.9908 - val_loss: 0.1119 - val_accuracy: 0.9845\n",
      "Epoch 2257/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0218 - accuracy: 0.9908 - val_loss: 0.1205 - val_accuracy: 0.9814\n",
      "Epoch 2258/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0208 - accuracy: 0.9939 - val_loss: 0.1155 - val_accuracy: 0.9845\n",
      "Epoch 2259/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0205 - accuracy: 0.9923 - val_loss: 0.1094 - val_accuracy: 0.9845\n",
      "Epoch 2260/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0218 - accuracy: 0.9893 - val_loss: 0.1192 - val_accuracy: 0.9845\n",
      "Epoch 2261/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1168 - val_accuracy: 0.9845\n",
      "Epoch 2262/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.1040 - val_accuracy: 0.9845\n",
      "Epoch 2263/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0254 - accuracy: 0.9893 - val_loss: 0.1172 - val_accuracy: 0.9845\n",
      "Epoch 2264/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.1386 - val_accuracy: 0.9752\n",
      "Epoch 2265/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.1216 - val_accuracy: 0.9814\n",
      "Epoch 2266/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.1056 - val_accuracy: 0.9845\n",
      "Epoch 2267/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0250 - accuracy: 0.9908 - val_loss: 0.1128 - val_accuracy: 0.9845\n",
      "Epoch 2268/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0208 - accuracy: 0.9908 - val_loss: 0.1363 - val_accuracy: 0.9752\n",
      "Epoch 2269/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.1248 - val_accuracy: 0.9814\n",
      "Epoch 2270/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.1034 - val_accuracy: 0.9845\n",
      "Epoch 2271/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0283 - accuracy: 0.9877 - val_loss: 0.1068 - val_accuracy: 0.9845\n",
      "Epoch 2272/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.1349 - val_accuracy: 0.9752\n",
      "Epoch 2273/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 17us/step - loss: 0.0244 - accuracy: 0.9923 - val_loss: 0.1427 - val_accuracy: 0.9752\n",
      "Epoch 2274/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0252 - accuracy: 0.9908 - val_loss: 0.1152 - val_accuracy: 0.9845\n",
      "Epoch 2275/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0202 - accuracy: 0.9923 - val_loss: 0.1088 - val_accuracy: 0.9845\n",
      "Epoch 2276/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0220 - accuracy: 0.9908 - val_loss: 0.1222 - val_accuracy: 0.9814\n",
      "Epoch 2277/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.1456 - val_accuracy: 0.9720\n",
      "Epoch 2278/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0262 - accuracy: 0.9908 - val_loss: 0.1208 - val_accuracy: 0.9845\n",
      "Epoch 2279/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0234 - accuracy: 0.9939 - val_loss: 0.1060 - val_accuracy: 0.9814\n",
      "Epoch 2280/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0261 - accuracy: 0.9893 - val_loss: 0.1234 - val_accuracy: 0.9814\n",
      "Epoch 2281/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.1466 - val_accuracy: 0.9720\n",
      "Epoch 2282/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0268 - accuracy: 0.9908 - val_loss: 0.1222 - val_accuracy: 0.9814\n",
      "Epoch 2283/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0212 - accuracy: 0.9954 - val_loss: 0.1030 - val_accuracy: 0.9814\n",
      "Epoch 2284/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0301 - accuracy: 0.9877 - val_loss: 0.1091 - val_accuracy: 0.9845\n",
      "Epoch 2285/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0202 - accuracy: 0.9923 - val_loss: 0.1405 - val_accuracy: 0.9783\n",
      "Epoch 2286/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0247 - accuracy: 0.9908 - val_loss: 0.1451 - val_accuracy: 0.9783\n",
      "Epoch 2287/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.1113 - val_accuracy: 0.9814\n",
      "Epoch 2288/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.1049 - val_accuracy: 0.9876\n",
      "Epoch 2289/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0237 - accuracy: 0.9908 - val_loss: 0.1316 - val_accuracy: 0.9783\n",
      "Epoch 2290/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.1578 - val_accuracy: 0.9658\n",
      "Epoch 2291/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0296 - accuracy: 0.9923 - val_loss: 0.1246 - val_accuracy: 0.9814\n",
      "Epoch 2292/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.1026 - val_accuracy: 0.9845\n",
      "Epoch 2293/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0280 - accuracy: 0.9893 - val_loss: 0.1106 - val_accuracy: 0.9845\n",
      "Epoch 2294/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0208 - accuracy: 0.9923 - val_loss: 0.1474 - val_accuracy: 0.9720\n",
      "Epoch 2295/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0273 - accuracy: 0.9908 - val_loss: 0.1463 - val_accuracy: 0.9720\n",
      "Epoch 2296/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0267 - accuracy: 0.9923 - val_loss: 0.1196 - val_accuracy: 0.9814\n",
      "Epoch 2297/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0219 - accuracy: 0.9939 - val_loss: 0.1124 - val_accuracy: 0.9845\n",
      "Epoch 2298/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0202 - accuracy: 0.9939 - val_loss: 0.1300 - val_accuracy: 0.9783\n",
      "Epoch 2299/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0215 - accuracy: 0.9954 - val_loss: 0.1406 - val_accuracy: 0.9752\n",
      "Epoch 2300/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.1249 - val_accuracy: 0.9814\n",
      "Epoch 2301/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0219 - accuracy: 0.9939 - val_loss: 0.1178 - val_accuracy: 0.9814\n",
      "Epoch 2302/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1215 - val_accuracy: 0.9814\n",
      "Epoch 2303/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1201 - val_accuracy: 0.9845\n",
      "Epoch 2304/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.1165 - val_accuracy: 0.9845\n",
      "Epoch 2305/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.1133 - val_accuracy: 0.9845\n",
      "Epoch 2306/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0205 - accuracy: 0.9923 - val_loss: 0.1252 - val_accuracy: 0.9814\n",
      "Epoch 2307/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.1173 - val_accuracy: 0.9845\n",
      "Epoch 2308/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0210 - accuracy: 0.9923 - val_loss: 0.1099 - val_accuracy: 0.9845\n",
      "Epoch 2309/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0211 - accuracy: 0.9893 - val_loss: 0.1235 - val_accuracy: 0.9814\n",
      "Epoch 2310/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0209 - accuracy: 0.9954 - val_loss: 0.1189 - val_accuracy: 0.9814\n",
      "Epoch 2311/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1087 - val_accuracy: 0.9845\n",
      "Epoch 2312/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0210 - accuracy: 0.9908 - val_loss: 0.1207 - val_accuracy: 0.9814\n",
      "Epoch 2313/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.1326 - val_accuracy: 0.9783\n",
      "Epoch 2314/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0218 - accuracy: 0.9939 - val_loss: 0.1227 - val_accuracy: 0.9814\n",
      "Epoch 2315/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0203 - accuracy: 0.9954 - val_loss: 0.1100 - val_accuracy: 0.9845\n",
      "Epoch 2316/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0206 - accuracy: 0.9908 - val_loss: 0.1145 - val_accuracy: 0.9845\n",
      "Epoch 2317/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.1222 - val_accuracy: 0.9814\n",
      "Epoch 2318/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0202 - accuracy: 0.9954 - val_loss: 0.1226 - val_accuracy: 0.9814\n",
      "Epoch 2319/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.1196 - val_accuracy: 0.9814\n",
      "Epoch 2320/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1194 - val_accuracy: 0.9814\n",
      "Epoch 2321/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.1349 - val_accuracy: 0.9783\n",
      "Epoch 2322/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.1344 - val_accuracy: 0.9783\n",
      "Epoch 2323/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.1178 - val_accuracy: 0.9814\n",
      "Epoch 2324/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0203 - accuracy: 0.9923 - val_loss: 0.1148 - val_accuracy: 0.9845\n",
      "Epoch 2325/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0199 - accuracy: 0.9923 - val_loss: 0.1253 - val_accuracy: 0.9783\n",
      "Epoch 2326/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0207 - accuracy: 0.9954 - val_loss: 0.1201 - val_accuracy: 0.9814\n",
      "Epoch 2327/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.1103 - val_accuracy: 0.9845\n",
      "Epoch 2328/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0210 - accuracy: 0.9923 - val_loss: 0.1233 - val_accuracy: 0.9814\n",
      "Epoch 2329/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.1416 - val_accuracy: 0.9752\n",
      "Epoch 2330/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0237 - accuracy: 0.9908 - val_loss: 0.1287 - val_accuracy: 0.9814\n",
      "Epoch 2331/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.1079 - val_accuracy: 0.9845\n",
      "Epoch 2332/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0226 - accuracy: 0.9908 - val_loss: 0.1074 - val_accuracy: 0.9845\n",
      "Epoch 2333/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0228 - accuracy: 0.9908 - val_loss: 0.1213 - val_accuracy: 0.9814\n",
      "Epoch 2334/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0204 - accuracy: 0.9954 - val_loss: 0.1264 - val_accuracy: 0.9814\n",
      "Epoch 2335/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.1119 - val_accuracy: 0.9845\n",
      "Epoch 2336/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.1117 - val_accuracy: 0.9845\n",
      "Epoch 2337/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0207 - accuracy: 0.9923 - val_loss: 0.1291 - val_accuracy: 0.9783\n",
      "Epoch 2338/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0204 - accuracy: 0.9954 - val_loss: 0.1357 - val_accuracy: 0.9752\n",
      "Epoch 2339/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0213 - accuracy: 0.9923 - val_loss: 0.1189 - val_accuracy: 0.9845\n",
      "Epoch 2340/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.1113 - val_accuracy: 0.9845\n",
      "Epoch 2341/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0219 - accuracy: 0.9908 - val_loss: 0.1204 - val_accuracy: 0.9845\n",
      "Epoch 2342/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.1263 - val_accuracy: 0.9814\n",
      "Epoch 2343/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0203 - accuracy: 0.9954 - val_loss: 0.1265 - val_accuracy: 0.9814\n",
      "Epoch 2344/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.1168 - val_accuracy: 0.9845\n",
      "Epoch 2345/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.1109 - val_accuracy: 0.9845\n",
      "Epoch 2346/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0212 - accuracy: 0.9893 - val_loss: 0.1306 - val_accuracy: 0.9783\n",
      "Epoch 2347/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0221 - accuracy: 0.9954 - val_loss: 0.1513 - val_accuracy: 0.9720\n",
      "Epoch 2348/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0268 - accuracy: 0.9908 - val_loss: 0.1200 - val_accuracy: 0.9814\n",
      "Epoch 2349/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.1048 - val_accuracy: 0.9876\n",
      "Epoch 2350/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0242 - accuracy: 0.9893 - val_loss: 0.1167 - val_accuracy: 0.9845\n",
      "Epoch 2351/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0200 - accuracy: 0.9923 - val_loss: 0.1439 - val_accuracy: 0.9720\n",
      "Epoch 2352/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0246 - accuracy: 0.9908 - val_loss: 0.1328 - val_accuracy: 0.9783\n",
      "Epoch 2353/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0210 - accuracy: 0.9939 - val_loss: 0.1133 - val_accuracy: 0.9845\n",
      "Epoch 2354/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0208 - accuracy: 0.9923 - val_loss: 0.1112 - val_accuracy: 0.9845\n",
      "Epoch 2355/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0199 - accuracy: 0.9923 - val_loss: 0.1310 - val_accuracy: 0.9783\n",
      "Epoch 2356/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0219 - accuracy: 0.9939 - val_loss: 0.1337 - val_accuracy: 0.9783\n",
      "Epoch 2357/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0217 - accuracy: 0.9939 - val_loss: 0.1125 - val_accuracy: 0.9845\n",
      "Epoch 2358/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0205 - accuracy: 0.9923 - val_loss: 0.1075 - val_accuracy: 0.9845\n",
      "Epoch 2359/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0219 - accuracy: 0.9908 - val_loss: 0.1196 - val_accuracy: 0.9814\n",
      "Epoch 2360/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.1280 - val_accuracy: 0.9783\n",
      "Epoch 2361/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0210 - accuracy: 0.9939 - val_loss: 0.1205 - val_accuracy: 0.9814\n",
      "Epoch 2362/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.1101 - val_accuracy: 0.9845\n",
      "Epoch 2363/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0209 - accuracy: 0.9908 - val_loss: 0.1177 - val_accuracy: 0.9814\n",
      "Epoch 2364/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0202 - accuracy: 0.9939 - val_loss: 0.1282 - val_accuracy: 0.9783\n",
      "Epoch 2365/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0209 - accuracy: 0.9954 - val_loss: 0.1182 - val_accuracy: 0.9845\n",
      "Epoch 2366/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.1163 - val_accuracy: 0.9845\n",
      "Epoch 2367/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.1221 - val_accuracy: 0.9814\n",
      "Epoch 2368/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.1228 - val_accuracy: 0.9814\n",
      "Epoch 2369/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.1099 - val_accuracy: 0.9845\n",
      "Epoch 2370/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.1058 - val_accuracy: 0.9845\n",
      "Epoch 2371/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0237 - accuracy: 0.9893 - val_loss: 0.1197 - val_accuracy: 0.9814\n",
      "Epoch 2372/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.1196 - val_accuracy: 0.9814\n",
      "Epoch 2373/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.1142 - val_accuracy: 0.9814\n",
      "Epoch 2374/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0207 - accuracy: 0.9923 - val_loss: 0.1147 - val_accuracy: 0.9814\n",
      "Epoch 2375/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0215 - accuracy: 0.9908 - val_loss: 0.1257 - val_accuracy: 0.9783\n",
      "Epoch 2376/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0198 - accuracy: 0.9954 - val_loss: 0.1516 - val_accuracy: 0.9720\n",
      "Epoch 2377/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0262 - accuracy: 0.9908 - val_loss: 0.1345 - val_accuracy: 0.9783\n",
      "Epoch 2378/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.1086 - val_accuracy: 0.9845\n",
      "Epoch 2379/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0247 - accuracy: 0.9908 - val_loss: 0.1174 - val_accuracy: 0.9814\n",
      "Epoch 2380/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1592 - val_accuracy: 0.9658\n",
      "Epoch 2381/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0298 - accuracy: 0.9893 - val_loss: 0.1478 - val_accuracy: 0.9720\n",
      "Epoch 2382/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.1136 - val_accuracy: 0.9845\n",
      "Epoch 2383/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 19us/step - loss: 0.0205 - accuracy: 0.9923 - val_loss: 0.1105 - val_accuracy: 0.9845\n",
      "Epoch 2384/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.1306 - val_accuracy: 0.9783\n",
      "Epoch 2385/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.1453 - val_accuracy: 0.9783\n",
      "Epoch 2386/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0244 - accuracy: 0.9908 - val_loss: 0.1164 - val_accuracy: 0.9814\n",
      "Epoch 2387/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0191 - accuracy: 0.9923 - val_loss: 0.1049 - val_accuracy: 0.9845\n",
      "Epoch 2388/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0239 - accuracy: 0.9893 - val_loss: 0.1218 - val_accuracy: 0.9814\n",
      "Epoch 2389/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0211 - accuracy: 0.9954 - val_loss: 0.1459 - val_accuracy: 0.9752\n",
      "Epoch 2390/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.1161 - val_accuracy: 0.9814\n",
      "Epoch 2391/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.1031 - val_accuracy: 0.9845\n",
      "Epoch 2392/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0261 - accuracy: 0.9893 - val_loss: 0.1230 - val_accuracy: 0.9814\n",
      "Epoch 2393/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0207 - accuracy: 0.9954 - val_loss: 0.1570 - val_accuracy: 0.9658\n",
      "Epoch 2394/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0281 - accuracy: 0.9893 - val_loss: 0.1236 - val_accuracy: 0.9814\n",
      "Epoch 2395/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0223 - accuracy: 0.9939 - val_loss: 0.1052 - val_accuracy: 0.9845\n",
      "Epoch 2396/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0252 - accuracy: 0.9908 - val_loss: 0.1266 - val_accuracy: 0.9783\n",
      "Epoch 2397/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0206 - accuracy: 0.9954 - val_loss: 0.1549 - val_accuracy: 0.9720\n",
      "Epoch 2398/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0282 - accuracy: 0.9908 - val_loss: 0.1311 - val_accuracy: 0.9783\n",
      "Epoch 2399/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.1047 - val_accuracy: 0.9876\n",
      "Epoch 2400/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0246 - accuracy: 0.9908 - val_loss: 0.1091 - val_accuracy: 0.9814\n",
      "Epoch 2401/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 0.1322 - val_accuracy: 0.9783\n",
      "Epoch 2402/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0227 - accuracy: 0.9939 - val_loss: 0.1377 - val_accuracy: 0.9783\n",
      "Epoch 2403/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0219 - accuracy: 0.9939 - val_loss: 0.1114 - val_accuracy: 0.9814\n",
      "Epoch 2404/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0220 - accuracy: 0.9893 - val_loss: 0.1057 - val_accuracy: 0.9876\n",
      "Epoch 2405/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0229 - accuracy: 0.9908 - val_loss: 0.1289 - val_accuracy: 0.9783\n",
      "Epoch 2406/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0226 - accuracy: 0.9939 - val_loss: 0.1318 - val_accuracy: 0.9783\n",
      "Epoch 2407/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0222 - accuracy: 0.9923 - val_loss: 0.1103 - val_accuracy: 0.9845\n",
      "Epoch 2408/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0209 - accuracy: 0.9923 - val_loss: 0.1189 - val_accuracy: 0.9814\n",
      "Epoch 2409/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.1409 - val_accuracy: 0.9720\n",
      "Epoch 2410/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0234 - accuracy: 0.9908 - val_loss: 0.1325 - val_accuracy: 0.9783\n",
      "Epoch 2411/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0202 - accuracy: 0.9939 - val_loss: 0.1090 - val_accuracy: 0.9845\n",
      "Epoch 2412/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0219 - accuracy: 0.9908 - val_loss: 0.1126 - val_accuracy: 0.9845\n",
      "Epoch 2413/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.1389 - val_accuracy: 0.9783\n",
      "Epoch 2414/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.1386 - val_accuracy: 0.9783\n",
      "Epoch 2415/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.1120 - val_accuracy: 0.9845\n",
      "Epoch 2416/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0212 - accuracy: 0.9923 - val_loss: 0.1078 - val_accuracy: 0.9845\n",
      "Epoch 2417/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0215 - accuracy: 0.9939 - val_loss: 0.1319 - val_accuracy: 0.9783\n",
      "Epoch 2418/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.1392 - val_accuracy: 0.9783\n",
      "Epoch 2419/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0213 - accuracy: 0.9923 - val_loss: 0.1119 - val_accuracy: 0.9845\n",
      "Epoch 2420/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0208 - accuracy: 0.9923 - val_loss: 0.1070 - val_accuracy: 0.9876\n",
      "Epoch 2421/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0234 - accuracy: 0.9923 - val_loss: 0.1217 - val_accuracy: 0.9814\n",
      "Epoch 2422/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 0.1394 - val_accuracy: 0.9752\n",
      "Epoch 2423/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.1186 - val_accuracy: 0.9845\n",
      "Epoch 2424/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.1093 - val_accuracy: 0.9845\n",
      "Epoch 2425/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0226 - accuracy: 0.9908 - val_loss: 0.1350 - val_accuracy: 0.9783\n",
      "Epoch 2426/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0208 - accuracy: 0.9923 - val_loss: 0.1355 - val_accuracy: 0.9783\n",
      "Epoch 2427/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0206 - accuracy: 0.9923 - val_loss: 0.1163 - val_accuracy: 0.9845\n",
      "Epoch 2428/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0202 - accuracy: 0.9923 - val_loss: 0.1140 - val_accuracy: 0.9845\n",
      "Epoch 2429/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0191 - accuracy: 0.9923 - val_loss: 0.1372 - val_accuracy: 0.9783\n",
      "Epoch 2430/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0229 - accuracy: 0.9908 - val_loss: 0.1443 - val_accuracy: 0.9720\n",
      "Epoch 2431/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.1120 - val_accuracy: 0.9845\n",
      "Epoch 2432/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0207 - accuracy: 0.9908 - val_loss: 0.1034 - val_accuracy: 0.9845\n",
      "Epoch 2433/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0273 - accuracy: 0.9877 - val_loss: 0.1160 - val_accuracy: 0.9845\n",
      "Epoch 2434/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1405 - val_accuracy: 0.9783\n",
      "Epoch 2435/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0225 - accuracy: 0.9923 - val_loss: 0.1316 - val_accuracy: 0.9783\n",
      "Epoch 2436/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.1125 - val_accuracy: 0.9845\n",
      "Epoch 2437/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0209 - accuracy: 0.9908 - val_loss: 0.1155 - val_accuracy: 0.9845\n",
      "Epoch 2438/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.1415 - val_accuracy: 0.9783\n",
      "Epoch 2439/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.1409 - val_accuracy: 0.9783\n",
      "Epoch 2440/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.1112 - val_accuracy: 0.9845\n",
      "Epoch 2441/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0234 - accuracy: 0.9877 - val_loss: 0.1154 - val_accuracy: 0.9845\n",
      "Epoch 2442/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0214 - accuracy: 0.9908 - val_loss: 0.1419 - val_accuracy: 0.9783\n",
      "Epoch 2443/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0231 - accuracy: 0.9923 - val_loss: 0.1167 - val_accuracy: 0.9845\n",
      "Epoch 2444/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0202 - accuracy: 0.9939 - val_loss: 0.1041 - val_accuracy: 0.9876\n",
      "Epoch 2445/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0256 - accuracy: 0.9877 - val_loss: 0.1184 - val_accuracy: 0.9814\n",
      "Epoch 2446/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.1338 - val_accuracy: 0.9783\n",
      "Epoch 2447/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.1226 - val_accuracy: 0.9814\n",
      "Epoch 2448/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.1079 - val_accuracy: 0.9845\n",
      "Epoch 2449/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0213 - accuracy: 0.9893 - val_loss: 0.1183 - val_accuracy: 0.9845\n",
      "Epoch 2450/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.1384 - val_accuracy: 0.9720\n",
      "Epoch 2451/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0228 - accuracy: 0.9908 - val_loss: 0.1228 - val_accuracy: 0.9814\n",
      "Epoch 2452/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0200 - accuracy: 0.9923 - val_loss: 0.1083 - val_accuracy: 0.9845\n",
      "Epoch 2453/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0211 - accuracy: 0.9893 - val_loss: 0.1223 - val_accuracy: 0.9814\n",
      "Epoch 2454/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1407 - val_accuracy: 0.9783\n",
      "Epoch 2455/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.1267 - val_accuracy: 0.9783\n",
      "Epoch 2456/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1104 - val_accuracy: 0.9845\n",
      "Epoch 2457/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0205 - accuracy: 0.9908 - val_loss: 0.1170 - val_accuracy: 0.9845\n",
      "Epoch 2458/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1271 - val_accuracy: 0.9814\n",
      "Epoch 2459/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.1268 - val_accuracy: 0.9814\n",
      "Epoch 2460/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.1127 - val_accuracy: 0.9845\n",
      "Epoch 2461/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.1072 - val_accuracy: 0.9845\n",
      "Epoch 2462/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.1218 - val_accuracy: 0.9814\n",
      "Epoch 2463/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.1352 - val_accuracy: 0.9783\n",
      "Epoch 2464/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1161 - val_accuracy: 0.9845\n",
      "Epoch 2465/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.1100 - val_accuracy: 0.9845\n",
      "Epoch 2466/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0211 - accuracy: 0.9893 - val_loss: 0.1178 - val_accuracy: 0.9845\n",
      "Epoch 2467/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1281 - val_accuracy: 0.9814\n",
      "Epoch 2468/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.1223 - val_accuracy: 0.9845\n",
      "Epoch 2469/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.1168 - val_accuracy: 0.9845\n",
      "Epoch 2470/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.1190 - val_accuracy: 0.9845\n",
      "Epoch 2471/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.1196 - val_accuracy: 0.9845\n",
      "Epoch 2472/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1155 - val_accuracy: 0.9845\n",
      "Epoch 2473/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0195 - accuracy: 0.9923 - val_loss: 0.1225 - val_accuracy: 0.9814\n",
      "Epoch 2474/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0198 - accuracy: 0.9954 - val_loss: 0.1270 - val_accuracy: 0.9814\n",
      "Epoch 2475/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.1203 - val_accuracy: 0.9814\n",
      "Epoch 2476/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1186 - val_accuracy: 0.9845\n",
      "Epoch 2477/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0197 - accuracy: 0.9923 - val_loss: 0.1187 - val_accuracy: 0.9845\n",
      "Epoch 2478/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0190 - accuracy: 0.9923 - val_loss: 0.1331 - val_accuracy: 0.9783\n",
      "Epoch 2479/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0223 - accuracy: 0.9939 - val_loss: 0.1177 - val_accuracy: 0.9845\n",
      "Epoch 2480/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.1059 - val_accuracy: 0.9845\n",
      "Epoch 2481/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0256 - accuracy: 0.9893 - val_loss: 0.1190 - val_accuracy: 0.9845\n",
      "Epoch 2482/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.1349 - val_accuracy: 0.9783\n",
      "Epoch 2483/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0203 - accuracy: 0.9954 - val_loss: 0.1271 - val_accuracy: 0.9814\n",
      "Epoch 2484/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.1159 - val_accuracy: 0.9845\n",
      "Epoch 2485/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.1172 - val_accuracy: 0.9845\n",
      "Epoch 2486/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0199 - accuracy: 0.9923 - val_loss: 0.1199 - val_accuracy: 0.9845\n",
      "Epoch 2487/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1154 - val_accuracy: 0.9845\n",
      "Epoch 2488/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0194 - accuracy: 0.9923 - val_loss: 0.1196 - val_accuracy: 0.9845\n",
      "Epoch 2489/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0189 - accuracy: 0.9923 - val_loss: 0.1260 - val_accuracy: 0.9814\n",
      "Epoch 2490/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.1280 - val_accuracy: 0.9814\n",
      "Epoch 2491/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.1236 - val_accuracy: 0.9814\n",
      "Epoch 2492/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 0.1109 - val_accuracy: 0.9845\n",
      "Epoch 2493/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 23us/step - loss: 0.0218 - accuracy: 0.9908 - val_loss: 0.1151 - val_accuracy: 0.9845\n",
      "Epoch 2494/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.1333 - val_accuracy: 0.9814\n",
      "Epoch 2495/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0215 - accuracy: 0.9954 - val_loss: 0.1208 - val_accuracy: 0.9845\n",
      "Epoch 2496/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0196 - accuracy: 0.9923 - val_loss: 0.1094 - val_accuracy: 0.9845\n",
      "Epoch 2497/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0244 - accuracy: 0.9908 - val_loss: 0.1321 - val_accuracy: 0.9752\n",
      "Epoch 2498/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.1711 - val_accuracy: 0.9658\n",
      "Epoch 2499/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0322 - accuracy: 0.9877 - val_loss: 0.1395 - val_accuracy: 0.9752\n",
      "Epoch 2500/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0209 - accuracy: 0.9923 - val_loss: 0.1085 - val_accuracy: 0.9876\n",
      "Epoch 2501/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0241 - accuracy: 0.9908 - val_loss: 0.1135 - val_accuracy: 0.9845\n",
      "Epoch 2502/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0191 - accuracy: 0.9923 - val_loss: 0.1432 - val_accuracy: 0.9720\n",
      "Epoch 2503/3500\n",
      "653/653 [==============================] - 0s 32us/step - loss: 0.0258 - accuracy: 0.9908 - val_loss: 0.1345 - val_accuracy: 0.9814\n",
      "Epoch 2504/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0218 - accuracy: 0.9939 - val_loss: 0.1082 - val_accuracy: 0.9876\n",
      "Epoch 2505/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0235 - accuracy: 0.9893 - val_loss: 0.1203 - val_accuracy: 0.9845\n",
      "Epoch 2506/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.1408 - val_accuracy: 0.9783\n",
      "Epoch 2507/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0216 - accuracy: 0.9923 - val_loss: 0.1291 - val_accuracy: 0.9783\n",
      "Epoch 2508/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0216 - accuracy: 0.9923 - val_loss: 0.1213 - val_accuracy: 0.9814\n",
      "Epoch 2509/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.1278 - val_accuracy: 0.9814\n",
      "Epoch 2510/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.1189 - val_accuracy: 0.9845\n",
      "Epoch 2511/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1128 - val_accuracy: 0.9845\n",
      "Epoch 2512/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0200 - accuracy: 0.9908 - val_loss: 0.1248 - val_accuracy: 0.9814\n",
      "Epoch 2513/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.1426 - val_accuracy: 0.9720\n",
      "Epoch 2514/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0231 - accuracy: 0.9908 - val_loss: 0.1223 - val_accuracy: 0.9845\n",
      "Epoch 2515/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0207 - accuracy: 0.9923 - val_loss: 0.1092 - val_accuracy: 0.9845\n",
      "Epoch 2516/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0201 - accuracy: 0.9908 - val_loss: 0.1275 - val_accuracy: 0.9814\n",
      "Epoch 2517/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0203 - accuracy: 0.9954 - val_loss: 0.1421 - val_accuracy: 0.9752\n",
      "Epoch 2518/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0227 - accuracy: 0.9923 - val_loss: 0.1156 - val_accuracy: 0.9845\n",
      "Epoch 2519/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0221 - accuracy: 0.9908 - val_loss: 0.1061 - val_accuracy: 0.9845\n",
      "Epoch 2520/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0217 - accuracy: 0.9908 - val_loss: 0.1354 - val_accuracy: 0.9783\n",
      "Epoch 2521/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.1518 - val_accuracy: 0.9720\n",
      "Epoch 2522/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0250 - accuracy: 0.9908 - val_loss: 0.1207 - val_accuracy: 0.9814\n",
      "Epoch 2523/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.1054 - val_accuracy: 0.9876\n",
      "Epoch 2524/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0257 - accuracy: 0.9893 - val_loss: 0.1235 - val_accuracy: 0.9783\n",
      "Epoch 2525/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.1643 - val_accuracy: 0.9658\n",
      "Epoch 2526/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0305 - accuracy: 0.9908 - val_loss: 0.1374 - val_accuracy: 0.9783\n",
      "Epoch 2527/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.1050 - val_accuracy: 0.9845\n",
      "Epoch 2528/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0288 - accuracy: 0.9877 - val_loss: 0.1047 - val_accuracy: 0.9814\n",
      "Epoch 2529/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0294 - accuracy: 0.9877 - val_loss: 0.1280 - val_accuracy: 0.9783\n",
      "Epoch 2530/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.1349 - val_accuracy: 0.9783\n",
      "Epoch 2531/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.1140 - val_accuracy: 0.9814\n",
      "Epoch 2532/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0196 - accuracy: 0.9923 - val_loss: 0.1085 - val_accuracy: 0.9845\n",
      "Epoch 2533/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0222 - accuracy: 0.9908 - val_loss: 0.1152 - val_accuracy: 0.9845\n",
      "Epoch 2534/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.1303 - val_accuracy: 0.9783\n",
      "Epoch 2535/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.1294 - val_accuracy: 0.9814\n",
      "Epoch 2536/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.1213 - val_accuracy: 0.9845\n",
      "Epoch 2537/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.1271 - val_accuracy: 0.9845\n",
      "Epoch 2538/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.1286 - val_accuracy: 0.9814\n",
      "Epoch 2539/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1100 - val_accuracy: 0.9845\n",
      "Epoch 2540/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0212 - accuracy: 0.9923 - val_loss: 0.1070 - val_accuracy: 0.9845\n",
      "Epoch 2541/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0218 - accuracy: 0.9908 - val_loss: 0.1294 - val_accuracy: 0.9814\n",
      "Epoch 2542/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1472 - val_accuracy: 0.9752\n",
      "Epoch 2543/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0249 - accuracy: 0.9908 - val_loss: 0.1262 - val_accuracy: 0.9814\n",
      "Epoch 2544/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.1153 - val_accuracy: 0.9845\n",
      "Epoch 2545/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0199 - accuracy: 0.9923 - val_loss: 0.1216 - val_accuracy: 0.9845\n",
      "Epoch 2546/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1279 - val_accuracy: 0.9814\n",
      "Epoch 2547/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.1255 - val_accuracy: 0.9845\n",
      "Epoch 2548/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.1161 - val_accuracy: 0.9845\n",
      "Epoch 2549/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0199 - accuracy: 0.9908 - val_loss: 0.1170 - val_accuracy: 0.9845\n",
      "Epoch 2550/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0197 - accuracy: 0.9923 - val_loss: 0.1268 - val_accuracy: 0.9845\n",
      "Epoch 2551/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.1394 - val_accuracy: 0.9783\n",
      "Epoch 2552/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.1261 - val_accuracy: 0.9814\n",
      "Epoch 2553/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.1092 - val_accuracy: 0.9845\n",
      "Epoch 2554/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0227 - accuracy: 0.9893 - val_loss: 0.1120 - val_accuracy: 0.9845\n",
      "Epoch 2555/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0220 - accuracy: 0.9908 - val_loss: 0.1239 - val_accuracy: 0.9814\n",
      "Epoch 2556/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.1307 - val_accuracy: 0.9814\n",
      "Epoch 2557/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.1297 - val_accuracy: 0.9814\n",
      "Epoch 2558/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.1138 - val_accuracy: 0.9845\n",
      "Epoch 2559/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.1098 - val_accuracy: 0.9845\n",
      "Epoch 2560/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0220 - accuracy: 0.9908 - val_loss: 0.1221 - val_accuracy: 0.9845\n",
      "Epoch 2561/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.1406 - val_accuracy: 0.9752\n",
      "Epoch 2562/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0217 - accuracy: 0.9923 - val_loss: 0.1372 - val_accuracy: 0.9814\n",
      "Epoch 2563/3500\n",
      "653/653 [==============================] - 0s 63us/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.1103 - val_accuracy: 0.9845\n",
      "Epoch 2564/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0226 - accuracy: 0.9908 - val_loss: 0.1077 - val_accuracy: 0.9845\n",
      "Epoch 2565/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0251 - accuracy: 0.9893 - val_loss: 0.1267 - val_accuracy: 0.9814\n",
      "Epoch 2566/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1487 - val_accuracy: 0.9752\n",
      "Epoch 2567/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0228 - accuracy: 0.9908 - val_loss: 0.1389 - val_accuracy: 0.9783\n",
      "Epoch 2568/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.1185 - val_accuracy: 0.9845\n",
      "Epoch 2569/3500\n",
      "653/653 [==============================] - 0s 38us/step - loss: 0.0191 - accuracy: 0.9923 - val_loss: 0.1121 - val_accuracy: 0.9845\n",
      "Epoch 2570/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0213 - accuracy: 0.9908 - val_loss: 0.1273 - val_accuracy: 0.9845\n",
      "Epoch 2571/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.1348 - val_accuracy: 0.9783\n",
      "Epoch 2572/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.1159 - val_accuracy: 0.9845\n",
      "Epoch 2573/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0204 - accuracy: 0.9893 - val_loss: 0.1150 - val_accuracy: 0.9845\n",
      "Epoch 2574/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0199 - accuracy: 0.9923 - val_loss: 0.1337 - val_accuracy: 0.9783\n",
      "Epoch 2575/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.1381 - val_accuracy: 0.9752\n",
      "Epoch 2576/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0202 - accuracy: 0.9954 - val_loss: 0.1285 - val_accuracy: 0.9814\n",
      "Epoch 2577/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0195 - accuracy: 0.9923 - val_loss: 0.1229 - val_accuracy: 0.9845\n",
      "Epoch 2578/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0188 - accuracy: 0.9923 - val_loss: 0.1262 - val_accuracy: 0.9814\n",
      "Epoch 2579/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1214 - val_accuracy: 0.9845\n",
      "Epoch 2580/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1204 - val_accuracy: 0.9845\n",
      "Epoch 2581/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0184 - accuracy: 0.9923 - val_loss: 0.1337 - val_accuracy: 0.9814\n",
      "Epoch 2582/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.1439 - val_accuracy: 0.9720\n",
      "Epoch 2583/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0222 - accuracy: 0.9923 - val_loss: 0.1324 - val_accuracy: 0.9814\n",
      "Epoch 2584/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.1251 - val_accuracy: 0.9845\n",
      "Epoch 2585/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.1206 - val_accuracy: 0.9845\n",
      "Epoch 2586/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1265 - val_accuracy: 0.9814\n",
      "Epoch 2587/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.1307 - val_accuracy: 0.9814\n",
      "Epoch 2588/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 0.1179 - val_accuracy: 0.9845\n",
      "Epoch 2589/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0189 - accuracy: 0.9923 - val_loss: 0.1206 - val_accuracy: 0.9845\n",
      "Epoch 2590/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0190 - accuracy: 0.9923 - val_loss: 0.1254 - val_accuracy: 0.9814\n",
      "Epoch 2591/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.1157 - val_accuracy: 0.9845\n",
      "Epoch 2592/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0196 - accuracy: 0.9923 - val_loss: 0.1120 - val_accuracy: 0.9845\n",
      "Epoch 2593/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0201 - accuracy: 0.9893 - val_loss: 0.1262 - val_accuracy: 0.9814\n",
      "Epoch 2594/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.1373 - val_accuracy: 0.9783\n",
      "Epoch 2595/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.1192 - val_accuracy: 0.9845\n",
      "Epoch 2596/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0245 - accuracy: 0.9893 - val_loss: 0.1148 - val_accuracy: 0.9845\n",
      "Epoch 2597/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.1538 - val_accuracy: 0.9720\n",
      "Epoch 2598/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0248 - accuracy: 0.9908 - val_loss: 0.1515 - val_accuracy: 0.9720\n",
      "Epoch 2599/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0228 - accuracy: 0.9923 - val_loss: 0.1188 - val_accuracy: 0.9845\n",
      "Epoch 2600/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0203 - accuracy: 0.9908 - val_loss: 0.1145 - val_accuracy: 0.9845\n",
      "Epoch 2601/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1391 - val_accuracy: 0.9752\n",
      "Epoch 2602/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0222 - accuracy: 0.9908 - val_loss: 0.1318 - val_accuracy: 0.9814\n",
      "Epoch 2603/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 18us/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.1085 - val_accuracy: 0.9845\n",
      "Epoch 2604/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0257 - accuracy: 0.9893 - val_loss: 0.1281 - val_accuracy: 0.9814\n",
      "Epoch 2605/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0229 - accuracy: 0.9923 - val_loss: 0.1699 - val_accuracy: 0.9658\n",
      "Epoch 2606/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.1258 - val_accuracy: 0.9814\n",
      "Epoch 2607/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1055 - val_accuracy: 0.9845\n",
      "Epoch 2608/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0279 - accuracy: 0.9877 - val_loss: 0.1141 - val_accuracy: 0.9845\n",
      "Epoch 2609/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 0.1586 - val_accuracy: 0.9720\n",
      "Epoch 2610/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0273 - accuracy: 0.9908 - val_loss: 0.1512 - val_accuracy: 0.9720\n",
      "Epoch 2611/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0223 - accuracy: 0.9908 - val_loss: 0.1119 - val_accuracy: 0.9845\n",
      "Epoch 2612/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0229 - accuracy: 0.9893 - val_loss: 0.1100 - val_accuracy: 0.9845\n",
      "Epoch 2613/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0212 - accuracy: 0.9908 - val_loss: 0.1364 - val_accuracy: 0.9752\n",
      "Epoch 2614/3500\n",
      "653/653 [==============================] - 0s 61us/step - loss: 0.0202 - accuracy: 0.9954 - val_loss: 0.1423 - val_accuracy: 0.9752\n",
      "Epoch 2615/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.1180 - val_accuracy: 0.9845\n",
      "Epoch 2616/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0195 - accuracy: 0.9908 - val_loss: 0.1125 - val_accuracy: 0.9845\n",
      "Epoch 2617/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0210 - accuracy: 0.9923 - val_loss: 0.1264 - val_accuracy: 0.9814\n",
      "Epoch 2618/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0187 - accuracy: 0.9923 - val_loss: 0.1332 - val_accuracy: 0.9783\n",
      "Epoch 2619/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.1330 - val_accuracy: 0.9814\n",
      "Epoch 2620/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1319 - val_accuracy: 0.9814\n",
      "Epoch 2621/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1204 - val_accuracy: 0.9845\n",
      "Epoch 2622/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1203 - val_accuracy: 0.9845\n",
      "Epoch 2623/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0194 - accuracy: 0.9908 - val_loss: 0.1199 - val_accuracy: 0.9845\n",
      "Epoch 2624/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0191 - accuracy: 0.9908 - val_loss: 0.1189 - val_accuracy: 0.9845\n",
      "Epoch 2625/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0192 - accuracy: 0.9908 - val_loss: 0.1253 - val_accuracy: 0.9845\n",
      "Epoch 2626/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.1221 - val_accuracy: 0.9845\n",
      "Epoch 2627/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.1129 - val_accuracy: 0.9845\n",
      "Epoch 2628/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0215 - accuracy: 0.9908 - val_loss: 0.1200 - val_accuracy: 0.9845\n",
      "Epoch 2629/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.1451 - val_accuracy: 0.9752\n",
      "Epoch 2630/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0212 - accuracy: 0.9939 - val_loss: 0.1497 - val_accuracy: 0.9752\n",
      "Epoch 2631/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0225 - accuracy: 0.9923 - val_loss: 0.1287 - val_accuracy: 0.9814\n",
      "Epoch 2632/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0190 - accuracy: 0.9923 - val_loss: 0.1186 - val_accuracy: 0.9845\n",
      "Epoch 2633/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.1244 - val_accuracy: 0.9845\n",
      "Epoch 2634/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.1217 - val_accuracy: 0.9845\n",
      "Epoch 2635/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0197 - accuracy: 0.9923 - val_loss: 0.1207 - val_accuracy: 0.9845\n",
      "Epoch 2636/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0191 - accuracy: 0.9923 - val_loss: 0.1372 - val_accuracy: 0.9814\n",
      "Epoch 2637/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.1326 - val_accuracy: 0.9814\n",
      "Epoch 2638/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.1257 - val_accuracy: 0.9845\n",
      "Epoch 2639/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.1245 - val_accuracy: 0.9845\n",
      "Epoch 2640/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.1247 - val_accuracy: 0.9845\n",
      "Epoch 2641/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.1298 - val_accuracy: 0.9814\n",
      "Epoch 2642/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.1275 - val_accuracy: 0.9814\n",
      "Epoch 2643/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.1224 - val_accuracy: 0.9845\n",
      "Epoch 2644/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.1365 - val_accuracy: 0.9783\n",
      "Epoch 2645/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0197 - accuracy: 0.9954 - val_loss: 0.1333 - val_accuracy: 0.9814\n",
      "Epoch 2646/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.1160 - val_accuracy: 0.9845\n",
      "Epoch 2647/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0197 - accuracy: 0.9908 - val_loss: 0.1226 - val_accuracy: 0.9845\n",
      "Epoch 2648/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1299 - val_accuracy: 0.9814\n",
      "Epoch 2649/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.1200 - val_accuracy: 0.9845\n",
      "Epoch 2650/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0183 - accuracy: 0.9923 - val_loss: 0.1211 - val_accuracy: 0.9845\n",
      "Epoch 2651/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1290 - val_accuracy: 0.9814\n",
      "Epoch 2652/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.1314 - val_accuracy: 0.9814\n",
      "Epoch 2653/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1270 - val_accuracy: 0.9814\n",
      "Epoch 2654/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.1227 - val_accuracy: 0.9814\n",
      "Epoch 2655/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1170 - val_accuracy: 0.9845\n",
      "Epoch 2656/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1123 - val_accuracy: 0.9845\n",
      "Epoch 2657/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0204 - accuracy: 0.9908 - val_loss: 0.1229 - val_accuracy: 0.9845\n",
      "Epoch 2658/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.1300 - val_accuracy: 0.9814\n",
      "Epoch 2659/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.1215 - val_accuracy: 0.9845\n",
      "Epoch 2660/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 0.1214 - val_accuracy: 0.9845\n",
      "Epoch 2661/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1280 - val_accuracy: 0.9845\n",
      "Epoch 2662/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.1242 - val_accuracy: 0.9845\n",
      "Epoch 2663/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.1209 - val_accuracy: 0.9845\n",
      "Epoch 2664/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0187 - accuracy: 0.9908 - val_loss: 0.1163 - val_accuracy: 0.9845\n",
      "Epoch 2665/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0189 - accuracy: 0.9923 - val_loss: 0.1317 - val_accuracy: 0.9814\n",
      "Epoch 2666/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.1299 - val_accuracy: 0.9814\n",
      "Epoch 2667/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1123 - val_accuracy: 0.9845\n",
      "Epoch 2668/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 0.1109 - val_accuracy: 0.9845\n",
      "Epoch 2669/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0207 - accuracy: 0.9908 - val_loss: 0.1385 - val_accuracy: 0.9783\n",
      "Epoch 2670/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1661 - val_accuracy: 0.9689\n",
      "Epoch 2671/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0284 - accuracy: 0.9908 - val_loss: 0.1475 - val_accuracy: 0.9752\n",
      "Epoch 2672/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0206 - accuracy: 0.9923 - val_loss: 0.1141 - val_accuracy: 0.9845\n",
      "Epoch 2673/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0208 - accuracy: 0.9923 - val_loss: 0.1083 - val_accuracy: 0.9876\n",
      "Epoch 2674/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.1318 - val_accuracy: 0.9814\n",
      "Epoch 2675/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.1660 - val_accuracy: 0.9689\n",
      "Epoch 2676/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0281 - accuracy: 0.9908 - val_loss: 0.1433 - val_accuracy: 0.9783\n",
      "Epoch 2677/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0206 - accuracy: 0.9908 - val_loss: 0.1153 - val_accuracy: 0.9845\n",
      "Epoch 2678/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.1193 - val_accuracy: 0.9845\n",
      "Epoch 2679/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.1306 - val_accuracy: 0.9814\n",
      "Epoch 2680/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.1322 - val_accuracy: 0.9814\n",
      "Epoch 2681/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.1298 - val_accuracy: 0.9814\n",
      "Epoch 2682/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.1302 - val_accuracy: 0.9814\n",
      "Epoch 2683/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.1303 - val_accuracy: 0.9814\n",
      "Epoch 2684/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.1199 - val_accuracy: 0.9845\n",
      "Epoch 2685/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0181 - accuracy: 0.9923 - val_loss: 0.1167 - val_accuracy: 0.9845\n",
      "Epoch 2686/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0188 - accuracy: 0.9923 - val_loss: 0.1195 - val_accuracy: 0.9845\n",
      "Epoch 2687/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0184 - accuracy: 0.9923 - val_loss: 0.1217 - val_accuracy: 0.9845\n",
      "Epoch 2688/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0184 - accuracy: 0.9923 - val_loss: 0.1157 - val_accuracy: 0.9845\n",
      "Epoch 2689/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.1132 - val_accuracy: 0.9845\n",
      "Epoch 2690/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0199 - accuracy: 0.9908 - val_loss: 0.1278 - val_accuracy: 0.9814\n",
      "Epoch 2691/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1336 - val_accuracy: 0.9814\n",
      "Epoch 2692/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.1250 - val_accuracy: 0.9845\n",
      "Epoch 2693/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.1239 - val_accuracy: 0.9845\n",
      "Epoch 2694/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.1328 - val_accuracy: 0.9814\n",
      "Epoch 2695/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.1347 - val_accuracy: 0.9814\n",
      "Epoch 2696/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.1232 - val_accuracy: 0.9845\n",
      "Epoch 2697/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.1266 - val_accuracy: 0.9845\n",
      "Epoch 2698/3500\n",
      "653/653 [==============================] - 0s 48us/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1412 - val_accuracy: 0.9752\n",
      "Epoch 2699/3500\n",
      "653/653 [==============================] - 0s 37us/step - loss: 0.0198 - accuracy: 0.9954 - val_loss: 0.1375 - val_accuracy: 0.9814\n",
      "Epoch 2700/3500\n",
      "653/653 [==============================] - 0s 46us/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1212 - val_accuracy: 0.9845\n",
      "Epoch 2701/3500\n",
      "653/653 [==============================] - 0s 36us/step - loss: 0.0183 - accuracy: 0.9923 - val_loss: 0.1211 - val_accuracy: 0.9845\n",
      "Epoch 2702/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0182 - accuracy: 0.9923 - val_loss: 0.1285 - val_accuracy: 0.9845\n",
      "Epoch 2703/3500\n",
      "653/653 [==============================] - 0s 44us/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.1306 - val_accuracy: 0.9845\n",
      "Epoch 2704/3500\n",
      "653/653 [==============================] - 0s 35us/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.1290 - val_accuracy: 0.9845\n",
      "Epoch 2705/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.1247 - val_accuracy: 0.9845\n",
      "Epoch 2706/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.1284 - val_accuracy: 0.9845\n",
      "Epoch 2707/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 0.1262 - val_accuracy: 0.9845\n",
      "Epoch 2708/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.1194 - val_accuracy: 0.9845\n",
      "Epoch 2709/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0190 - accuracy: 0.9923 - val_loss: 0.1304 - val_accuracy: 0.9814\n",
      "Epoch 2710/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.1339 - val_accuracy: 0.9783\n",
      "Epoch 2711/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1210 - val_accuracy: 0.9814\n",
      "Epoch 2712/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 0.1253 - val_accuracy: 0.9814\n",
      "Epoch 2713/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 19us/step - loss: 0.0190 - accuracy: 0.9923 - val_loss: 0.1337 - val_accuracy: 0.9783\n",
      "Epoch 2714/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.1193 - val_accuracy: 0.9814\n",
      "Epoch 2715/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.1143 - val_accuracy: 0.9845\n",
      "Epoch 2716/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0204 - accuracy: 0.9908 - val_loss: 0.1243 - val_accuracy: 0.9845\n",
      "Epoch 2717/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.1386 - val_accuracy: 0.9783\n",
      "Epoch 2718/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.1306 - val_accuracy: 0.9814\n",
      "Epoch 2719/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0199 - accuracy: 0.9908 - val_loss: 0.1271 - val_accuracy: 0.9845\n",
      "Epoch 2720/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1424 - val_accuracy: 0.9752\n",
      "Epoch 2721/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 0.1322 - val_accuracy: 0.9814\n",
      "Epoch 2722/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.1194 - val_accuracy: 0.9845\n",
      "Epoch 2723/3500\n",
      "653/653 [==============================] - 0s 78us/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.1238 - val_accuracy: 0.9845\n",
      "Epoch 2724/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0184 - accuracy: 0.9923 - val_loss: 0.1284 - val_accuracy: 0.9814\n",
      "Epoch 2725/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.1247 - val_accuracy: 0.9845\n",
      "Epoch 2726/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0183 - accuracy: 0.9923 - val_loss: 0.1219 - val_accuracy: 0.9845\n",
      "Epoch 2727/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0194 - accuracy: 0.9923 - val_loss: 0.1262 - val_accuracy: 0.9845\n",
      "Epoch 2728/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.1515 - val_accuracy: 0.9720\n",
      "Epoch 2729/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0226 - accuracy: 0.9908 - val_loss: 0.1365 - val_accuracy: 0.9783\n",
      "Epoch 2730/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1120 - val_accuracy: 0.9845\n",
      "Epoch 2731/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0219 - accuracy: 0.9908 - val_loss: 0.1143 - val_accuracy: 0.9845\n",
      "Epoch 2732/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0196 - accuracy: 0.9923 - val_loss: 0.1360 - val_accuracy: 0.9814\n",
      "Epoch 2733/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0200 - accuracy: 0.9923 - val_loss: 0.1333 - val_accuracy: 0.9814\n",
      "Epoch 2734/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.1141 - val_accuracy: 0.9845\n",
      "Epoch 2735/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0205 - accuracy: 0.9908 - val_loss: 0.1229 - val_accuracy: 0.9845\n",
      "Epoch 2736/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0177 - accuracy: 0.9923 - val_loss: 0.1400 - val_accuracy: 0.9783\n",
      "Epoch 2737/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.1357 - val_accuracy: 0.9814\n",
      "Epoch 2738/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.1163 - val_accuracy: 0.9845\n",
      "Epoch 2739/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0218 - accuracy: 0.9908 - val_loss: 0.1290 - val_accuracy: 0.9845\n",
      "Epoch 2740/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.1673 - val_accuracy: 0.9720\n",
      "Epoch 2741/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0285 - accuracy: 0.9893 - val_loss: 0.1426 - val_accuracy: 0.9752\n",
      "Epoch 2742/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1106 - val_accuracy: 0.9876\n",
      "Epoch 2743/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0239 - accuracy: 0.9893 - val_loss: 0.1163 - val_accuracy: 0.9845\n",
      "Epoch 2744/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1390 - val_accuracy: 0.9783\n",
      "Epoch 2745/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.1308 - val_accuracy: 0.9814\n",
      "Epoch 2746/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.1207 - val_accuracy: 0.9845\n",
      "Epoch 2747/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0178 - accuracy: 0.9923 - val_loss: 0.1297 - val_accuracy: 0.9814\n",
      "Epoch 2748/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1410 - val_accuracy: 0.9783\n",
      "Epoch 2749/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1339 - val_accuracy: 0.9814\n",
      "Epoch 2750/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.1236 - val_accuracy: 0.9845\n",
      "Epoch 2751/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0179 - accuracy: 0.9923 - val_loss: 0.1363 - val_accuracy: 0.9783\n",
      "Epoch 2752/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.1442 - val_accuracy: 0.9752\n",
      "Epoch 2753/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1188 - val_accuracy: 0.9845\n",
      "Epoch 2754/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0196 - accuracy: 0.9908 - val_loss: 0.1168 - val_accuracy: 0.9845\n",
      "Epoch 2755/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.1425 - val_accuracy: 0.9783\n",
      "Epoch 2756/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0202 - accuracy: 0.9939 - val_loss: 0.1427 - val_accuracy: 0.9783\n",
      "Epoch 2757/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.1177 - val_accuracy: 0.9845\n",
      "Epoch 2758/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.1175 - val_accuracy: 0.9845\n",
      "Epoch 2759/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.1444 - val_accuracy: 0.9752\n",
      "Epoch 2760/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0224 - accuracy: 0.9923 - val_loss: 0.1353 - val_accuracy: 0.9814\n",
      "Epoch 2761/3500\n",
      "653/653 [==============================] - 0s 30us/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.1092 - val_accuracy: 0.9876\n",
      "Epoch 2762/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0232 - accuracy: 0.9893 - val_loss: 0.1170 - val_accuracy: 0.9845\n",
      "Epoch 2763/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.1444 - val_accuracy: 0.9752\n",
      "Epoch 2764/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.1312 - val_accuracy: 0.9814\n",
      "Epoch 2765/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1120 - val_accuracy: 0.9845\n",
      "Epoch 2766/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0213 - accuracy: 0.9893 - val_loss: 0.1155 - val_accuracy: 0.9845\n",
      "Epoch 2767/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0191 - accuracy: 0.9923 - val_loss: 0.1351 - val_accuracy: 0.9783\n",
      "Epoch 2768/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.1306 - val_accuracy: 0.9814\n",
      "Epoch 2769/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1143 - val_accuracy: 0.9845\n",
      "Epoch 2770/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0214 - accuracy: 0.9908 - val_loss: 0.1165 - val_accuracy: 0.9845\n",
      "Epoch 2771/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0186 - accuracy: 0.9908 - val_loss: 0.1459 - val_accuracy: 0.9720\n",
      "Epoch 2772/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0222 - accuracy: 0.9939 - val_loss: 0.1371 - val_accuracy: 0.9783\n",
      "Epoch 2773/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0205 - accuracy: 0.9923 - val_loss: 0.1143 - val_accuracy: 0.9845\n",
      "Epoch 2774/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0197 - accuracy: 0.9908 - val_loss: 0.1263 - val_accuracy: 0.9845\n",
      "Epoch 2775/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 0.1372 - val_accuracy: 0.9783\n",
      "Epoch 2776/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.1171 - val_accuracy: 0.9845\n",
      "Epoch 2777/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0192 - accuracy: 0.9908 - val_loss: 0.1202 - val_accuracy: 0.9845\n",
      "Epoch 2778/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 0.1448 - val_accuracy: 0.9752\n",
      "Epoch 2779/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0209 - accuracy: 0.9923 - val_loss: 0.1413 - val_accuracy: 0.9783\n",
      "Epoch 2780/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.1111 - val_accuracy: 0.9876\n",
      "Epoch 2781/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0239 - accuracy: 0.9893 - val_loss: 0.1093 - val_accuracy: 0.9814\n",
      "Epoch 2782/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0321 - accuracy: 0.9877 - val_loss: 0.1258 - val_accuracy: 0.9814\n",
      "Epoch 2783/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.1398 - val_accuracy: 0.9783\n",
      "Epoch 2784/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.1222 - val_accuracy: 0.9845\n",
      "Epoch 2785/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0178 - accuracy: 0.9923 - val_loss: 0.1211 - val_accuracy: 0.9845\n",
      "Epoch 2786/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0188 - accuracy: 0.9923 - val_loss: 0.1229 - val_accuracy: 0.9845\n",
      "Epoch 2787/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0176 - accuracy: 0.9923 - val_loss: 0.1204 - val_accuracy: 0.9845\n",
      "Epoch 2788/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0179 - accuracy: 0.9923 - val_loss: 0.1293 - val_accuracy: 0.9814\n",
      "Epoch 2789/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.1429 - val_accuracy: 0.9783\n",
      "Epoch 2790/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0203 - accuracy: 0.9923 - val_loss: 0.1325 - val_accuracy: 0.9814\n",
      "Epoch 2791/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.1128 - val_accuracy: 0.9876\n",
      "Epoch 2792/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0223 - accuracy: 0.9908 - val_loss: 0.1240 - val_accuracy: 0.9845\n",
      "Epoch 2793/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.1406 - val_accuracy: 0.9783\n",
      "Epoch 2794/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.1353 - val_accuracy: 0.9783\n",
      "Epoch 2795/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1231 - val_accuracy: 0.9845\n",
      "Epoch 2796/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0182 - accuracy: 0.9923 - val_loss: 0.1195 - val_accuracy: 0.9845\n",
      "Epoch 2797/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.1395 - val_accuracy: 0.9783\n",
      "Epoch 2798/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0210 - accuracy: 0.9908 - val_loss: 0.1403 - val_accuracy: 0.9783\n",
      "Epoch 2799/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.1112 - val_accuracy: 0.9845\n",
      "Epoch 2800/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0250 - accuracy: 0.9893 - val_loss: 0.1103 - val_accuracy: 0.9814\n",
      "Epoch 2801/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0256 - accuracy: 0.9908 - val_loss: 0.1350 - val_accuracy: 0.9814\n",
      "Epoch 2802/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.1657 - val_accuracy: 0.9720\n",
      "Epoch 2803/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0252 - accuracy: 0.9908 - val_loss: 0.1304 - val_accuracy: 0.9814\n",
      "Epoch 2804/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1109 - val_accuracy: 0.9845\n",
      "Epoch 2805/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0263 - accuracy: 0.9893 - val_loss: 0.1214 - val_accuracy: 0.9845\n",
      "Epoch 2806/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0173 - accuracy: 0.9923 - val_loss: 0.1552 - val_accuracy: 0.9720\n",
      "Epoch 2807/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0234 - accuracy: 0.9908 - val_loss: 0.1418 - val_accuracy: 0.9783\n",
      "Epoch 2808/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.1121 - val_accuracy: 0.9876\n",
      "Epoch 2809/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0245 - accuracy: 0.9893 - val_loss: 0.1134 - val_accuracy: 0.9876\n",
      "Epoch 2810/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0205 - accuracy: 0.9908 - val_loss: 0.1474 - val_accuracy: 0.9752\n",
      "Epoch 2811/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0215 - accuracy: 0.9939 - val_loss: 0.1758 - val_accuracy: 0.9658\n",
      "Epoch 2812/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0302 - accuracy: 0.9893 - val_loss: 0.1307 - val_accuracy: 0.9814\n",
      "Epoch 2813/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.1176 - val_accuracy: 0.9845\n",
      "Epoch 2814/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.1317 - val_accuracy: 0.9814\n",
      "Epoch 2815/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1466 - val_accuracy: 0.9783\n",
      "Epoch 2816/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.1371 - val_accuracy: 0.9783\n",
      "Epoch 2817/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.1224 - val_accuracy: 0.9814\n",
      "Epoch 2818/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.1278 - val_accuracy: 0.9814\n",
      "Epoch 2819/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0186 - accuracy: 0.9908 - val_loss: 0.1382 - val_accuracy: 0.9783\n",
      "Epoch 2820/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.1308 - val_accuracy: 0.9814\n",
      "Epoch 2821/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1313 - val_accuracy: 0.9845\n",
      "Epoch 2822/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.1297 - val_accuracy: 0.9845\n",
      "Epoch 2823/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 15us/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1254 - val_accuracy: 0.9845\n",
      "Epoch 2824/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.1275 - val_accuracy: 0.9845\n",
      "Epoch 2825/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1364 - val_accuracy: 0.9814\n",
      "Epoch 2826/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.1228 - val_accuracy: 0.9845\n",
      "Epoch 2827/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0180 - accuracy: 0.9923 - val_loss: 0.1148 - val_accuracy: 0.9845\n",
      "Epoch 2828/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0200 - accuracy: 0.9923 - val_loss: 0.1318 - val_accuracy: 0.9845\n",
      "Epoch 2829/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1438 - val_accuracy: 0.9783\n",
      "Epoch 2830/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1257 - val_accuracy: 0.9845\n",
      "Epoch 2831/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0176 - accuracy: 0.9923 - val_loss: 0.1213 - val_accuracy: 0.9845\n",
      "Epoch 2832/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0181 - accuracy: 0.9923 - val_loss: 0.1310 - val_accuracy: 0.9845\n",
      "Epoch 2833/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1476 - val_accuracy: 0.9783\n",
      "Epoch 2834/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0209 - accuracy: 0.9939 - val_loss: 0.1375 - val_accuracy: 0.9845\n",
      "Epoch 2835/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0191 - accuracy: 0.9954 - val_loss: 0.1276 - val_accuracy: 0.9845\n",
      "Epoch 2836/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.1282 - val_accuracy: 0.9845\n",
      "Epoch 2837/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1296 - val_accuracy: 0.9845\n",
      "Epoch 2838/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1286 - val_accuracy: 0.9845\n",
      "Epoch 2839/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.1265 - val_accuracy: 0.9845\n",
      "Epoch 2840/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1262 - val_accuracy: 0.9845\n",
      "Epoch 2841/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.1294 - val_accuracy: 0.9845\n",
      "Epoch 2842/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1238 - val_accuracy: 0.9845\n",
      "Epoch 2843/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0176 - accuracy: 0.9923 - val_loss: 0.1289 - val_accuracy: 0.9845\n",
      "Epoch 2844/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.1446 - val_accuracy: 0.9752\n",
      "Epoch 2845/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.1408 - val_accuracy: 0.9814\n",
      "Epoch 2846/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.1198 - val_accuracy: 0.9845\n",
      "Epoch 2847/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0194 - accuracy: 0.9908 - val_loss: 0.1173 - val_accuracy: 0.9845\n",
      "Epoch 2848/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1420 - val_accuracy: 0.9783\n",
      "Epoch 2849/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.1376 - val_accuracy: 0.9814\n",
      "Epoch 2850/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.1196 - val_accuracy: 0.9845\n",
      "Epoch 2851/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0182 - accuracy: 0.9923 - val_loss: 0.1244 - val_accuracy: 0.9845\n",
      "Epoch 2852/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.1239 - val_accuracy: 0.9845\n",
      "Epoch 2853/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 0.1160 - val_accuracy: 0.9845\n",
      "Epoch 2854/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0200 - accuracy: 0.9923 - val_loss: 0.1250 - val_accuracy: 0.9845\n",
      "Epoch 2855/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1568 - val_accuracy: 0.9752\n",
      "Epoch 2856/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0247 - accuracy: 0.9908 - val_loss: 0.1413 - val_accuracy: 0.9783\n",
      "Epoch 2857/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1110 - val_accuracy: 0.9845\n",
      "Epoch 2858/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0252 - accuracy: 0.9893 - val_loss: 0.1159 - val_accuracy: 0.9845\n",
      "Epoch 2859/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1507 - val_accuracy: 0.9752\n",
      "Epoch 2860/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0219 - accuracy: 0.9923 - val_loss: 0.1479 - val_accuracy: 0.9752\n",
      "Epoch 2861/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1146 - val_accuracy: 0.9845\n",
      "Epoch 2862/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0217 - accuracy: 0.9893 - val_loss: 0.1120 - val_accuracy: 0.9876\n",
      "Epoch 2863/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0236 - accuracy: 0.9893 - val_loss: 0.1380 - val_accuracy: 0.9783\n",
      "Epoch 2864/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.1477 - val_accuracy: 0.9752\n",
      "Epoch 2865/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0201 - accuracy: 0.9923 - val_loss: 0.1289 - val_accuracy: 0.9845\n",
      "Epoch 2866/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.1175 - val_accuracy: 0.9845\n",
      "Epoch 2867/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0199 - accuracy: 0.9908 - val_loss: 0.1175 - val_accuracy: 0.9845\n",
      "Epoch 2868/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.1277 - val_accuracy: 0.9845\n",
      "Epoch 2869/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.1399 - val_accuracy: 0.9783\n",
      "Epoch 2870/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.1289 - val_accuracy: 0.9845\n",
      "Epoch 2871/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0176 - accuracy: 0.9923 - val_loss: 0.1225 - val_accuracy: 0.9845\n",
      "Epoch 2872/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.1348 - val_accuracy: 0.9814\n",
      "Epoch 2873/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0185 - accuracy: 0.9954 - val_loss: 0.1229 - val_accuracy: 0.9845\n",
      "Epoch 2874/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1182 - val_accuracy: 0.9845\n",
      "Epoch 2875/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0203 - accuracy: 0.9908 - val_loss: 0.1404 - val_accuracy: 0.9783\n",
      "Epoch 2876/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.1384 - val_accuracy: 0.9783\n",
      "Epoch 2877/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1242 - val_accuracy: 0.9845\n",
      "Epoch 2878/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0171 - accuracy: 0.9923 - val_loss: 0.1200 - val_accuracy: 0.9845\n",
      "Epoch 2879/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0178 - accuracy: 0.9923 - val_loss: 0.1370 - val_accuracy: 0.9814\n",
      "Epoch 2880/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.1550 - val_accuracy: 0.9752\n",
      "Epoch 2881/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0220 - accuracy: 0.9908 - val_loss: 0.1404 - val_accuracy: 0.9814\n",
      "Epoch 2882/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.1224 - val_accuracy: 0.9845\n",
      "Epoch 2883/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0176 - accuracy: 0.9923 - val_loss: 0.1165 - val_accuracy: 0.9845\n",
      "Epoch 2884/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0200 - accuracy: 0.9893 - val_loss: 0.1262 - val_accuracy: 0.9845\n",
      "Epoch 2885/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0168 - accuracy: 0.9923 - val_loss: 0.1537 - val_accuracy: 0.9752\n",
      "Epoch 2886/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0213 - accuracy: 0.9923 - val_loss: 0.1462 - val_accuracy: 0.9752\n",
      "Epoch 2887/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.1297 - val_accuracy: 0.9845\n",
      "Epoch 2888/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.1224 - val_accuracy: 0.9845\n",
      "Epoch 2889/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0176 - accuracy: 0.9923 - val_loss: 0.1225 - val_accuracy: 0.9845\n",
      "Epoch 2890/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0175 - accuracy: 0.9923 - val_loss: 0.1368 - val_accuracy: 0.9814\n",
      "Epoch 2891/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.1377 - val_accuracy: 0.9814\n",
      "Epoch 2892/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.1293 - val_accuracy: 0.9845\n",
      "Epoch 2893/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0175 - accuracy: 0.9939 - val_loss: 0.1402 - val_accuracy: 0.9814\n",
      "Epoch 2894/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.1478 - val_accuracy: 0.9752\n",
      "Epoch 2895/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.1266 - val_accuracy: 0.9845\n",
      "Epoch 2896/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0171 - accuracy: 0.9923 - val_loss: 0.1183 - val_accuracy: 0.9845\n",
      "Epoch 2897/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.1209 - val_accuracy: 0.9845\n",
      "Epoch 2898/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.1339 - val_accuracy: 0.9814\n",
      "Epoch 2899/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.1472 - val_accuracy: 0.9752\n",
      "Epoch 2900/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.1348 - val_accuracy: 0.9814\n",
      "Epoch 2901/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.1173 - val_accuracy: 0.9845\n",
      "Epoch 2902/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1238 - val_accuracy: 0.9845\n",
      "Epoch 2903/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.1399 - val_accuracy: 0.9783\n",
      "Epoch 2904/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.1469 - val_accuracy: 0.9783\n",
      "Epoch 2905/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 0.1256 - val_accuracy: 0.9845\n",
      "Epoch 2906/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0184 - accuracy: 0.9923 - val_loss: 0.1150 - val_accuracy: 0.9876\n",
      "Epoch 2907/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0223 - accuracy: 0.9908 - val_loss: 0.1368 - val_accuracy: 0.9814\n",
      "Epoch 2908/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0175 - accuracy: 0.9939 - val_loss: 0.1523 - val_accuracy: 0.9752\n",
      "Epoch 2909/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0195 - accuracy: 0.9923 - val_loss: 0.1273 - val_accuracy: 0.9845\n",
      "Epoch 2910/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0202 - accuracy: 0.9923 - val_loss: 0.1297 - val_accuracy: 0.9814\n",
      "Epoch 2911/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 0.1650 - val_accuracy: 0.9720\n",
      "Epoch 2912/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0242 - accuracy: 0.9908 - val_loss: 0.1497 - val_accuracy: 0.9752\n",
      "Epoch 2913/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0215 - accuracy: 0.9923 - val_loss: 0.1276 - val_accuracy: 0.9814\n",
      "Epoch 2914/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.1362 - val_accuracy: 0.9814\n",
      "Epoch 2915/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.1395 - val_accuracy: 0.9814\n",
      "Epoch 2916/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.1228 - val_accuracy: 0.9845\n",
      "Epoch 2917/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0191 - accuracy: 0.9923 - val_loss: 0.1378 - val_accuracy: 0.9814\n",
      "Epoch 2918/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.1621 - val_accuracy: 0.9720\n",
      "Epoch 2919/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.1318 - val_accuracy: 0.9845\n",
      "Epoch 2920/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.1148 - val_accuracy: 0.9876\n",
      "Epoch 2921/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0219 - accuracy: 0.9893 - val_loss: 0.1216 - val_accuracy: 0.9845\n",
      "Epoch 2922/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0174 - accuracy: 0.9923 - val_loss: 0.1367 - val_accuracy: 0.9814\n",
      "Epoch 2923/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.1406 - val_accuracy: 0.9783\n",
      "Epoch 2924/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1320 - val_accuracy: 0.9845\n",
      "Epoch 2925/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.1195 - val_accuracy: 0.9845\n",
      "Epoch 2926/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1234 - val_accuracy: 0.9845\n",
      "Epoch 2927/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0192 - accuracy: 0.9908 - val_loss: 0.1391 - val_accuracy: 0.9783\n",
      "Epoch 2928/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.1326 - val_accuracy: 0.9845\n",
      "Epoch 2929/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.1312 - val_accuracy: 0.9845\n",
      "Epoch 2930/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.1333 - val_accuracy: 0.9845\n",
      "Epoch 2931/3500\n",
      "653/653 [==============================] - 0s 40us/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.1341 - val_accuracy: 0.9845\n",
      "Epoch 2932/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.1340 - val_accuracy: 0.9814\n",
      "Epoch 2933/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 29us/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.1359 - val_accuracy: 0.9814\n",
      "Epoch 2934/3500\n",
      "653/653 [==============================] - 0s 37us/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1317 - val_accuracy: 0.9845\n",
      "Epoch 2935/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.1344 - val_accuracy: 0.9814\n",
      "Epoch 2936/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.1376 - val_accuracy: 0.9783\n",
      "Epoch 2937/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1325 - val_accuracy: 0.9814\n",
      "Epoch 2938/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.1327 - val_accuracy: 0.9814\n",
      "Epoch 2939/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1288 - val_accuracy: 0.9845\n",
      "Epoch 2940/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1259 - val_accuracy: 0.9845\n",
      "Epoch 2941/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0168 - accuracy: 0.9923 - val_loss: 0.1282 - val_accuracy: 0.9845\n",
      "Epoch 2942/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.1382 - val_accuracy: 0.9814\n",
      "Epoch 2943/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1427 - val_accuracy: 0.9783\n",
      "Epoch 2944/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1288 - val_accuracy: 0.9845\n",
      "Epoch 2945/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.1188 - val_accuracy: 0.9845\n",
      "Epoch 2946/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0184 - accuracy: 0.9923 - val_loss: 0.1412 - val_accuracy: 0.9814\n",
      "Epoch 2947/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0223 - accuracy: 0.9939 - val_loss: 0.1475 - val_accuracy: 0.9783\n",
      "Epoch 2948/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.1148 - val_accuracy: 0.9876\n",
      "Epoch 2949/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0243 - accuracy: 0.9893 - val_loss: 0.1163 - val_accuracy: 0.9876\n",
      "Epoch 2950/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0187 - accuracy: 0.9908 - val_loss: 0.1586 - val_accuracy: 0.9752\n",
      "Epoch 2951/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0237 - accuracy: 0.9908 - val_loss: 0.1706 - val_accuracy: 0.9689\n",
      "Epoch 2952/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0239 - accuracy: 0.9908 - val_loss: 0.1216 - val_accuracy: 0.9845\n",
      "Epoch 2953/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.1135 - val_accuracy: 0.9814\n",
      "Epoch 2954/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0272 - accuracy: 0.9893 - val_loss: 0.1339 - val_accuracy: 0.9814\n",
      "Epoch 2955/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.1617 - val_accuracy: 0.9752\n",
      "Epoch 2956/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.1263 - val_accuracy: 0.9845\n",
      "Epoch 2957/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0170 - accuracy: 0.9923 - val_loss: 0.1162 - val_accuracy: 0.9876\n",
      "Epoch 2958/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0210 - accuracy: 0.9908 - val_loss: 0.1285 - val_accuracy: 0.9814\n",
      "Epoch 2959/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0165 - accuracy: 0.9939 - val_loss: 0.1488 - val_accuracy: 0.9752\n",
      "Epoch 2960/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.1483 - val_accuracy: 0.9752\n",
      "Epoch 2961/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1313 - val_accuracy: 0.9814\n",
      "Epoch 2962/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0165 - accuracy: 0.9939 - val_loss: 0.1211 - val_accuracy: 0.9845\n",
      "Epoch 2963/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.1275 - val_accuracy: 0.9845\n",
      "Epoch 2964/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0162 - accuracy: 0.9923 - val_loss: 0.1460 - val_accuracy: 0.9783\n",
      "Epoch 2965/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.1453 - val_accuracy: 0.9783\n",
      "Epoch 2966/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1210 - val_accuracy: 0.9845\n",
      "Epoch 2967/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0183 - accuracy: 0.9923 - val_loss: 0.1177 - val_accuracy: 0.9845\n",
      "Epoch 2968/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1406 - val_accuracy: 0.9783\n",
      "Epoch 2969/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1740 - val_accuracy: 0.9689\n",
      "Epoch 2970/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0276 - accuracy: 0.9893 - val_loss: 0.1408 - val_accuracy: 0.9783\n",
      "Epoch 2971/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.1130 - val_accuracy: 0.9814\n",
      "Epoch 2972/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0314 - accuracy: 0.9893 - val_loss: 0.1168 - val_accuracy: 0.9845\n",
      "Epoch 2973/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1652 - val_accuracy: 0.9720\n",
      "Epoch 2974/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0269 - accuracy: 0.9893 - val_loss: 0.1664 - val_accuracy: 0.9720\n",
      "Epoch 2975/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0240 - accuracy: 0.9908 - val_loss: 0.1174 - val_accuracy: 0.9845\n",
      "Epoch 2976/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0210 - accuracy: 0.9908 - val_loss: 0.1145 - val_accuracy: 0.9845\n",
      "Epoch 2977/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0252 - accuracy: 0.9893 - val_loss: 0.1394 - val_accuracy: 0.9783\n",
      "Epoch 2978/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.1863 - val_accuracy: 0.9658\n",
      "Epoch 2979/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0342 - accuracy: 0.9877 - val_loss: 0.1475 - val_accuracy: 0.9783\n",
      "Epoch 2980/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0190 - accuracy: 0.9908 - val_loss: 0.1129 - val_accuracy: 0.9814\n",
      "Epoch 2981/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0276 - accuracy: 0.9893 - val_loss: 0.1183 - val_accuracy: 0.9845\n",
      "Epoch 2982/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0183 - accuracy: 0.9923 - val_loss: 0.1488 - val_accuracy: 0.9783\n",
      "Epoch 2983/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.1488 - val_accuracy: 0.9783\n",
      "Epoch 2984/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.1174 - val_accuracy: 0.9845\n",
      "Epoch 2985/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0206 - accuracy: 0.9893 - val_loss: 0.1141 - val_accuracy: 0.9876\n",
      "Epoch 2986/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0216 - accuracy: 0.9908 - val_loss: 0.1421 - val_accuracy: 0.9814\n",
      "Epoch 2987/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0226 - accuracy: 0.9923 - val_loss: 0.1391 - val_accuracy: 0.9814\n",
      "Epoch 2988/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1121 - val_accuracy: 0.9845\n",
      "Epoch 2989/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0267 - accuracy: 0.9893 - val_loss: 0.1217 - val_accuracy: 0.9845\n",
      "Epoch 2990/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.1566 - val_accuracy: 0.9752\n",
      "Epoch 2991/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0220 - accuracy: 0.9908 - val_loss: 0.1414 - val_accuracy: 0.9783\n",
      "Epoch 2992/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.1250 - val_accuracy: 0.9845\n",
      "Epoch 2993/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0173 - accuracy: 0.9923 - val_loss: 0.1297 - val_accuracy: 0.9814\n",
      "Epoch 2994/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.1441 - val_accuracy: 0.9752\n",
      "Epoch 2995/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.1407 - val_accuracy: 0.9783\n",
      "Epoch 2996/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.1227 - val_accuracy: 0.9845\n",
      "Epoch 2997/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0181 - accuracy: 0.9923 - val_loss: 0.1239 - val_accuracy: 0.9845\n",
      "Epoch 2998/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.1460 - val_accuracy: 0.9783\n",
      "Epoch 2999/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0188 - accuracy: 0.9954 - val_loss: 0.1467 - val_accuracy: 0.9783\n",
      "Epoch 3000/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.1238 - val_accuracy: 0.9845\n",
      "Epoch 3001/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0188 - accuracy: 0.9908 - val_loss: 0.1166 - val_accuracy: 0.9845\n",
      "Epoch 3002/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0197 - accuracy: 0.9923 - val_loss: 0.1393 - val_accuracy: 0.9814\n",
      "Epoch 3003/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0187 - accuracy: 0.9923 - val_loss: 0.1437 - val_accuracy: 0.9783\n",
      "Epoch 3004/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1185 - val_accuracy: 0.9845\n",
      "Epoch 3005/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0191 - accuracy: 0.9893 - val_loss: 0.1186 - val_accuracy: 0.9845\n",
      "Epoch 3006/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0194 - accuracy: 0.9908 - val_loss: 0.1341 - val_accuracy: 0.9814\n",
      "Epoch 3007/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.1377 - val_accuracy: 0.9814\n",
      "Epoch 3008/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.1322 - val_accuracy: 0.9845\n",
      "Epoch 3009/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.1260 - val_accuracy: 0.9845\n",
      "Epoch 3010/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0165 - accuracy: 0.9923 - val_loss: 0.1197 - val_accuracy: 0.9845\n",
      "Epoch 3011/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1254 - val_accuracy: 0.9845\n",
      "Epoch 3012/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0167 - accuracy: 0.9923 - val_loss: 0.1445 - val_accuracy: 0.9783\n",
      "Epoch 3013/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1491 - val_accuracy: 0.9752\n",
      "Epoch 3014/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0187 - accuracy: 0.9939 - val_loss: 0.1378 - val_accuracy: 0.9814\n",
      "Epoch 3015/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.1248 - val_accuracy: 0.9845\n",
      "Epoch 3016/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0170 - accuracy: 0.9923 - val_loss: 0.1355 - val_accuracy: 0.9814\n",
      "Epoch 3017/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1450 - val_accuracy: 0.9783\n",
      "Epoch 3018/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.1296 - val_accuracy: 0.9845\n",
      "Epoch 3019/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.1237 - val_accuracy: 0.9845\n",
      "Epoch 3020/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0180 - accuracy: 0.9923 - val_loss: 0.1305 - val_accuracy: 0.9845\n",
      "Epoch 3021/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1258 - val_accuracy: 0.9845\n",
      "Epoch 3022/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0167 - accuracy: 0.9923 - val_loss: 0.1274 - val_accuracy: 0.9845\n",
      "Epoch 3023/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0167 - accuracy: 0.9923 - val_loss: 0.1347 - val_accuracy: 0.9814\n",
      "Epoch 3024/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.1312 - val_accuracy: 0.9845\n",
      "Epoch 3025/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.1320 - val_accuracy: 0.9845\n",
      "Epoch 3026/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0170 - accuracy: 0.9954 - val_loss: 0.1277 - val_accuracy: 0.9845\n",
      "Epoch 3027/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0166 - accuracy: 0.9923 - val_loss: 0.1228 - val_accuracy: 0.9845\n",
      "Epoch 3028/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0173 - accuracy: 0.9939 - val_loss: 0.1388 - val_accuracy: 0.9783\n",
      "Epoch 3029/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1466 - val_accuracy: 0.9783\n",
      "Epoch 3030/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.1297 - val_accuracy: 0.9845\n",
      "Epoch 3031/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0164 - accuracy: 0.9939 - val_loss: 0.1214 - val_accuracy: 0.9845\n",
      "Epoch 3032/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0182 - accuracy: 0.9923 - val_loss: 0.1267 - val_accuracy: 0.9845\n",
      "Epoch 3033/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0163 - accuracy: 0.9923 - val_loss: 0.1476 - val_accuracy: 0.9752\n",
      "Epoch 3034/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.1496 - val_accuracy: 0.9752\n",
      "Epoch 3035/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.1351 - val_accuracy: 0.9814\n",
      "Epoch 3036/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.1317 - val_accuracy: 0.9845\n",
      "Epoch 3037/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0167 - accuracy: 0.9939 - val_loss: 0.1335 - val_accuracy: 0.9814\n",
      "Epoch 3038/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.1256 - val_accuracy: 0.9845\n",
      "Epoch 3039/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0170 - accuracy: 0.9939 - val_loss: 0.1188 - val_accuracy: 0.9845\n",
      "Epoch 3040/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0197 - accuracy: 0.9923 - val_loss: 0.1330 - val_accuracy: 0.9845\n",
      "Epoch 3041/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1687 - val_accuracy: 0.9720\n",
      "Epoch 3042/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0242 - accuracy: 0.9908 - val_loss: 0.1542 - val_accuracy: 0.9752\n",
      "Epoch 3043/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 15us/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 0.1235 - val_accuracy: 0.9845\n",
      "Epoch 3044/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0202 - accuracy: 0.9908 - val_loss: 0.1269 - val_accuracy: 0.9845\n",
      "Epoch 3045/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0176 - accuracy: 0.9923 - val_loss: 0.1530 - val_accuracy: 0.9752\n",
      "Epoch 3046/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.1472 - val_accuracy: 0.9783\n",
      "Epoch 3047/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0189 - accuracy: 0.9954 - val_loss: 0.1301 - val_accuracy: 0.9845\n",
      "Epoch 3048/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0165 - accuracy: 0.9939 - val_loss: 0.1291 - val_accuracy: 0.9845\n",
      "Epoch 3049/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0167 - accuracy: 0.9939 - val_loss: 0.1320 - val_accuracy: 0.9845\n",
      "Epoch 3050/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.1246 - val_accuracy: 0.9845\n",
      "Epoch 3051/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0169 - accuracy: 0.9923 - val_loss: 0.1196 - val_accuracy: 0.9845\n",
      "Epoch 3052/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0187 - accuracy: 0.9923 - val_loss: 0.1256 - val_accuracy: 0.9845\n",
      "Epoch 3053/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0166 - accuracy: 0.9923 - val_loss: 0.1437 - val_accuracy: 0.9783\n",
      "Epoch 3054/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1395 - val_accuracy: 0.9783\n",
      "Epoch 3055/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1256 - val_accuracy: 0.9845\n",
      "Epoch 3056/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0173 - accuracy: 0.9923 - val_loss: 0.1356 - val_accuracy: 0.9814\n",
      "Epoch 3057/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.1531 - val_accuracy: 0.9752\n",
      "Epoch 3058/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.1368 - val_accuracy: 0.9814\n",
      "Epoch 3059/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.1231 - val_accuracy: 0.9845\n",
      "Epoch 3060/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0179 - accuracy: 0.9923 - val_loss: 0.1289 - val_accuracy: 0.9845\n",
      "Epoch 3061/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1489 - val_accuracy: 0.9783\n",
      "Epoch 3062/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.1463 - val_accuracy: 0.9783\n",
      "Epoch 3063/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.1305 - val_accuracy: 0.9845\n",
      "Epoch 3064/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0162 - accuracy: 0.9939 - val_loss: 0.1227 - val_accuracy: 0.9845\n",
      "Epoch 3065/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.1317 - val_accuracy: 0.9845\n",
      "Epoch 3066/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.1327 - val_accuracy: 0.9814\n",
      "Epoch 3067/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0166 - accuracy: 0.9939 - val_loss: 0.1245 - val_accuracy: 0.9845\n",
      "Epoch 3068/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0175 - accuracy: 0.9939 - val_loss: 0.1345 - val_accuracy: 0.9814\n",
      "Epoch 3069/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.1498 - val_accuracy: 0.9752\n",
      "Epoch 3070/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.1570 - val_accuracy: 0.9752\n",
      "Epoch 3071/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.1305 - val_accuracy: 0.9845\n",
      "Epoch 3072/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0171 - accuracy: 0.9923 - val_loss: 0.1196 - val_accuracy: 0.9876\n",
      "Epoch 3073/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0198 - accuracy: 0.9908 - val_loss: 0.1381 - val_accuracy: 0.9814\n",
      "Epoch 3074/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.1531 - val_accuracy: 0.9752\n",
      "Epoch 3075/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1371 - val_accuracy: 0.9814\n",
      "Epoch 3076/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.1190 - val_accuracy: 0.9876\n",
      "Epoch 3077/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0206 - accuracy: 0.9908 - val_loss: 0.1215 - val_accuracy: 0.9845\n",
      "Epoch 3078/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.1534 - val_accuracy: 0.9752\n",
      "Epoch 3079/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.1565 - val_accuracy: 0.9752\n",
      "Epoch 3080/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 0.1208 - val_accuracy: 0.9845\n",
      "Epoch 3081/3500\n",
      "653/653 [==============================] - 0s 111us/step - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.1195 - val_accuracy: 0.9876\n",
      "Epoch 3082/3500\n",
      "653/653 [==============================] - 0s 37us/step - loss: 0.0182 - accuracy: 0.9923 - val_loss: 0.1538 - val_accuracy: 0.9752\n",
      "Epoch 3083/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.1572 - val_accuracy: 0.9752\n",
      "Epoch 3084/3500\n",
      "653/653 [==============================] - 0s 44us/step - loss: 0.0191 - accuracy: 0.9923 - val_loss: 0.1251 - val_accuracy: 0.9845\n",
      "Epoch 3085/3500\n",
      "653/653 [==============================] - 0s 36us/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.1208 - val_accuracy: 0.9876\n",
      "Epoch 3086/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0192 - accuracy: 0.9923 - val_loss: 0.1404 - val_accuracy: 0.9783\n",
      "Epoch 3087/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.1599 - val_accuracy: 0.9720\n",
      "Epoch 3088/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0190 - accuracy: 0.9923 - val_loss: 0.1246 - val_accuracy: 0.9845\n",
      "Epoch 3089/3500\n",
      "653/653 [==============================] - 0s 44us/step - loss: 0.0189 - accuracy: 0.9923 - val_loss: 0.1177 - val_accuracy: 0.9845\n",
      "Epoch 3090/3500\n",
      "653/653 [==============================] - 0s 116us/step - loss: 0.0225 - accuracy: 0.9908 - val_loss: 0.1463 - val_accuracy: 0.9752\n",
      "Epoch 3091/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1892 - val_accuracy: 0.9658\n",
      "Epoch 3092/3500\n",
      "653/653 [==============================] - 0s 46us/step - loss: 0.0310 - accuracy: 0.9893 - val_loss: 0.1450 - val_accuracy: 0.9752\n",
      "Epoch 3093/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.1199 - val_accuracy: 0.9876\n",
      "Epoch 3094/3500\n",
      "653/653 [==============================] - 0s 33us/step - loss: 0.0206 - accuracy: 0.9939 - val_loss: 0.1327 - val_accuracy: 0.9845\n",
      "Epoch 3095/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1596 - val_accuracy: 0.9752\n",
      "Epoch 3096/3500\n",
      "653/653 [==============================] - 0s 30us/step - loss: 0.0211 - accuracy: 0.9908 - val_loss: 0.1519 - val_accuracy: 0.9752\n",
      "Epoch 3097/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.1225 - val_accuracy: 0.9845\n",
      "Epoch 3098/3500\n",
      "653/653 [==============================] - 0s 33us/step - loss: 0.0211 - accuracy: 0.9893 - val_loss: 0.1337 - val_accuracy: 0.9845\n",
      "Epoch 3099/3500\n",
      "653/653 [==============================] - 0s 63us/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.1608 - val_accuracy: 0.9752\n",
      "Epoch 3100/3500\n",
      "653/653 [==============================] - 0s 47us/step - loss: 0.0205 - accuracy: 0.9923 - val_loss: 0.1313 - val_accuracy: 0.9814\n",
      "Epoch 3101/3500\n",
      "653/653 [==============================] - 0s 122us/step - loss: 0.0175 - accuracy: 0.9923 - val_loss: 0.1161 - val_accuracy: 0.9876\n",
      "Epoch 3102/3500\n",
      "653/653 [==============================] - 0s 50us/step - loss: 0.0230 - accuracy: 0.9893 - val_loss: 0.1365 - val_accuracy: 0.9814\n",
      "Epoch 3103/3500\n",
      "653/653 [==============================] - 0s 42us/step - loss: 0.0174 - accuracy: 0.9939 - val_loss: 0.1736 - val_accuracy: 0.9720\n",
      "Epoch 3104/3500\n",
      "653/653 [==============================] - 0s 69us/step - loss: 0.0262 - accuracy: 0.9908 - val_loss: 0.1470 - val_accuracy: 0.9752\n",
      "Epoch 3105/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.1220 - val_accuracy: 0.9845\n",
      "Epoch 3106/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1218 - val_accuracy: 0.9845\n",
      "Epoch 3107/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.1427 - val_accuracy: 0.9783\n",
      "Epoch 3108/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1609 - val_accuracy: 0.9752\n",
      "Epoch 3109/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0214 - accuracy: 0.9923 - val_loss: 0.1402 - val_accuracy: 0.9783\n",
      "Epoch 3110/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.1293 - val_accuracy: 0.9845\n",
      "Epoch 3111/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.1346 - val_accuracy: 0.9845\n",
      "Epoch 3112/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.1358 - val_accuracy: 0.9845\n",
      "Epoch 3113/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.1300 - val_accuracy: 0.9845\n",
      "Epoch 3114/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0157 - accuracy: 0.9939 - val_loss: 0.1228 - val_accuracy: 0.9845\n",
      "Epoch 3115/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.1264 - val_accuracy: 0.9845\n",
      "Epoch 3116/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0180 - accuracy: 0.9923 - val_loss: 0.1263 - val_accuracy: 0.9845\n",
      "Epoch 3117/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0175 - accuracy: 0.9939 - val_loss: 0.1275 - val_accuracy: 0.9845\n",
      "Epoch 3118/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0164 - accuracy: 0.9939 - val_loss: 0.1448 - val_accuracy: 0.9783\n",
      "Epoch 3119/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 0.1307 - val_accuracy: 0.9845\n",
      "Epoch 3120/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0179 - accuracy: 0.9923 - val_loss: 0.1177 - val_accuracy: 0.9876\n",
      "Epoch 3121/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0221 - accuracy: 0.9908 - val_loss: 0.1383 - val_accuracy: 0.9845\n",
      "Epoch 3122/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.1589 - val_accuracy: 0.9752\n",
      "Epoch 3123/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0194 - accuracy: 0.9923 - val_loss: 0.1322 - val_accuracy: 0.9845\n",
      "Epoch 3124/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1199 - val_accuracy: 0.9876\n",
      "Epoch 3125/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0235 - accuracy: 0.9893 - val_loss: 0.1511 - val_accuracy: 0.9752\n",
      "Epoch 3126/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0235 - accuracy: 0.9939 - val_loss: 0.2036 - val_accuracy: 0.9658\n",
      "Epoch 3127/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0358 - accuracy: 0.9862 - val_loss: 0.1416 - val_accuracy: 0.9814\n",
      "Epoch 3128/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1201 - val_accuracy: 0.9783\n",
      "Epoch 3129/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0360 - accuracy: 0.9877 - val_loss: 0.1213 - val_accuracy: 0.9876\n",
      "Epoch 3130/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.1733 - val_accuracy: 0.9720\n",
      "Epoch 3131/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0294 - accuracy: 0.9908 - val_loss: 0.1686 - val_accuracy: 0.9752\n",
      "Epoch 3132/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0246 - accuracy: 0.9908 - val_loss: 0.1217 - val_accuracy: 0.9845\n",
      "Epoch 3133/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.1218 - val_accuracy: 0.9845\n",
      "Epoch 3134/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.1502 - val_accuracy: 0.9752\n",
      "Epoch 3135/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0201 - accuracy: 0.9923 - val_loss: 0.1462 - val_accuracy: 0.9752\n",
      "Epoch 3136/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0154 - accuracy: 0.9939 - val_loss: 0.1162 - val_accuracy: 0.9876\n",
      "Epoch 3137/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0229 - accuracy: 0.9893 - val_loss: 0.1146 - val_accuracy: 0.9876\n",
      "Epoch 3138/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0231 - accuracy: 0.9908 - val_loss: 0.1447 - val_accuracy: 0.9783\n",
      "Epoch 3139/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.1678 - val_accuracy: 0.9720\n",
      "Epoch 3140/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0243 - accuracy: 0.9923 - val_loss: 0.1416 - val_accuracy: 0.9783\n",
      "Epoch 3141/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.1188 - val_accuracy: 0.9845\n",
      "Epoch 3142/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0196 - accuracy: 0.9908 - val_loss: 0.1185 - val_accuracy: 0.9845\n",
      "Epoch 3143/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0190 - accuracy: 0.9908 - val_loss: 0.1443 - val_accuracy: 0.9814\n",
      "Epoch 3144/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.1514 - val_accuracy: 0.9752\n",
      "Epoch 3145/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.1304 - val_accuracy: 0.9845\n",
      "Epoch 3146/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.1239 - val_accuracy: 0.9845\n",
      "Epoch 3147/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1314 - val_accuracy: 0.9845\n",
      "Epoch 3148/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.1301 - val_accuracy: 0.9845\n",
      "Epoch 3149/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.1346 - val_accuracy: 0.9814\n",
      "Epoch 3150/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.1376 - val_accuracy: 0.9814\n",
      "Epoch 3151/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1269 - val_accuracy: 0.9845\n",
      "Epoch 3152/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0175 - accuracy: 0.9923 - val_loss: 0.1193 - val_accuracy: 0.9876\n",
      "Epoch 3153/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 19us/step - loss: 0.0203 - accuracy: 0.9893 - val_loss: 0.1319 - val_accuracy: 0.9814\n",
      "Epoch 3154/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0169 - accuracy: 0.9923 - val_loss: 0.1360 - val_accuracy: 0.9814\n",
      "Epoch 3155/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0173 - accuracy: 0.9939 - val_loss: 0.1339 - val_accuracy: 0.9814\n",
      "Epoch 3156/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0165 - accuracy: 0.9939 - val_loss: 0.1433 - val_accuracy: 0.9783\n",
      "Epoch 3157/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0173 - accuracy: 0.9939 - val_loss: 0.1416 - val_accuracy: 0.9814\n",
      "Epoch 3158/3500\n",
      "653/653 [==============================] - 0s 14us/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.1267 - val_accuracy: 0.9845\n",
      "Epoch 3159/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0166 - accuracy: 0.9939 - val_loss: 0.1230 - val_accuracy: 0.9845\n",
      "Epoch 3160/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0174 - accuracy: 0.9923 - val_loss: 0.1351 - val_accuracy: 0.9845\n",
      "Epoch 3161/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.1460 - val_accuracy: 0.9783\n",
      "Epoch 3162/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.1312 - val_accuracy: 0.9845\n",
      "Epoch 3163/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0163 - accuracy: 0.9939 - val_loss: 0.1203 - val_accuracy: 0.9845\n",
      "Epoch 3164/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0200 - accuracy: 0.9923 - val_loss: 0.1303 - val_accuracy: 0.9845\n",
      "Epoch 3165/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0167 - accuracy: 0.9939 - val_loss: 0.1493 - val_accuracy: 0.9783\n",
      "Epoch 3166/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.1395 - val_accuracy: 0.9783\n",
      "Epoch 3167/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.1244 - val_accuracy: 0.9845\n",
      "Epoch 3168/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1252 - val_accuracy: 0.9845\n",
      "Epoch 3169/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0182 - accuracy: 0.9923 - val_loss: 0.1456 - val_accuracy: 0.9752\n",
      "Epoch 3170/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0173 - accuracy: 0.9939 - val_loss: 0.1337 - val_accuracy: 0.9814\n",
      "Epoch 3171/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1206 - val_accuracy: 0.9845\n",
      "Epoch 3172/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0193 - accuracy: 0.9908 - val_loss: 0.1342 - val_accuracy: 0.9845\n",
      "Epoch 3173/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1697 - val_accuracy: 0.9720\n",
      "Epoch 3174/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0240 - accuracy: 0.9908 - val_loss: 0.1552 - val_accuracy: 0.9752\n",
      "Epoch 3175/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0196 - accuracy: 0.9923 - val_loss: 0.1213 - val_accuracy: 0.9845\n",
      "Epoch 3176/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0197 - accuracy: 0.9923 - val_loss: 0.1249 - val_accuracy: 0.9845\n",
      "Epoch 3177/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1594 - val_accuracy: 0.9752\n",
      "Epoch 3178/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.1520 - val_accuracy: 0.9783\n",
      "Epoch 3179/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0192 - accuracy: 0.9923 - val_loss: 0.1192 - val_accuracy: 0.9876\n",
      "Epoch 3180/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0203 - accuracy: 0.9893 - val_loss: 0.1261 - val_accuracy: 0.9845\n",
      "Epoch 3181/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0160 - accuracy: 0.9923 - val_loss: 0.1552 - val_accuracy: 0.9752\n",
      "Epoch 3182/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1494 - val_accuracy: 0.9783\n",
      "Epoch 3183/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.1211 - val_accuracy: 0.9845\n",
      "Epoch 3184/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0194 - accuracy: 0.9923 - val_loss: 0.1198 - val_accuracy: 0.9876\n",
      "Epoch 3185/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0209 - accuracy: 0.9893 - val_loss: 0.1369 - val_accuracy: 0.9845\n",
      "Epoch 3186/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1441 - val_accuracy: 0.9783\n",
      "Epoch 3187/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.1387 - val_accuracy: 0.9845\n",
      "Epoch 3188/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1294 - val_accuracy: 0.9845\n",
      "Epoch 3189/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0169 - accuracy: 0.9908 - val_loss: 0.1295 - val_accuracy: 0.9845\n",
      "Epoch 3190/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0176 - accuracy: 0.9923 - val_loss: 0.1404 - val_accuracy: 0.9783\n",
      "Epoch 3191/3500\n",
      "653/653 [==============================] - 0s 32us/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.1326 - val_accuracy: 0.9845\n",
      "Epoch 3192/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0157 - accuracy: 0.9939 - val_loss: 0.1353 - val_accuracy: 0.9845\n",
      "Epoch 3193/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0156 - accuracy: 0.9939 - val_loss: 0.1381 - val_accuracy: 0.9814\n",
      "Epoch 3194/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1296 - val_accuracy: 0.9845\n",
      "Epoch 3195/3500\n",
      "653/653 [==============================] - 0s 36us/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1233 - val_accuracy: 0.9845\n",
      "Epoch 3196/3500\n",
      "653/653 [==============================] - 0s 33us/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 0.1328 - val_accuracy: 0.9845\n",
      "Epoch 3197/3500\n",
      "653/653 [==============================] - 0s 33us/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.1549 - val_accuracy: 0.9752\n",
      "Epoch 3198/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.1511 - val_accuracy: 0.9752\n",
      "Epoch 3199/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1331 - val_accuracy: 0.9845\n",
      "Epoch 3200/3500\n",
      "653/653 [==============================] - 0s 34us/step - loss: 0.0156 - accuracy: 0.9939 - val_loss: 0.1260 - val_accuracy: 0.9845\n",
      "Epoch 3201/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0174 - accuracy: 0.9923 - val_loss: 0.1348 - val_accuracy: 0.9845\n",
      "Epoch 3202/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1445 - val_accuracy: 0.9783\n",
      "Epoch 3203/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.1320 - val_accuracy: 0.9845\n",
      "Epoch 3204/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.1283 - val_accuracy: 0.9845\n",
      "Epoch 3205/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1443 - val_accuracy: 0.9783\n",
      "Epoch 3206/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.1496 - val_accuracy: 0.9783\n",
      "Epoch 3207/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.1296 - val_accuracy: 0.9845\n",
      "Epoch 3208/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0171 - accuracy: 0.9908 - val_loss: 0.1222 - val_accuracy: 0.9845\n",
      "Epoch 3209/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0193 - accuracy: 0.9908 - val_loss: 0.1363 - val_accuracy: 0.9814\n",
      "Epoch 3210/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.1442 - val_accuracy: 0.9814\n",
      "Epoch 3211/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1455 - val_accuracy: 0.9783\n",
      "Epoch 3212/3500\n",
      "653/653 [==============================] - 0s 50us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1322 - val_accuracy: 0.9845\n",
      "Epoch 3213/3500\n",
      "653/653 [==============================] - 0s 71us/step - loss: 0.0159 - accuracy: 0.9923 - val_loss: 0.1226 - val_accuracy: 0.9845\n",
      "Epoch 3214/3500\n",
      "653/653 [==============================] - 0s 40us/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 0.1319 - val_accuracy: 0.9845\n",
      "Epoch 3215/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0189 - accuracy: 0.9923 - val_loss: 0.1332 - val_accuracy: 0.9845\n",
      "Epoch 3216/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.1205 - val_accuracy: 0.9876\n",
      "Epoch 3217/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0206 - accuracy: 0.9908 - val_loss: 0.1244 - val_accuracy: 0.9845\n",
      "Epoch 3218/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0180 - accuracy: 0.9923 - val_loss: 0.1444 - val_accuracy: 0.9783\n",
      "Epoch 3219/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.1460 - val_accuracy: 0.9783\n",
      "Epoch 3220/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.1308 - val_accuracy: 0.9845\n",
      "Epoch 3221/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0160 - accuracy: 0.9939 - val_loss: 0.1336 - val_accuracy: 0.9845\n",
      "Epoch 3222/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0157 - accuracy: 0.9939 - val_loss: 0.1358 - val_accuracy: 0.9845\n",
      "Epoch 3223/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0154 - accuracy: 0.9939 - val_loss: 0.1360 - val_accuracy: 0.9845\n",
      "Epoch 3224/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0158 - accuracy: 0.9939 - val_loss: 0.1326 - val_accuracy: 0.9845\n",
      "Epoch 3225/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0157 - accuracy: 0.9923 - val_loss: 0.1284 - val_accuracy: 0.9845\n",
      "Epoch 3226/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0164 - accuracy: 0.9939 - val_loss: 0.1378 - val_accuracy: 0.9814\n",
      "Epoch 3227/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.1422 - val_accuracy: 0.9814\n",
      "Epoch 3228/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.1325 - val_accuracy: 0.9845\n",
      "Epoch 3229/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1344 - val_accuracy: 0.9845\n",
      "Epoch 3230/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0156 - accuracy: 0.9939 - val_loss: 0.1453 - val_accuracy: 0.9783\n",
      "Epoch 3231/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.1416 - val_accuracy: 0.9783\n",
      "Epoch 3232/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0160 - accuracy: 0.9939 - val_loss: 0.1408 - val_accuracy: 0.9814\n",
      "Epoch 3233/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.1472 - val_accuracy: 0.9783\n",
      "Epoch 3234/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0176 - accuracy: 0.9954 - val_loss: 0.1347 - val_accuracy: 0.9845\n",
      "Epoch 3235/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0181 - accuracy: 0.9923 - val_loss: 0.1280 - val_accuracy: 0.9845\n",
      "Epoch 3236/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.1400 - val_accuracy: 0.9845\n",
      "Epoch 3237/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1369 - val_accuracy: 0.9845\n",
      "Epoch 3238/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.1286 - val_accuracy: 0.9845\n",
      "Epoch 3239/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0167 - accuracy: 0.9908 - val_loss: 0.1232 - val_accuracy: 0.9845\n",
      "Epoch 3240/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0188 - accuracy: 0.9923 - val_loss: 0.1280 - val_accuracy: 0.9845\n",
      "Epoch 3241/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0163 - accuracy: 0.9923 - val_loss: 0.1485 - val_accuracy: 0.9783\n",
      "Epoch 3242/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1569 - val_accuracy: 0.9783\n",
      "Epoch 3243/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1446 - val_accuracy: 0.9783\n",
      "Epoch 3244/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1282 - val_accuracy: 0.9845\n",
      "Epoch 3245/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.1310 - val_accuracy: 0.9845\n",
      "Epoch 3246/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0159 - accuracy: 0.9939 - val_loss: 0.1614 - val_accuracy: 0.9752\n",
      "Epoch 3247/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1626 - val_accuracy: 0.9752\n",
      "Epoch 3248/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0197 - accuracy: 0.9923 - val_loss: 0.1363 - val_accuracy: 0.9845\n",
      "Epoch 3249/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1240 - val_accuracy: 0.9845\n",
      "Epoch 3250/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0185 - accuracy: 0.9908 - val_loss: 0.1313 - val_accuracy: 0.9845\n",
      "Epoch 3251/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0157 - accuracy: 0.9923 - val_loss: 0.1443 - val_accuracy: 0.9783\n",
      "Epoch 3252/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.1441 - val_accuracy: 0.9814\n",
      "Epoch 3253/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.1336 - val_accuracy: 0.9845\n",
      "Epoch 3254/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0159 - accuracy: 0.9939 - val_loss: 0.1288 - val_accuracy: 0.9845\n",
      "Epoch 3255/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0162 - accuracy: 0.9923 - val_loss: 0.1446 - val_accuracy: 0.9783\n",
      "Epoch 3256/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0169 - accuracy: 0.9954 - val_loss: 0.1494 - val_accuracy: 0.9783\n",
      "Epoch 3257/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.1302 - val_accuracy: 0.9845\n",
      "Epoch 3258/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1278 - val_accuracy: 0.9845\n",
      "Epoch 3259/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0163 - accuracy: 0.9923 - val_loss: 0.1487 - val_accuracy: 0.9783\n",
      "Epoch 3260/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.1722 - val_accuracy: 0.9720\n",
      "Epoch 3261/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0230 - accuracy: 0.9908 - val_loss: 0.1374 - val_accuracy: 0.9845\n",
      "Epoch 3262/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0189 - accuracy: 0.9908 - val_loss: 0.1222 - val_accuracy: 0.9845\n",
      "Epoch 3263/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 18us/step - loss: 0.0189 - accuracy: 0.9893 - val_loss: 0.1425 - val_accuracy: 0.9814\n",
      "Epoch 3264/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1496 - val_accuracy: 0.9783\n",
      "Epoch 3265/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0165 - accuracy: 0.9954 - val_loss: 0.1319 - val_accuracy: 0.9845\n",
      "Epoch 3266/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0162 - accuracy: 0.9939 - val_loss: 0.1269 - val_accuracy: 0.9845\n",
      "Epoch 3267/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0179 - accuracy: 0.9908 - val_loss: 0.1323 - val_accuracy: 0.9845\n",
      "Epoch 3268/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1331 - val_accuracy: 0.9845\n",
      "Epoch 3269/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0164 - accuracy: 0.9954 - val_loss: 0.1312 - val_accuracy: 0.9845\n",
      "Epoch 3270/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0173 - accuracy: 0.9908 - val_loss: 0.1350 - val_accuracy: 0.9845\n",
      "Epoch 3271/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.1660 - val_accuracy: 0.9752\n",
      "Epoch 3272/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.1610 - val_accuracy: 0.9752\n",
      "Epoch 3273/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.1286 - val_accuracy: 0.9845\n",
      "Epoch 3274/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0167 - accuracy: 0.9923 - val_loss: 0.1292 - val_accuracy: 0.9845\n",
      "Epoch 3275/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0168 - accuracy: 0.9923 - val_loss: 0.1432 - val_accuracy: 0.9783\n",
      "Epoch 3276/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.1422 - val_accuracy: 0.9783\n",
      "Epoch 3277/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.1385 - val_accuracy: 0.9814\n",
      "Epoch 3278/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.1391 - val_accuracy: 0.9783\n",
      "Epoch 3279/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1397 - val_accuracy: 0.9783\n",
      "Epoch 3280/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1362 - val_accuracy: 0.9845\n",
      "Epoch 3281/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1355 - val_accuracy: 0.9814\n",
      "Epoch 3282/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.1459 - val_accuracy: 0.9814\n",
      "Epoch 3283/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.1461 - val_accuracy: 0.9814\n",
      "Epoch 3284/3500\n",
      "653/653 [==============================] - 0s 36us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1325 - val_accuracy: 0.9845\n",
      "Epoch 3285/3500\n",
      "653/653 [==============================] - 0s 33us/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.1340 - val_accuracy: 0.9845\n",
      "Epoch 3286/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1412 - val_accuracy: 0.9814\n",
      "Epoch 3287/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1406 - val_accuracy: 0.9814\n",
      "Epoch 3288/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1349 - val_accuracy: 0.9845\n",
      "Epoch 3289/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0156 - accuracy: 0.9939 - val_loss: 0.1365 - val_accuracy: 0.9845\n",
      "Epoch 3290/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1380 - val_accuracy: 0.9845\n",
      "Epoch 3291/3500\n",
      "653/653 [==============================] - 0s 34us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1402 - val_accuracy: 0.9814\n",
      "Epoch 3292/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1401 - val_accuracy: 0.9814\n",
      "Epoch 3293/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1314 - val_accuracy: 0.9845\n",
      "Epoch 3294/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0157 - accuracy: 0.9923 - val_loss: 0.1260 - val_accuracy: 0.9845\n",
      "Epoch 3295/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0168 - accuracy: 0.9923 - val_loss: 0.1416 - val_accuracy: 0.9783\n",
      "Epoch 3296/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1631 - val_accuracy: 0.9752\n",
      "Epoch 3297/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1418 - val_accuracy: 0.9783\n",
      "Epoch 3298/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0158 - accuracy: 0.9939 - val_loss: 0.1257 - val_accuracy: 0.9845\n",
      "Epoch 3299/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0178 - accuracy: 0.9923 - val_loss: 0.1428 - val_accuracy: 0.9783\n",
      "Epoch 3300/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.1777 - val_accuracy: 0.9689\n",
      "Epoch 3301/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0262 - accuracy: 0.9908 - val_loss: 0.1388 - val_accuracy: 0.9814\n",
      "Epoch 3302/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.1226 - val_accuracy: 0.9783\n",
      "Epoch 3303/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0373 - accuracy: 0.9877 - val_loss: 0.1370 - val_accuracy: 0.9845\n",
      "Epoch 3304/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0141 - accuracy: 0.9954 - val_loss: 0.2234 - val_accuracy: 0.9658\n",
      "Epoch 3305/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0460 - accuracy: 0.9847 - val_loss: 0.2091 - val_accuracy: 0.9658\n",
      "Epoch 3306/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0357 - accuracy: 0.9877 - val_loss: 0.1321 - val_accuracy: 0.9845\n",
      "Epoch 3307/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0180 - accuracy: 0.9923 - val_loss: 0.1219 - val_accuracy: 0.9814\n",
      "Epoch 3308/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0341 - accuracy: 0.9877 - val_loss: 0.1348 - val_accuracy: 0.9845\n",
      "Epoch 3309/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1534 - val_accuracy: 0.9783\n",
      "Epoch 3310/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.1328 - val_accuracy: 0.9845\n",
      "Epoch 3311/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0158 - accuracy: 0.9923 - val_loss: 0.1275 - val_accuracy: 0.9845\n",
      "Epoch 3312/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0177 - accuracy: 0.9908 - val_loss: 0.1361 - val_accuracy: 0.9845\n",
      "Epoch 3313/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.1447 - val_accuracy: 0.9783\n",
      "Epoch 3314/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1472 - val_accuracy: 0.9783\n",
      "Epoch 3315/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1307 - val_accuracy: 0.9845\n",
      "Epoch 3316/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.1317 - val_accuracy: 0.9845\n",
      "Epoch 3317/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.1609 - val_accuracy: 0.9752\n",
      "Epoch 3318/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0191 - accuracy: 0.9908 - val_loss: 0.1639 - val_accuracy: 0.9752\n",
      "Epoch 3319/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0187 - accuracy: 0.9923 - val_loss: 0.1304 - val_accuracy: 0.9845\n",
      "Epoch 3320/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0155 - accuracy: 0.9939 - val_loss: 0.1204 - val_accuracy: 0.9814\n",
      "Epoch 3321/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0255 - accuracy: 0.9893 - val_loss: 0.1354 - val_accuracy: 0.9845\n",
      "Epoch 3322/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0153 - accuracy: 0.9939 - val_loss: 0.1893 - val_accuracy: 0.9689\n",
      "Epoch 3323/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0302 - accuracy: 0.9893 - val_loss: 0.1589 - val_accuracy: 0.9752\n",
      "Epoch 3324/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 0.1216 - val_accuracy: 0.9876\n",
      "Epoch 3325/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0231 - accuracy: 0.9893 - val_loss: 0.1333 - val_accuracy: 0.9845\n",
      "Epoch 3326/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0140 - accuracy: 0.9939 - val_loss: 0.1743 - val_accuracy: 0.9720\n",
      "Epoch 3327/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0247 - accuracy: 0.9908 - val_loss: 0.1621 - val_accuracy: 0.9752\n",
      "Epoch 3328/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0174 - accuracy: 0.9923 - val_loss: 0.1195 - val_accuracy: 0.9876\n",
      "Epoch 3329/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0252 - accuracy: 0.9877 - val_loss: 0.1190 - val_accuracy: 0.9845\n",
      "Epoch 3330/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0208 - accuracy: 0.9908 - val_loss: 0.1606 - val_accuracy: 0.9752\n",
      "Epoch 3331/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0196 - accuracy: 0.9923 - val_loss: 0.1998 - val_accuracy: 0.9658\n",
      "Epoch 3332/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0347 - accuracy: 0.9877 - val_loss: 0.1638 - val_accuracy: 0.9752\n",
      "Epoch 3333/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.1249 - val_accuracy: 0.9845\n",
      "Epoch 3334/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0204 - accuracy: 0.9908 - val_loss: 0.1292 - val_accuracy: 0.9845\n",
      "Epoch 3335/3500\n",
      "653/653 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.99 - 0s 18us/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.1543 - val_accuracy: 0.9783\n",
      "Epoch 3336/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 0.1361 - val_accuracy: 0.9845\n",
      "Epoch 3337/3500\n",
      "653/653 [==============================] - 0s 28us/step - loss: 0.0164 - accuracy: 0.9939 - val_loss: 0.1296 - val_accuracy: 0.9845\n",
      "Epoch 3338/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0173 - accuracy: 0.9939 - val_loss: 0.1396 - val_accuracy: 0.9814\n",
      "Epoch 3339/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.1399 - val_accuracy: 0.9814\n",
      "Epoch 3340/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.1358 - val_accuracy: 0.9845\n",
      "Epoch 3341/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1310 - val_accuracy: 0.9845\n",
      "Epoch 3342/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0156 - accuracy: 0.9939 - val_loss: 0.1416 - val_accuracy: 0.9814\n",
      "Epoch 3343/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1431 - val_accuracy: 0.9783\n",
      "Epoch 3344/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.1344 - val_accuracy: 0.9845\n",
      "Epoch 3345/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.1327 - val_accuracy: 0.9845\n",
      "Epoch 3346/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0157 - accuracy: 0.9923 - val_loss: 0.1333 - val_accuracy: 0.9845\n",
      "Epoch 3347/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0155 - accuracy: 0.9923 - val_loss: 0.1353 - val_accuracy: 0.9845\n",
      "Epoch 3348/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0153 - accuracy: 0.9939 - val_loss: 0.1393 - val_accuracy: 0.9845\n",
      "Epoch 3349/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.1444 - val_accuracy: 0.9814\n",
      "Epoch 3350/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1474 - val_accuracy: 0.9783\n",
      "Epoch 3351/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1356 - val_accuracy: 0.9845\n",
      "Epoch 3352/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0169 - accuracy: 0.9923 - val_loss: 0.1365 - val_accuracy: 0.9814\n",
      "Epoch 3353/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0182 - accuracy: 0.9954 - val_loss: 0.1429 - val_accuracy: 0.9814\n",
      "Epoch 3354/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1275 - val_accuracy: 0.9845\n",
      "Epoch 3355/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0179 - accuracy: 0.9923 - val_loss: 0.1371 - val_accuracy: 0.9814\n",
      "Epoch 3356/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.1558 - val_accuracy: 0.9752\n",
      "Epoch 3357/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.1387 - val_accuracy: 0.9814\n",
      "Epoch 3358/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 0.1359 - val_accuracy: 0.9814\n",
      "Epoch 3359/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0158 - accuracy: 0.9923 - val_loss: 0.1530 - val_accuracy: 0.9752\n",
      "Epoch 3360/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0174 - accuracy: 0.9939 - val_loss: 0.1522 - val_accuracy: 0.9752\n",
      "Epoch 3361/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0170 - accuracy: 0.9939 - val_loss: 0.1331 - val_accuracy: 0.9845\n",
      "Epoch 3362/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1231 - val_accuracy: 0.9876\n",
      "Epoch 3363/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0195 - accuracy: 0.9923 - val_loss: 0.1355 - val_accuracy: 0.9845\n",
      "Epoch 3364/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0152 - accuracy: 0.9923 - val_loss: 0.1610 - val_accuracy: 0.9752\n",
      "Epoch 3365/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0190 - accuracy: 0.9923 - val_loss: 0.1599 - val_accuracy: 0.9752\n",
      "Epoch 3366/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.1348 - val_accuracy: 0.9845\n",
      "Epoch 3367/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0150 - accuracy: 0.9923 - val_loss: 0.1254 - val_accuracy: 0.9845\n",
      "Epoch 3368/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1302 - val_accuracy: 0.9845\n",
      "Epoch 3369/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1438 - val_accuracy: 0.9814\n",
      "Epoch 3370/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1433 - val_accuracy: 0.9814\n",
      "Epoch 3371/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0157 - accuracy: 0.9939 - val_loss: 0.1315 - val_accuracy: 0.9845\n",
      "Epoch 3372/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0156 - accuracy: 0.9939 - val_loss: 0.1442 - val_accuracy: 0.9814\n",
      "Epoch 3373/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 25us/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.1557 - val_accuracy: 0.9783\n",
      "Epoch 3374/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.1410 - val_accuracy: 0.9814\n",
      "Epoch 3375/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0159 - accuracy: 0.9939 - val_loss: 0.1336 - val_accuracy: 0.9845\n",
      "Epoch 3376/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0154 - accuracy: 0.9939 - val_loss: 0.1510 - val_accuracy: 0.9814\n",
      "Epoch 3377/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.1636 - val_accuracy: 0.9783\n",
      "Epoch 3378/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0185 - accuracy: 0.9939 - val_loss: 0.1428 - val_accuracy: 0.9814\n",
      "Epoch 3379/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1272 - val_accuracy: 0.9845\n",
      "Epoch 3380/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.1389 - val_accuracy: 0.9845\n",
      "Epoch 3381/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.1620 - val_accuracy: 0.9752\n",
      "Epoch 3382/3500\n",
      "653/653 [==============================] - 0s 15us/step - loss: 0.0198 - accuracy: 0.9923 - val_loss: 0.1497 - val_accuracy: 0.9783\n",
      "Epoch 3383/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0175 - accuracy: 0.9939 - val_loss: 0.1289 - val_accuracy: 0.9845\n",
      "Epoch 3384/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.1384 - val_accuracy: 0.9845\n",
      "Epoch 3385/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.1499 - val_accuracy: 0.9783\n",
      "Epoch 3386/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.1512 - val_accuracy: 0.9783\n",
      "Epoch 3387/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1425 - val_accuracy: 0.9814\n",
      "Epoch 3388/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0154 - accuracy: 0.9939 - val_loss: 0.1347 - val_accuracy: 0.9845\n",
      "Epoch 3389/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0158 - accuracy: 0.9954 - val_loss: 0.1347 - val_accuracy: 0.9845\n",
      "Epoch 3390/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0163 - accuracy: 0.9923 - val_loss: 0.1415 - val_accuracy: 0.9814\n",
      "Epoch 3391/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.1628 - val_accuracy: 0.9752\n",
      "Epoch 3392/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0195 - accuracy: 0.9923 - val_loss: 0.1521 - val_accuracy: 0.9783\n",
      "Epoch 3393/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.1369 - val_accuracy: 0.9845\n",
      "Epoch 3394/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0153 - accuracy: 0.9939 - val_loss: 0.1390 - val_accuracy: 0.9845\n",
      "Epoch 3395/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.1426 - val_accuracy: 0.9814\n",
      "Epoch 3396/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.1382 - val_accuracy: 0.9814\n",
      "Epoch 3397/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1283 - val_accuracy: 0.9845\n",
      "Epoch 3398/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0204 - accuracy: 0.9923 - val_loss: 0.1297 - val_accuracy: 0.9814\n",
      "Epoch 3399/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0191 - accuracy: 0.9923 - val_loss: 0.1556 - val_accuracy: 0.9783\n",
      "Epoch 3400/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0180 - accuracy: 0.9923 - val_loss: 0.1616 - val_accuracy: 0.9752\n",
      "Epoch 3401/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.1381 - val_accuracy: 0.9845\n",
      "Epoch 3402/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0146 - accuracy: 0.9969 - val_loss: 0.1241 - val_accuracy: 0.9876\n",
      "Epoch 3403/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0185 - accuracy: 0.9923 - val_loss: 0.1315 - val_accuracy: 0.9845\n",
      "Epoch 3404/3500\n",
      "653/653 [==============================] - 0s 29us/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.1586 - val_accuracy: 0.9752\n",
      "Epoch 3405/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0191 - accuracy: 0.9923 - val_loss: 0.1624 - val_accuracy: 0.9752\n",
      "Epoch 3406/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0188 - accuracy: 0.9923 - val_loss: 0.1302 - val_accuracy: 0.9845\n",
      "Epoch 3407/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0167 - accuracy: 0.9939 - val_loss: 0.1229 - val_accuracy: 0.9876\n",
      "Epoch 3408/3500\n",
      "653/653 [==============================] - 0s 27us/step - loss: 0.0197 - accuracy: 0.9893 - val_loss: 0.1354 - val_accuracy: 0.9845\n",
      "Epoch 3409/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1452 - val_accuracy: 0.9814\n",
      "Epoch 3410/3500\n",
      "653/653 [==============================] - 0s 26us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1402 - val_accuracy: 0.9814\n",
      "Epoch 3411/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1308 - val_accuracy: 0.9845\n",
      "Epoch 3412/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0164 - accuracy: 0.9923 - val_loss: 0.1265 - val_accuracy: 0.9845\n",
      "Epoch 3413/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0180 - accuracy: 0.9923 - val_loss: 0.1510 - val_accuracy: 0.9783\n",
      "Epoch 3414/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.1792 - val_accuracy: 0.9720\n",
      "Epoch 3415/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0227 - accuracy: 0.9908 - val_loss: 0.1410 - val_accuracy: 0.9814\n",
      "Epoch 3416/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.1216 - val_accuracy: 0.9845\n",
      "Epoch 3417/3500\n",
      "653/653 [==============================] - 0s 24us/step - loss: 0.0264 - accuracy: 0.9877 - val_loss: 0.1287 - val_accuracy: 0.9845\n",
      "Epoch 3418/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1573 - val_accuracy: 0.9752\n",
      "Epoch 3419/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.1510 - val_accuracy: 0.9783\n",
      "Epoch 3420/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.1236 - val_accuracy: 0.9876\n",
      "Epoch 3421/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0213 - accuracy: 0.9908 - val_loss: 0.1250 - val_accuracy: 0.9876\n",
      "Epoch 3422/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0186 - accuracy: 0.9923 - val_loss: 0.1583 - val_accuracy: 0.9783\n",
      "Epoch 3423/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.1678 - val_accuracy: 0.9720\n",
      "Epoch 3424/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0192 - accuracy: 0.9923 - val_loss: 0.1361 - val_accuracy: 0.9814\n",
      "Epoch 3425/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0152 - accuracy: 0.9939 - val_loss: 0.1244 - val_accuracy: 0.9876\n",
      "Epoch 3426/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0193 - accuracy: 0.9923 - val_loss: 0.1416 - val_accuracy: 0.9814\n",
      "Epoch 3427/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0167 - accuracy: 0.9939 - val_loss: 0.1619 - val_accuracy: 0.9752\n",
      "Epoch 3428/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0173 - accuracy: 0.9923 - val_loss: 0.1293 - val_accuracy: 0.9845\n",
      "Epoch 3429/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0164 - accuracy: 0.9923 - val_loss: 0.1225 - val_accuracy: 0.9814\n",
      "Epoch 3430/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0297 - accuracy: 0.9893 - val_loss: 0.1367 - val_accuracy: 0.9814\n",
      "Epoch 3431/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0133 - accuracy: 0.9954 - val_loss: 0.1822 - val_accuracy: 0.9720\n",
      "Epoch 3432/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0287 - accuracy: 0.9908 - val_loss: 0.1683 - val_accuracy: 0.9752\n",
      "Epoch 3433/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0165 - accuracy: 0.9908 - val_loss: 0.1230 - val_accuracy: 0.9814\n",
      "Epoch 3434/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0259 - accuracy: 0.9893 - val_loss: 0.1228 - val_accuracy: 0.9814\n",
      "Epoch 3435/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0248 - accuracy: 0.9893 - val_loss: 0.1587 - val_accuracy: 0.9752\n",
      "Epoch 3436/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.2050 - val_accuracy: 0.9658\n",
      "Epoch 3437/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0333 - accuracy: 0.9893 - val_loss: 0.1505 - val_accuracy: 0.9783\n",
      "Epoch 3438/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0155 - accuracy: 0.9954 - val_loss: 0.1225 - val_accuracy: 0.9783\n",
      "Epoch 3439/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0372 - accuracy: 0.9877 - val_loss: 0.1259 - val_accuracy: 0.9845\n",
      "Epoch 3440/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0189 - accuracy: 0.9923 - val_loss: 0.1981 - val_accuracy: 0.9658\n",
      "Epoch 3441/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0351 - accuracy: 0.9877 - val_loss: 0.1726 - val_accuracy: 0.9720\n",
      "Epoch 3442/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0220 - accuracy: 0.9923 - val_loss: 0.1209 - val_accuracy: 0.9845\n",
      "Epoch 3443/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0294 - accuracy: 0.9893 - val_loss: 0.1215 - val_accuracy: 0.9876\n",
      "Epoch 3444/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0243 - accuracy: 0.9877 - val_loss: 0.1620 - val_accuracy: 0.9752\n",
      "Epoch 3445/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0190 - accuracy: 0.9923 - val_loss: 0.1598 - val_accuracy: 0.9752\n",
      "Epoch 3446/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0178 - accuracy: 0.9939 - val_loss: 0.1311 - val_accuracy: 0.9845\n",
      "Epoch 3447/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0172 - accuracy: 0.9923 - val_loss: 0.1300 - val_accuracy: 0.9845\n",
      "Epoch 3448/3500\n",
      "653/653 [==============================] - 0s 31us/step - loss: 0.0163 - accuracy: 0.9939 - val_loss: 0.1543 - val_accuracy: 0.9752\n",
      "Epoch 3449/3500\n",
      "653/653 [==============================] - 0s 23us/step - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.1549 - val_accuracy: 0.9752\n",
      "Epoch 3450/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0173 - accuracy: 0.9939 - val_loss: 0.1305 - val_accuracy: 0.9845\n",
      "Epoch 3451/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0167 - accuracy: 0.9939 - val_loss: 0.1319 - val_accuracy: 0.9845\n",
      "Epoch 3452/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0152 - accuracy: 0.9939 - val_loss: 0.1513 - val_accuracy: 0.9783\n",
      "Epoch 3453/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0163 - accuracy: 0.9954 - val_loss: 0.1562 - val_accuracy: 0.9783\n",
      "Epoch 3454/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.1398 - val_accuracy: 0.9845\n",
      "Epoch 3455/3500\n",
      "653/653 [==============================] - 0s 25us/step - loss: 0.0160 - accuracy: 0.9939 - val_loss: 0.1353 - val_accuracy: 0.9845\n",
      "Epoch 3456/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1405 - val_accuracy: 0.9845\n",
      "Epoch 3457/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.1368 - val_accuracy: 0.9845\n",
      "Epoch 3458/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0152 - accuracy: 0.9939 - val_loss: 0.1458 - val_accuracy: 0.9783\n",
      "Epoch 3459/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.1556 - val_accuracy: 0.9752\n",
      "Epoch 3460/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.1405 - val_accuracy: 0.9814\n",
      "Epoch 3461/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0163 - accuracy: 0.9939 - val_loss: 0.1378 - val_accuracy: 0.9814\n",
      "Epoch 3462/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0154 - accuracy: 0.9939 - val_loss: 0.1512 - val_accuracy: 0.9783\n",
      "Epoch 3463/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1455 - val_accuracy: 0.9814\n",
      "Epoch 3464/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.1370 - val_accuracy: 0.9845\n",
      "Epoch 3465/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.1387 - val_accuracy: 0.9845\n",
      "Epoch 3466/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.1466 - val_accuracy: 0.9814\n",
      "Epoch 3467/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.1354 - val_accuracy: 0.9845\n",
      "Epoch 3468/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0159 - accuracy: 0.9939 - val_loss: 0.1308 - val_accuracy: 0.9845\n",
      "Epoch 3469/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0162 - accuracy: 0.9923 - val_loss: 0.1447 - val_accuracy: 0.9814\n",
      "Epoch 3470/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.1494 - val_accuracy: 0.9783\n",
      "Epoch 3471/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0157 - accuracy: 0.9954 - val_loss: 0.1409 - val_accuracy: 0.9845\n",
      "Epoch 3472/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0154 - accuracy: 0.9939 - val_loss: 0.1360 - val_accuracy: 0.9845\n",
      "Epoch 3473/3500\n",
      "653/653 [==============================] - 0s 21us/step - loss: 0.0158 - accuracy: 0.9939 - val_loss: 0.1376 - val_accuracy: 0.9845\n",
      "Epoch 3474/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0149 - accuracy: 0.9939 - val_loss: 0.1355 - val_accuracy: 0.9845\n",
      "Epoch 3475/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0148 - accuracy: 0.9939 - val_loss: 0.1488 - val_accuracy: 0.9783\n",
      "Epoch 3476/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.1529 - val_accuracy: 0.9783\n",
      "Epoch 3477/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 0.1352 - val_accuracy: 0.9845\n",
      "Epoch 3478/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0165 - accuracy: 0.9923 - val_loss: 0.1351 - val_accuracy: 0.9845\n",
      "Epoch 3479/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0150 - accuracy: 0.9939 - val_loss: 0.1594 - val_accuracy: 0.9783\n",
      "Epoch 3480/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0177 - accuracy: 0.9939 - val_loss: 0.1551 - val_accuracy: 0.9783\n",
      "Epoch 3481/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0172 - accuracy: 0.9954 - val_loss: 0.1367 - val_accuracy: 0.9845\n",
      "Epoch 3482/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0150 - accuracy: 0.9939 - val_loss: 0.1430 - val_accuracy: 0.9814\n",
      "Epoch 3483/3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - 0s 16us/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.1466 - val_accuracy: 0.9783\n",
      "Epoch 3484/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.1341 - val_accuracy: 0.9845\n",
      "Epoch 3485/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0154 - accuracy: 0.9939 - val_loss: 0.1376 - val_accuracy: 0.9845\n",
      "Epoch 3486/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0145 - accuracy: 0.9939 - val_loss: 0.1526 - val_accuracy: 0.9783\n",
      "Epoch 3487/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.1460 - val_accuracy: 0.9783\n",
      "Epoch 3488/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0171 - accuracy: 0.9923 - val_loss: 0.1390 - val_accuracy: 0.9845\n",
      "Epoch 3489/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.1533 - val_accuracy: 0.9783\n",
      "Epoch 3490/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1414 - val_accuracy: 0.9814\n",
      "Epoch 3491/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0152 - accuracy: 0.9939 - val_loss: 0.1394 - val_accuracy: 0.9814\n",
      "Epoch 3492/3500\n",
      "653/653 [==============================] - 0s 18us/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.1534 - val_accuracy: 0.9783\n",
      "Epoch 3493/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.1534 - val_accuracy: 0.9783\n",
      "Epoch 3494/3500\n",
      "653/653 [==============================] - 0s 19us/step - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.1429 - val_accuracy: 0.9814\n",
      "Epoch 3495/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.1373 - val_accuracy: 0.9845\n",
      "Epoch 3496/3500\n",
      "653/653 [==============================] - 0s 16us/step - loss: 0.0147 - accuracy: 0.9939 - val_loss: 0.1410 - val_accuracy: 0.9845\n",
      "Epoch 3497/3500\n",
      "653/653 [==============================] - 0s 17us/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.1462 - val_accuracy: 0.9783\n",
      "Epoch 3498/3500\n",
      "653/653 [==============================] - 0s 20us/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.1389 - val_accuracy: 0.9845\n",
      "Epoch 3499/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0150 - accuracy: 0.9939 - val_loss: 0.1352 - val_accuracy: 0.9845\n",
      "Epoch 3500/3500\n",
      "653/653 [==============================] - 0s 22us/step - loss: 0.0160 - accuracy: 0.9939 - val_loss: 0.1405 - val_accuracy: 0.9845\n"
     ]
    }
   ],
   "source": [
    "wine_new_df = wine_df.sample(frac=0.15)\n",
    "\n",
    "wine_new_dataset = wine_new_df.values\n",
    "X = wine_new_dataset[:,0:12]\n",
    "Y = wine_new_dataset[:,12]\n",
    "\n",
    "del model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X, Y, validation_split=0.33, epochs=3500, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [2.018369197845459,\n",
       "  1.1032445430755615,\n",
       "  0.7161375284194946,\n",
       "  0.6225603222846985,\n",
       "  0.6345586180686951,\n",
       "  0.668462336063385,\n",
       "  0.6893263459205627,\n",
       "  0.6915045976638794,\n",
       "  0.6753789782524109,\n",
       "  0.6461543440818787,\n",
       "  0.6089807152748108,\n",
       "  0.5662046074867249,\n",
       "  0.5259838104248047,\n",
       "  0.49013832211494446,\n",
       "  0.46024271845817566,\n",
       "  0.43668290972709656,\n",
       "  0.41927701234817505,\n",
       "  0.4071800410747528,\n",
       "  0.3981079161167145,\n",
       "  0.38896724581718445,\n",
       "  0.37910911440849304,\n",
       "  0.3712170422077179,\n",
       "  0.3640419542789459,\n",
       "  0.3586428761482239,\n",
       "  0.3556784987449646,\n",
       "  0.35363301634788513,\n",
       "  0.35189396142959595,\n",
       "  0.3493001163005829,\n",
       "  0.3457639813423157,\n",
       "  0.3411673903465271,\n",
       "  0.3352748155593872,\n",
       "  0.32906845211982727,\n",
       "  0.3232531249523163,\n",
       "  0.31795379519462585,\n",
       "  0.3136662542819977,\n",
       "  0.30976298451423645,\n",
       "  0.30625462532043457,\n",
       "  0.3030330240726471,\n",
       "  0.29989299178123474,\n",
       "  0.2968456745147705,\n",
       "  0.2940787076950073,\n",
       "  0.29150161147117615,\n",
       "  0.28870078921318054,\n",
       "  0.2858778238296509,\n",
       "  0.28291887044906616,\n",
       "  0.2803964614868164,\n",
       "  0.2778235673904419,\n",
       "  0.27567481994628906,\n",
       "  0.27415022253990173,\n",
       "  0.2731781005859375,\n",
       "  0.27253851294517517,\n",
       "  0.2717503309249878,\n",
       "  0.27060461044311523,\n",
       "  0.2696516513824463,\n",
       "  0.2688547372817993,\n",
       "  0.267962783575058,\n",
       "  0.2671022117137909,\n",
       "  0.26630765199661255,\n",
       "  0.2658444046974182,\n",
       "  0.265750914812088,\n",
       "  0.265659898519516,\n",
       "  0.2647108733654022,\n",
       "  0.26301950216293335,\n",
       "  0.2618902921676636,\n",
       "  0.2613682150840759,\n",
       "  0.2610238194465637,\n",
       "  0.2602463662624359,\n",
       "  0.25947126746177673,\n",
       "  0.258786678314209,\n",
       "  0.2583584189414978,\n",
       "  0.25800514221191406,\n",
       "  0.25744572281837463,\n",
       "  0.2569505274295807,\n",
       "  0.256492555141449,\n",
       "  0.25612762570381165,\n",
       "  0.2558623254299164,\n",
       "  0.2556909918785095,\n",
       "  0.2557659149169922,\n",
       "  0.25598403811454773,\n",
       "  0.25575876235961914,\n",
       "  0.25505077838897705,\n",
       "  0.2544094920158386,\n",
       "  0.25391915440559387,\n",
       "  0.2535116374492645,\n",
       "  0.2534484267234802,\n",
       "  0.2535461187362671,\n",
       "  0.25338122248649597,\n",
       "  0.2533826529979706,\n",
       "  0.25326141715049744,\n",
       "  0.2531872093677521,\n",
       "  0.253059446811676,\n",
       "  0.25295332074165344,\n",
       "  0.2526742219924927,\n",
       "  0.252492755651474,\n",
       "  0.25255998969078064,\n",
       "  0.25252044200897217,\n",
       "  0.2517736554145813,\n",
       "  0.2509313225746155,\n",
       "  0.2504119575023651,\n",
       "  0.2498263716697693,\n",
       "  0.2492133229970932,\n",
       "  0.24891629815101624,\n",
       "  0.2488008588552475,\n",
       "  0.24869509041309357,\n",
       "  0.24847973883152008,\n",
       "  0.2483661025762558,\n",
       "  0.24865742027759552,\n",
       "  0.24922634661197662,\n",
       "  0.24975962936878204,\n",
       "  0.24955178797245026,\n",
       "  0.2483849823474884,\n",
       "  0.24733690917491913,\n",
       "  0.24650000035762787,\n",
       "  0.24610289931297302,\n",
       "  0.24610885977745056,\n",
       "  0.24627768993377686,\n",
       "  0.24646931886672974,\n",
       "  0.24662218987941742,\n",
       "  0.2472800761461258,\n",
       "  0.24791741371154785,\n",
       "  0.24784691631793976,\n",
       "  0.24682749807834625,\n",
       "  0.24542592465877533,\n",
       "  0.244306743144989,\n",
       "  0.24364914000034332,\n",
       "  0.2433910220861435,\n",
       "  0.2431105077266693,\n",
       "  0.24304236471652985,\n",
       "  0.2431042194366455,\n",
       "  0.24309881031513214,\n",
       "  0.24299560487270355,\n",
       "  0.2430994063615799,\n",
       "  0.2424831986427307,\n",
       "  0.24169886112213135,\n",
       "  0.24099738895893097,\n",
       "  0.24050867557525635,\n",
       "  0.2402421534061432,\n",
       "  0.24022215604782104,\n",
       "  0.24043551087379456,\n",
       "  0.24077332019805908,\n",
       "  0.24038727581501007,\n",
       "  0.23985058069229126,\n",
       "  0.23917336761951447,\n",
       "  0.2385302484035492,\n",
       "  0.23777124285697937,\n",
       "  0.23714423179626465,\n",
       "  0.2367895543575287,\n",
       "  0.23742592334747314,\n",
       "  0.23798048496246338,\n",
       "  0.2373092621564865,\n",
       "  0.2355143129825592,\n",
       "  0.23428906500339508,\n",
       "  0.23396100103855133,\n",
       "  0.23375283181667328,\n",
       "  0.23351188004016876,\n",
       "  0.23392996191978455,\n",
       "  0.23483823239803314,\n",
       "  0.23519575595855713,\n",
       "  0.23522856831550598,\n",
       "  0.23490716516971588,\n",
       "  0.23441755771636963,\n",
       "  0.2341286987066269,\n",
       "  0.2338685393333435,\n",
       "  0.23365655541419983,\n",
       "  0.2334214448928833,\n",
       "  0.23325656354427338,\n",
       "  0.23344416916370392,\n",
       "  0.23332394659519196,\n",
       "  0.23278944194316864,\n",
       "  0.23190298676490784,\n",
       "  0.23089781403541565,\n",
       "  0.23007676005363464,\n",
       "  0.2297082245349884,\n",
       "  0.23025569319725037,\n",
       "  0.23199664056301117,\n",
       "  0.23302693665027618,\n",
       "  0.23152928054332733,\n",
       "  0.22815825045108795,\n",
       "  0.2260492891073227,\n",
       "  0.22552523016929626,\n",
       "  0.22511300444602966,\n",
       "  0.2246110737323761,\n",
       "  0.22470273077487946,\n",
       "  0.22636659443378448,\n",
       "  0.22855918109416962,\n",
       "  0.2278437614440918,\n",
       "  0.2253897786140442,\n",
       "  0.22451917827129364,\n",
       "  0.2245975285768509,\n",
       "  0.22514817118644714,\n",
       "  0.2247774600982666,\n",
       "  0.22450605034828186,\n",
       "  0.22444558143615723,\n",
       "  0.2241663783788681,\n",
       "  0.22295573353767395,\n",
       "  0.221865713596344,\n",
       "  0.22125227749347687,\n",
       "  0.22125130891799927,\n",
       "  0.22171393036842346,\n",
       "  0.22249040007591248,\n",
       "  0.22194863855838776,\n",
       "  0.22034984827041626,\n",
       "  0.21878840029239655,\n",
       "  0.21829821169376373,\n",
       "  0.21920375525951385,\n",
       "  0.21996909379959106,\n",
       "  0.21923790872097015,\n",
       "  0.2177215963602066,\n",
       "  0.2162560671567917,\n",
       "  0.21453796327114105,\n",
       "  0.21312658488750458,\n",
       "  0.2122069001197815,\n",
       "  0.21177271008491516,\n",
       "  0.21234452724456787,\n",
       "  0.21394997835159302,\n",
       "  0.2155640423297882,\n",
       "  0.2156878411769867,\n",
       "  0.21552172303199768,\n",
       "  0.21445396542549133,\n",
       "  0.21363215148448944,\n",
       "  0.2129584103822708,\n",
       "  0.21247880160808563,\n",
       "  0.21293263137340546,\n",
       "  0.2137962281703949,\n",
       "  0.21375395357608795,\n",
       "  0.21287186443805695,\n",
       "  0.21144995093345642,\n",
       "  0.21078655123710632,\n",
       "  0.2111893594264984,\n",
       "  0.2132246196269989,\n",
       "  0.21373189985752106,\n",
       "  0.21274201571941376,\n",
       "  0.2124907672405243,\n",
       "  0.21225982904434204,\n",
       "  0.2112426459789276,\n",
       "  0.2093352973461151,\n",
       "  0.20802916586399078,\n",
       "  0.2074849158525467,\n",
       "  0.20771782100200653,\n",
       "  0.2077370434999466,\n",
       "  0.20747391879558563,\n",
       "  0.208136186003685,\n",
       "  0.20840024948120117,\n",
       "  0.2076108753681183,\n",
       "  0.20722061395645142,\n",
       "  0.2058417797088623,\n",
       "  0.20501741766929626,\n",
       "  0.20429731905460358,\n",
       "  0.20435084402561188,\n",
       "  0.20497022569179535,\n",
       "  0.20587070286273956,\n",
       "  0.20580734312534332,\n",
       "  0.20442862808704376,\n",
       "  0.2020610123872757,\n",
       "  0.19996266067028046,\n",
       "  0.1982896327972412,\n",
       "  0.1976405680179596,\n",
       "  0.19800622761249542,\n",
       "  0.2000836431980133,\n",
       "  0.20139460265636444,\n",
       "  0.20193248987197876,\n",
       "  0.2018561065196991,\n",
       "  0.19958169758319855,\n",
       "  0.19599328935146332,\n",
       "  0.19545407593250275,\n",
       "  0.19673855602741241,\n",
       "  0.19917690753936768,\n",
       "  0.20190072059631348,\n",
       "  0.2004445642232895,\n",
       "  0.1968313604593277,\n",
       "  0.19360725581645966,\n",
       "  0.1923329085111618,\n",
       "  0.1911744922399521,\n",
       "  0.1906951665878296,\n",
       "  0.18961593508720398,\n",
       "  0.1882210671901703,\n",
       "  0.18736152350902557,\n",
       "  0.1873309314250946,\n",
       "  0.18918031454086304,\n",
       "  0.19151245057582855,\n",
       "  0.1895081251859665,\n",
       "  0.18860721588134766,\n",
       "  0.1887257695198059,\n",
       "  0.1890600323677063,\n",
       "  0.1905142366886139,\n",
       "  0.19194599986076355,\n",
       "  0.1896824985742569,\n",
       "  0.18447040021419525,\n",
       "  0.1811569333076477,\n",
       "  0.18190741539001465,\n",
       "  0.1850089132785797,\n",
       "  0.18761180341243744,\n",
       "  0.18779808282852173,\n",
       "  0.18710963428020477,\n",
       "  0.1864439994096756,\n",
       "  0.18366025388240814,\n",
       "  0.18038763105869293,\n",
       "  0.17927293479442596,\n",
       "  0.17981697618961334,\n",
       "  0.18325896561145782,\n",
       "  0.1834760308265686,\n",
       "  0.1777346432209015,\n",
       "  0.17285774648189545,\n",
       "  0.17259831726551056,\n",
       "  0.173458531498909,\n",
       "  0.17992877960205078,\n",
       "  0.18602383136749268,\n",
       "  0.18238669633865356,\n",
       "  0.17525707185268402,\n",
       "  0.17192474007606506,\n",
       "  0.17128518223762512,\n",
       "  0.17501558363437653,\n",
       "  0.18093058466911316,\n",
       "  0.18122471868991852,\n",
       "  0.17585091292858124,\n",
       "  0.17077454924583435,\n",
       "  0.16792762279510498,\n",
       "  0.16641688346862793,\n",
       "  0.1671408712863922,\n",
       "  0.17396609485149384,\n",
       "  0.18228673934936523,\n",
       "  0.17766961455345154,\n",
       "  0.1696178913116455,\n",
       "  0.16690461337566376,\n",
       "  0.16926366090774536,\n",
       "  0.17646849155426025,\n",
       "  0.17925576865673065,\n",
       "  0.17207114398479462,\n",
       "  0.16529330611228943,\n",
       "  0.16183805465698242,\n",
       "  0.16343264281749725,\n",
       "  0.16806858777999878,\n",
       "  0.1671096235513687,\n",
       "  0.1632712036371231,\n",
       "  0.16054432094097137,\n",
       "  0.16067034006118774,\n",
       "  0.16414088010787964,\n",
       "  0.1717110276222229,\n",
       "  0.1735610067844391,\n",
       "  0.16816042363643646,\n",
       "  0.16329967975616455,\n",
       "  0.16163748502731323,\n",
       "  0.16443495452404022,\n",
       "  0.1675252467393875,\n",
       "  0.16745980083942413,\n",
       "  0.16525183618068695,\n",
       "  0.16208991408348083,\n",
       "  0.16311860084533691,\n",
       "  0.16493669152259827,\n",
       "  0.16299645602703094,\n",
       "  0.16060420870780945,\n",
       "  0.1596180945634842,\n",
       "  0.1590387523174286,\n",
       "  0.15883174538612366,\n",
       "  0.15961112082004547,\n",
       "  0.16023100912570953,\n",
       "  0.16254551708698273,\n",
       "  0.16106511652469635,\n",
       "  0.156307190656662,\n",
       "  0.1527957022190094,\n",
       "  0.15284322202205658,\n",
       "  0.1568012237548828,\n",
       "  0.1613968163728714,\n",
       "  0.16280002892017365,\n",
       "  0.15999431908130646,\n",
       "  0.15782596170902252,\n",
       "  0.15832598507404327,\n",
       "  0.15831013023853302,\n",
       "  0.15709838271141052,\n",
       "  0.15398988127708435,\n",
       "  0.15263213217258453,\n",
       "  0.15336626768112183,\n",
       "  0.15267647802829742,\n",
       "  0.15150755643844604,\n",
       "  0.1481665074825287,\n",
       "  0.1451897919178009,\n",
       "  0.14399762451648712,\n",
       "  0.14733044803142548,\n",
       "  0.14993664622306824,\n",
       "  0.15063126385211945,\n",
       "  0.1515011340379715,\n",
       "  0.15265554189682007,\n",
       "  0.1491958349943161,\n",
       "  0.14748534560203552,\n",
       "  0.15013061463832855,\n",
       "  0.15389493107795715,\n",
       "  0.15508955717086792,\n",
       "  0.15262016654014587,\n",
       "  0.1495632529258728,\n",
       "  0.14864717423915863,\n",
       "  0.15163202583789825,\n",
       "  0.15302908420562744,\n",
       "  0.14687053859233856,\n",
       "  0.14176352322101593,\n",
       "  0.14011499285697937,\n",
       "  0.13966438174247742,\n",
       "  0.1426825225353241,\n",
       "  0.1464787870645523,\n",
       "  0.14604812860488892,\n",
       "  0.14028988778591156,\n",
       "  0.1361607313156128,\n",
       "  0.13661612570285797,\n",
       "  0.14085853099822998,\n",
       "  0.1478988230228424,\n",
       "  0.15165264904499054,\n",
       "  0.14592647552490234,\n",
       "  0.13970555365085602,\n",
       "  0.13995574414730072,\n",
       "  0.14553523063659668,\n",
       "  0.14738015830516815,\n",
       "  0.1398991346359253,\n",
       "  0.1336977332830429,\n",
       "  0.13354402780532837,\n",
       "  0.13775235414505005,\n",
       "  0.1407797634601593,\n",
       "  0.14133481681346893,\n",
       "  0.13731224834918976,\n",
       "  0.1335393339395523,\n",
       "  0.13878734409809113,\n",
       "  0.14661329984664917,\n",
       "  0.14542730152606964,\n",
       "  0.13885962963104248,\n",
       "  0.13302390277385712,\n",
       "  0.13165946304798126,\n",
       "  0.13410675525665283,\n",
       "  0.1386641412973404,\n",
       "  0.14320504665374756,\n",
       "  0.13946633040905,\n",
       "  0.13399924337863922,\n",
       "  0.1329890489578247,\n",
       "  0.13634014129638672,\n",
       "  0.14273416996002197,\n",
       "  0.14409779012203217,\n",
       "  0.13860994577407837,\n",
       "  0.13123323023319244,\n",
       "  0.12979470193386078,\n",
       "  0.13075338304042816,\n",
       "  0.13536886870861053,\n",
       "  0.14247652888298035,\n",
       "  0.1433330774307251,\n",
       "  0.13404473662376404,\n",
       "  0.13078485429286957,\n",
       "  0.13444475829601288,\n",
       "  0.14093826711177826,\n",
       "  0.14387618005275726,\n",
       "  0.13331173360347748,\n",
       "  0.1268957108259201,\n",
       "  0.13050855696201324,\n",
       "  0.13836976885795593,\n",
       "  0.13356263935565948,\n",
       "  0.13074161112308502,\n",
       "  0.13123148679733276,\n",
       "  0.13508932292461395,\n",
       "  0.13322263956069946,\n",
       "  0.12885230779647827,\n",
       "  0.13064277172088623,\n",
       "  0.1336514949798584,\n",
       "  0.1356375515460968,\n",
       "  0.13813070952892303,\n",
       "  0.1343911737203598,\n",
       "  0.12825682759284973,\n",
       "  0.12782607972621918,\n",
       "  0.1295110285282135,\n",
       "  0.13703592121601105,\n",
       "  0.1399185210466385,\n",
       "  0.13354268670082092,\n",
       "  0.12816764414310455,\n",
       "  0.12758192420005798,\n",
       "  0.13668449223041534,\n",
       "  0.1438765972852707,\n",
       "  0.1384076029062271,\n",
       "  0.12889552116394043,\n",
       "  0.12795624136924744,\n",
       "  0.1347150206565857,\n",
       "  0.1436322182416916,\n",
       "  0.14346009492874146,\n",
       "  0.13673128187656403,\n",
       "  0.1271948367357254,\n",
       "  0.12237362563610077,\n",
       "  0.12439597398042679,\n",
       "  0.14000947773456573,\n",
       "  0.14732438325881958,\n",
       "  0.12874792516231537,\n",
       "  0.1211920902132988,\n",
       "  0.12080724537372589,\n",
       "  0.13754285871982574,\n",
       "  0.15457448363304138,\n",
       "  0.13463479280471802,\n",
       "  0.12012989073991776,\n",
       "  0.11979343742132187,\n",
       "  0.13072623312473297,\n",
       "  0.14570224285125732,\n",
       "  0.13612541556358337,\n",
       "  0.12158187478780746,\n",
       "  0.1203160360455513,\n",
       "  0.12499206513166428,\n",
       "  0.1482507884502411,\n",
       "  0.1479734182357788,\n",
       "  0.12827777862548828,\n",
       "  0.1198093593120575,\n",
       "  0.12044563889503479,\n",
       "  0.12948676943778992,\n",
       "  0.1340860277414322,\n",
       "  0.12602485716342926,\n",
       "  0.1198631227016449,\n",
       "  0.11888048052787781,\n",
       "  0.12719787657260895,\n",
       "  0.1409992128610611,\n",
       "  0.13866086304187775,\n",
       "  0.1251068413257599,\n",
       "  0.11940012872219086,\n",
       "  0.12332761287689209,\n",
       "  0.1308160275220871,\n",
       "  0.128859281539917,\n",
       "  0.12487868964672089,\n",
       "  0.12373041361570358,\n",
       "  0.12458495795726776,\n",
       "  0.12275980412960052,\n",
       "  0.12452741712331772,\n",
       "  0.12787188589572906,\n",
       "  0.13307267427444458,\n",
       "  0.12971921265125275,\n",
       "  0.12127954512834549,\n",
       "  0.11923433095216751,\n",
       "  0.12189999222755432,\n",
       "  0.12949860095977783,\n",
       "  0.12930694222450256,\n",
       "  0.12201156467199326,\n",
       "  0.1195513904094696,\n",
       "  0.12467406690120697,\n",
       "  0.13363687694072723,\n",
       "  0.13440172374248505,\n",
       "  0.12446794658899307,\n",
       "  0.11998078227043152,\n",
       "  0.12078787386417389,\n",
       "  0.12551338970661163,\n",
       "  0.12792131304740906,\n",
       "  0.12426068633794785,\n",
       "  0.12555749714374542,\n",
       "  0.13094407320022583,\n",
       "  0.12990525364875793,\n",
       "  0.12385982275009155,\n",
       "  0.12033847719430923,\n",
       "  0.1229889765381813,\n",
       "  0.13094551861286163,\n",
       "  0.1348663866519928,\n",
       "  0.131697878241539,\n",
       "  0.1209193617105484,\n",
       "  0.11672616004943848,\n",
       "  0.12271035462617874,\n",
       "  0.13493341207504272,\n",
       "  0.13140864670276642,\n",
       "  0.12174799293279648,\n",
       "  0.11890285462141037,\n",
       "  0.11956435441970825,\n",
       "  0.12796294689178467,\n",
       "  0.12960569560527802,\n",
       "  0.12298550456762314,\n",
       "  0.11556220054626465,\n",
       "  0.11559406667947769,\n",
       "  0.12171361595392227,\n",
       "  0.12670886516571045,\n",
       "  0.12721101939678192,\n",
       "  0.1207595020532608,\n",
       "  0.11740101873874664,\n",
       "  0.11755917221307755,\n",
       "  0.11852071434259415,\n",
       "  0.12347626686096191,\n",
       "  0.12635433673858643,\n",
       "  0.12079457938671112,\n",
       "  0.11658866703510284,\n",
       "  0.11557898670434952,\n",
       "  0.12019679695367813,\n",
       "  0.13106800615787506,\n",
       "  0.13494496047496796,\n",
       "  0.1220322847366333,\n",
       "  0.11407596617937088,\n",
       "  0.11539603769779205,\n",
       "  0.13054081797599792,\n",
       "  0.14118394255638123,\n",
       "  0.13440042734146118,\n",
       "  0.12183257937431335,\n",
       "  0.1162409707903862,\n",
       "  0.12039286643266678,\n",
       "  0.12382888793945312,\n",
       "  0.1224016547203064,\n",
       "  0.12364397943019867,\n",
       "  0.12146265059709549,\n",
       "  0.11995213478803635,\n",
       "  0.12224577367305756,\n",
       "  0.12122135609388351,\n",
       "  0.11709004640579224,\n",
       "  0.11655640602111816,\n",
       "  0.11941999942064285,\n",
       "  0.12542824447155,\n",
       "  0.120111845433712,\n",
       "  0.11419219523668289,\n",
       "  0.11607497930526733,\n",
       "  0.12147577852010727,\n",
       "  0.12288489192724228,\n",
       "  0.11821310967206955,\n",
       "  0.11861152946949005,\n",
       "  0.122625932097435,\n",
       "  0.12224558740854263,\n",
       "  0.12068471312522888,\n",
       "  0.11557459086179733,\n",
       "  0.11540696024894714,\n",
       "  0.12425151467323303,\n",
       "  0.13196353614330292,\n",
       "  0.12253110855817795,\n",
       "  0.11281511187553406,\n",
       "  0.11450178921222687,\n",
       "  0.12461701035499573,\n",
       "  0.13403552770614624,\n",
       "  0.12705755233764648,\n",
       "  0.11783555895090103,\n",
       "  0.1164141371846199,\n",
       "  0.11742032319307327,\n",
       "  0.11946555227041245,\n",
       "  0.12043936550617218,\n",
       "  0.12033763527870178,\n",
       "  0.12193366140127182,\n",
       "  0.1198175698518753,\n",
       "  0.11785868555307388,\n",
       "  0.1186399981379509,\n",
       "  0.12240052968263626,\n",
       "  0.12385669350624084,\n",
       "  0.12182177603244781,\n",
       "  0.11933866888284683,\n",
       "  0.1153956800699234,\n",
       "  0.11568544805049896,\n",
       "  0.12270889431238174,\n",
       "  0.12803542613983154,\n",
       "  0.12068238854408264,\n",
       "  0.11393782496452332,\n",
       "  0.11516561359167099,\n",
       "  0.1251721829175949,\n",
       "  0.13199608027935028,\n",
       "  0.1226733922958374,\n",
       "  0.11231230199337006,\n",
       "  0.11482609808444977,\n",
       "  0.1226038932800293,\n",
       "  0.12405283004045486,\n",
       "  0.11758161336183548,\n",
       "  0.11085452139377594,\n",
       "  0.11071118712425232,\n",
       "  0.11681701242923737,\n",
       "  0.11815815418958664,\n",
       "  0.1170915961265564,\n",
       "  0.1145663857460022,\n",
       "  0.11312276124954224,\n",
       "  0.11712586879730225,\n",
       "  0.11974485218524933,\n",
       "  0.11309263855218887,\n",
       "  0.1096649169921875,\n",
       "  0.11503379791975021,\n",
       "  0.1195083037018776,\n",
       "  0.11396929621696472,\n",
       "  0.10970230400562286,\n",
       "  0.11216555535793304,\n",
       "  0.11606833338737488,\n",
       "  0.11523137986660004,\n",
       "  0.11300747096538544,\n",
       "  0.11165129393339157,\n",
       "  0.11289647221565247,\n",
       "  0.11711820960044861,\n",
       "  0.12132972478866577,\n",
       "  0.12304408848285675,\n",
       "  0.11539015173912048,\n",
       "  0.10819870233535767,\n",
       "  0.10932628810405731,\n",
       "  0.12194191664457321,\n",
       "  0.12734554708003998,\n",
       "  0.11706924438476562,\n",
       "  0.10865314304828644,\n",
       "  0.11024312674999237,\n",
       "  0.12226328998804092,\n",
       "  0.12263426929712296,\n",
       "  0.11336871236562729,\n",
       "  0.11282476782798767,\n",
       "  0.11786505579948425,\n",
       "  0.12281288951635361,\n",
       "  0.11986467242240906,\n",
       "  0.11227742582559586,\n",
       "  0.10831310600042343,\n",
       "  0.11121878027915955,\n",
       "  0.12177503854036331,\n",
       "  0.12628142535686493,\n",
       "  0.11709468066692352,\n",
       "  0.11030574142932892,\n",
       "  0.1101142093539238,\n",
       "  0.11260360479354858,\n",
       "  0.11485591530799866,\n",
       "  0.11747350543737411,\n",
       "  0.11861612647771835,\n",
       "  0.11345721036195755,\n",
       "  0.11260218918323517,\n",
       "  0.11538443714380264,\n",
       "  0.12065225094556808,\n",
       "  0.12429741770029068,\n",
       "  0.11281272023916245,\n",
       "  0.10697218775749207,\n",
       "  0.11094479262828827,\n",
       "  0.12471004575490952,\n",
       "  0.12914735078811646,\n",
       "  0.1221095472574234,\n",
       "  0.11287552863359451,\n",
       "  0.11441680788993835,\n",
       "  0.12400571256875992,\n",
       "  0.12645305693149567,\n",
       "  0.11859123408794403,\n",
       "  0.11258846521377563,\n",
       "  0.11104700714349747,\n",
       "  0.11673722416162491,\n",
       "  0.12315282225608826,\n",
       "  0.12300728261470795,\n",
       "  0.11720816045999527,\n",
       "  0.1103539690375328,\n",
       "  0.11002492159605026,\n",
       "  0.11713114380836487,\n",
       "  0.11955329030752182,\n",
       "  0.11467786878347397,\n",
       "  0.10741795599460602,\n",
       "  0.10931209474802017,\n",
       "  0.11577833443880081,\n",
       "  0.11556482315063477,\n",
       "  0.11300402134656906,\n",
       "  0.1128121018409729,\n",
       "  0.11575060337781906,\n",
       "  0.1117778792977333,\n",
       "  0.10621857643127441,\n",
       "  0.10914283990859985,\n",
       "  0.11907033622264862,\n",
       "  0.12239721417427063,\n",
       "  0.11491076648235321,\n",
       "  0.11020327359437943,\n",
       "  0.11273682862520218,\n",
       "  0.11781267076730728,\n",
       "  0.12064175307750702,\n",
       "  0.12191947549581528,\n",
       "  0.11596763879060745,\n",
       "  0.10948438197374344,\n",
       "  0.11204102635383606,\n",
       "  0.1229764074087143,\n",
       "  0.12923279404640198,\n",
       "  0.11849520355463028,\n",
       "  0.10673801600933075,\n",
       "  0.10430251061916351,\n",
       "  0.10872016102075577,\n",
       "  0.11998134851455688,\n",
       "  0.12356089800596237,\n",
       "  0.11242107301950455,\n",
       "  0.10575125366449356,\n",
       "  0.10743128508329391,\n",
       "  0.10689307749271393,\n",
       "  0.10620124638080597,\n",
       "  0.11218862235546112,\n",
       "  0.12041864544153214,\n",
       "  0.11728831380605698,\n",
       "  0.10877038538455963,\n",
       "  0.10703913867473602,\n",
       "  0.10914772003889084,\n",
       "  0.11219821870326996,\n",
       "  0.11015211045742035,\n",
       "  0.10806924849748611,\n",
       "  0.1106186956167221,\n",
       "  0.11293473839759827,\n",
       "  0.11156295239925385,\n",
       "  0.11044411361217499,\n",
       "  0.11336738616228104,\n",
       "  0.11352458596229553,\n",
       "  0.11569833755493164,\n",
       "  0.1174176037311554,\n",
       "  0.11617805063724518,\n",
       "  0.11190219968557358,\n",
       "  0.11091548949480057,\n",
       "  0.11395004391670227,\n",
       "  0.11754820495843887,\n",
       "  0.12090803682804108,\n",
       "  0.11555495113134384,\n",
       "  0.1073760837316513,\n",
       "  0.10883615911006927,\n",
       "  0.11785385012626648,\n",
       "  0.12516237795352936,\n",
       "  0.11858990788459778,\n",
       "  0.10766281932592392,\n",
       "  0.10823388397693634,\n",
       "  0.11112497746944427,\n",
       "  0.10979156196117401,\n",
       "  0.10914332419633865,\n",
       "  0.10816780477762222,\n",
       "  0.10689898580312729,\n",
       "  0.1101936399936676,\n",
       "  0.11324488371610641,\n",
       "  0.11449633538722992,\n",
       "  0.11445643752813339,\n",
       "  0.10968200862407684,\n",
       "  0.10400306433439255,\n",
       "  0.10674209147691727,\n",
       "  0.11321523040533066,\n",
       "  0.11960350722074509,\n",
       "  0.11425955593585968,\n",
       "  0.10247740894556046,\n",
       "  0.10242146998643875,\n",
       "  0.11649653315544128,\n",
       "  0.12230389565229416,\n",
       "  0.11332215368747711,\n",
       "  0.10869178175926208,\n",
       "  0.11047587543725967,\n",
       "  0.11866173893213272,\n",
       "  0.11591991037130356,\n",
       "  0.10659915953874588,\n",
       "  0.11369677633047104,\n",
       "  0.1274353563785553,\n",
       "  0.11990214139223099,\n",
       "  0.10786542296409607,\n",
       "  0.10810966044664383,\n",
       "  0.11529795825481415,\n",
       "  0.11499770730733871,\n",
       "  0.10661426186561584,\n",
       "  0.103587307035923,\n",
       "  0.10743944346904755,\n",
       "  0.12021775543689728,\n",
       "  0.12739373743534088,\n",
       "  0.11052639782428741,\n",
       "  0.10367437452077866,\n",
       "  0.11171441525220871,\n",
       "  0.1224713921546936,\n",
       "  0.11942996084690094,\n",
       "  0.10930316895246506,\n",
       "  0.1082010567188263,\n",
       "  0.11292796581983566,\n",
       "  0.11344252526760101,\n",
       "  0.10547982901334763,\n",
       "  0.10581307113170624,\n",
       "  0.11016003042459488,\n",
       "  0.11265753209590912,\n",
       "  0.11091320216655731,\n",
       "  0.11343036592006683,\n",
       "  0.11481013894081116,\n",
       "  0.10935032367706299,\n",
       "  0.10950008034706116,\n",
       "  0.10538209229707718,\n",
       "  0.1028672307729721,\n",
       "  0.1092190146446228,\n",
       "  0.11654078215360641,\n",
       "  0.11525101959705353,\n",
       "  0.10548969358205795,\n",
       "  0.10058631747961044,\n",
       "  0.10624656081199646,\n",
       "  0.12019974738359451,\n",
       "  0.12392296642065048,\n",
       "  0.11298338323831558,\n",
       "  0.10537584871053696,\n",
       "  0.10743885487318039,\n",
       "  0.11013303697109222,\n",
       "  0.1092597097158432,\n",
       "  0.11081007122993469,\n",
       "  0.11024688184261322,\n",
       "  0.10622869431972504,\n",
       "  0.11211521178483963,\n",
       "  0.11757465451955795,\n",
       "  0.11200247704982758,\n",
       "  0.10403215885162354,\n",
       "  0.10262272506952286,\n",
       "  0.11198074370622635,\n",
       "  0.11882095783948898,\n",
       "  0.10736280679702759,\n",
       "  0.10496152192354202,\n",
       "  0.11450962722301483,\n",
       "  0.1194792240858078,\n",
       "  0.10580414533615112,\n",
       "  0.09960871189832687,\n",
       "  0.10472174733877182,\n",
       "  0.12207836657762527,\n",
       "  0.12309753149747849,\n",
       "  0.10835067927837372,\n",
       "  0.09912186115980148,\n",
       "  0.10111285001039505,\n",
       "  0.11355581134557724,\n",
       "  0.11972671747207642,\n",
       "  0.11361262202262878,\n",
       "  0.10050629079341888,\n",
       "  0.10042917728424072,\n",
       "  0.11766660958528519,\n",
       "  0.1283610612154007,\n",
       "  0.10990225523710251,\n",
       "  0.09871654212474823,\n",
       "  0.10837464034557343,\n",
       "  0.13193857669830322,\n",
       "  0.12738454341888428,\n",
       "  0.1127883717417717,\n",
       "  0.10879387706518173,\n",
       "  0.1101391613483429,\n",
       "  0.10933801531791687,\n",
       "  0.10740625113248825,\n",
       "  0.10691779106855392,\n",
       "  0.10918508470058441,\n",
       "  0.11039581894874573,\n",
       "  0.10889115184545517,\n",
       "  0.10712059587240219,\n",
       "  0.1021290197968483,\n",
       "  0.1017230749130249,\n",
       "  0.10963399708271027,\n",
       "  0.1180184856057167,\n",
       "  0.11846991628408432,\n",
       "  0.11191944777965546,\n",
       "  0.10553916543722153,\n",
       "  0.1043277382850647,\n",
       "  0.10595124959945679,\n",
       "  0.10332005470991135,\n",
       "  0.10293762385845184,\n",
       "  0.10980674624443054,\n",
       "  0.11545327305793762,\n",
       "  0.1103372871875763,\n",
       "  0.1041046604514122,\n",
       "  0.10565677285194397,\n",
       "  0.10606598854064941,\n",
       "  0.10768748074769974,\n",
       "  0.11123326420783997,\n",
       "  0.10718043893575668,\n",
       "  0.10272445529699326,\n",
       "  0.1035587340593338,\n",
       "  0.10862623900175095,\n",
       "  0.11889331042766571,\n",
       "  0.11416244506835938,\n",
       "  0.09975339472293854,\n",
       "  0.09892803430557251,\n",
       "  0.10924187302589417,\n",
       "  0.11783432960510254,\n",
       "  0.10991977900266647,\n",
       "  0.10409758985042572,\n",
       "  0.10719108581542969,\n",
       "  0.11127586662769318,\n",
       "  0.10435008257627487,\n",
       "  0.10005462914705276,\n",
       "  0.10440143942832947,\n",
       "  0.11286789178848267,\n",
       "  0.11861827224493027,\n",
       "  0.1093478798866272,\n",
       "  0.10357776284217834,\n",
       "  0.10175325721502304,\n",
       "  0.10312524437904358,\n",
       "  0.10654795169830322,\n",
       "  0.10446199029684067,\n",
       "  0.10518617928028107,\n",
       "  0.10919073224067688,\n",
       "  0.10911863297224045,\n",
       "  0.10119825601577759,\n",
       "  0.10116663575172424,\n",
       "  0.10578948259353638,\n",
       "  0.10985738039016724,\n",
       "  0.1188565343618393,\n",
       "  0.11188037693500519,\n",
       "  0.10335227102041245,\n",
       "  0.10324622690677643,\n",
       "  0.11138062924146652,\n",
       "  0.11537090688943863,\n",
       "  0.10516234487295151,\n",
       "  0.1005018949508667,\n",
       "  0.10796675086021423,\n",
       "  0.1192331463098526,\n",
       "  0.11572850495576859,\n",
       "  0.10203739255666733,\n",
       "  0.10333175957202911,\n",
       "  0.12038703262805939,\n",
       "  0.122258260846138,\n",
       "  0.1083788201212883,\n",
       "  0.0978659987449646,\n",
       "  0.09998355805873871,\n",
       "  0.11107507348060608,\n",
       "  0.11398565769195557,\n",
       "  0.10548026859760284,\n",
       "  0.09792767465114594,\n",
       "  0.10049470514059067,\n",
       "  0.10845489054918289,\n",
       "  0.11606204509735107,\n",
       "  0.10963214933872223,\n",
       "  0.09842436760663986,\n",
       "  0.1039297953248024,\n",
       "  0.11585173010826111,\n",
       "  0.10758473724126816,\n",
       "  0.09758632630109787,\n",
       "  0.09795167297124863,\n",
       "  0.10857412964105606,\n",
       "  0.11618781089782715,\n",
       "  0.10463754832744598,\n",
       "  0.09696182608604431,\n",
       "  0.10536219924688339,\n",
       "  0.11689681559801102,\n",
       "  0.10870140045881271,\n",
       "  0.1025240495800972,\n",
       "  0.10628089308738708,\n",
       "  0.11552000045776367,\n",
       "  0.1157849133014679,\n",
       "  0.10308629274368286,\n",
       "  0.09845240414142609,\n",
       "  0.10255327820777893,\n",
       "  0.11324946582317352,\n",
       "  0.11448919773101807,\n",
       "  ...],\n",
       " 'val_accuracy': [0.3788819909095764,\n",
       "  0.4968944191932678,\n",
       "  0.7080745100975037,\n",
       "  0.7484471797943115,\n",
       "  0.7546584010124207,\n",
       "  0.7484471797943115,\n",
       "  0.739130437374115,\n",
       "  0.7360248565673828,\n",
       "  0.7360248565673828,\n",
       "  0.7360248565673828,\n",
       "  0.7360248565673828,\n",
       "  0.739130437374115,\n",
       "  0.7515528202056885,\n",
       "  0.7577639818191528,\n",
       "  0.7701863646507263,\n",
       "  0.7888198494911194,\n",
       "  0.804347813129425,\n",
       "  0.8136646151542664,\n",
       "  0.8416149020195007,\n",
       "  0.850931704044342,\n",
       "  0.8540372848510742,\n",
       "  0.8571428656578064,\n",
       "  0.8633540272712708,\n",
       "  0.8695651888847351,\n",
       "  0.8726708292961121,\n",
       "  0.8726708292961121,\n",
       "  0.8726708292961121,\n",
       "  0.8788819909095764,\n",
       "  0.8975155353546143,\n",
       "  0.9068322777748108,\n",
       "  0.9223602414131165,\n",
       "  0.9285714030265808,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9378882050514221,\n",
       "  0.9316770434379578,\n",
       "  0.9285714030265808,\n",
       "  0.9285714030265808,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9316770434379578,\n",
       "  0.9347826242446899,\n",
       "  0.9347826242446899,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9409937858581543,\n",
       "  0.9378882050514221,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9378882050514221,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9409937858581543,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9440993666648865,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9503105878829956,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9503105878829956,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9472049474716187,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9534161686897278,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.95652174949646,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.95652174949646,\n",
       "  0.9627329111099243,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9689440727233887,\n",
       "  0.9689440727233887,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9627329111099243,\n",
       "  0.9627329111099243,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9720497131347656,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9658384919166565,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9720497131347656,\n",
       "  0.9596273303031921,\n",
       "  0.9658384919166565,\n",
       "  0.97826087474823,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9596273303031921,\n",
       "  0.9596273303031921,\n",
       "  0.9720497131347656,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9720497131347656,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9658384919166565,\n",
       "  0.9658384919166565,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9720497131347656,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9689440727233887,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9844720363616943,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9813664555549622,\n",
       "  0.9813664555549622,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9844720363616943,\n",
       "  0.9813664555549622,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9844720363616943,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9844720363616943,\n",
       "  0.9844720363616943,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9844720363616943,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  0.97826087474823,\n",
       "  0.9844720363616943,\n",
       "  0.97826087474823,\n",
       "  0.9751552939414978,\n",
       "  0.9751552939414978,\n",
       "  ...],\n",
       " 'loss': [3.1994958599347614,\n",
       "  1.8952127892244464,\n",
       "  1.00406069576649,\n",
       "  0.6289429764105021,\n",
       "  0.5120962251200245,\n",
       "  0.5013534308027527,\n",
       "  0.5212798100335675,\n",
       "  0.5373722457228539,\n",
       "  0.5385861037153562,\n",
       "  0.5260110218163472,\n",
       "  0.5022365631589846,\n",
       "  0.4723272371985828,\n",
       "  0.43911289096243716,\n",
       "  0.4064097863152783,\n",
       "  0.37868989253299706,\n",
       "  0.3568066600453434,\n",
       "  0.3410313324303941,\n",
       "  0.32975670649665784,\n",
       "  0.32278855384582766,\n",
       "  0.3180957849138911,\n",
       "  0.31259753519689143,\n",
       "  0.3058273235470011,\n",
       "  0.29795063655555337,\n",
       "  0.288977062400961,\n",
       "  0.2815132028612937,\n",
       "  0.2758449806873138,\n",
       "  0.27130083125911847,\n",
       "  0.26768057643910825,\n",
       "  0.2638847586480618,\n",
       "  0.2606325722763038,\n",
       "  0.25625767057924137,\n",
       "  0.2520644828152328,\n",
       "  0.24744249836956742,\n",
       "  0.24414705305972106,\n",
       "  0.241822071407692,\n",
       "  0.2391416464474446,\n",
       "  0.23642157123154556,\n",
       "  0.23332349561257532,\n",
       "  0.23047315428578288,\n",
       "  0.22786750540616502,\n",
       "  0.22549296461422264,\n",
       "  0.22309421215086583,\n",
       "  0.2206798138105486,\n",
       "  0.21819263849561465,\n",
       "  0.21566211301006183,\n",
       "  0.21337247615933966,\n",
       "  0.2111802431562228,\n",
       "  0.20899666868618036,\n",
       "  0.2071161107894655,\n",
       "  0.2048612396028834,\n",
       "  0.20308913612256188,\n",
       "  0.20193642964315633,\n",
       "  0.20011048477605145,\n",
       "  0.19845421002980942,\n",
       "  0.19719386527494487,\n",
       "  0.19581244082673585,\n",
       "  0.1947151465857741,\n",
       "  0.1936087399310394,\n",
       "  0.192404868158228,\n",
       "  0.19166192323380188,\n",
       "  0.1914060036174006,\n",
       "  0.1913113356275544,\n",
       "  0.18995325340337446,\n",
       "  0.18852337327704496,\n",
       "  0.18803555736315378,\n",
       "  0.18810224706509576,\n",
       "  0.18760530463495445,\n",
       "  0.18680869827865643,\n",
       "  0.18565180970735243,\n",
       "  0.1849017897207193,\n",
       "  0.18448608634482846,\n",
       "  0.18376669381241703,\n",
       "  0.18317003628013692,\n",
       "  0.18251269807037862,\n",
       "  0.18214510534112704,\n",
       "  0.18146415486087478,\n",
       "  0.1808717421057761,\n",
       "  0.18025968129109096,\n",
       "  0.17955491236577903,\n",
       "  0.17922568485528734,\n",
       "  0.1784185758502341,\n",
       "  0.17792112283283135,\n",
       "  0.17764429387509548,\n",
       "  0.17751258429122738,\n",
       "  0.1768546544848321,\n",
       "  0.17673879955391789,\n",
       "  0.17589353428865465,\n",
       "  0.17532951625528964,\n",
       "  0.17483500264596427,\n",
       "  0.17441238729913602,\n",
       "  0.17405952319251816,\n",
       "  0.1737322347954623,\n",
       "  0.17333876602865106,\n",
       "  0.17288441638851604,\n",
       "  0.17253270460033124,\n",
       "  0.1723189237563205,\n",
       "  0.1719371232833103,\n",
       "  0.17162145612615903,\n",
       "  0.17113453419066132,\n",
       "  0.17082193471023535,\n",
       "  0.17043020031992545,\n",
       "  0.17045959494836846,\n",
       "  0.17052631780710556,\n",
       "  0.1703737785221059,\n",
       "  0.16979594802509587,\n",
       "  0.1692565837233209,\n",
       "  0.16873156723165586,\n",
       "  0.1684300712157172,\n",
       "  0.16846875196028996,\n",
       "  0.16877329799062807,\n",
       "  0.16843309975692725,\n",
       "  0.16765540571252932,\n",
       "  0.16719854149573796,\n",
       "  0.16714033888364824,\n",
       "  0.16682657236618434,\n",
       "  0.16643366627224007,\n",
       "  0.16621862705597287,\n",
       "  0.16613281624331774,\n",
       "  0.16593995964180272,\n",
       "  0.16606314045592435,\n",
       "  0.16621024279097868,\n",
       "  0.16590395604030644,\n",
       "  0.1651543484256242,\n",
       "  0.16437317314593386,\n",
       "  0.1644836867065561,\n",
       "  0.16447114894255013,\n",
       "  0.16420004177969777,\n",
       "  0.16387303172401047,\n",
       "  0.16321041269561598,\n",
       "  0.16303767318656215,\n",
       "  0.16278094041858662,\n",
       "  0.16263184044937992,\n",
       "  0.16220852716684706,\n",
       "  0.16196315074952783,\n",
       "  0.16178125535547278,\n",
       "  0.161595592044214,\n",
       "  0.16131190437635273,\n",
       "  0.1611665293779709,\n",
       "  0.1608028040363084,\n",
       "  0.1606199227923821,\n",
       "  0.16034298116315562,\n",
       "  0.1600358050778668,\n",
       "  0.1597678126228166,\n",
       "  0.15975942187707237,\n",
       "  0.15992755968848194,\n",
       "  0.15980023522198109,\n",
       "  0.1591357120967386,\n",
       "  0.1586270216641346,\n",
       "  0.15867423125832222,\n",
       "  0.15879778568540195,\n",
       "  0.15808374686865492,\n",
       "  0.15776558784029202,\n",
       "  0.1577042197697378,\n",
       "  0.1580688036966835,\n",
       "  0.15778191670615677,\n",
       "  0.15692435293978974,\n",
       "  0.15651861821254215,\n",
       "  0.15625062436549988,\n",
       "  0.15620516477920013,\n",
       "  0.15600827035093381,\n",
       "  0.15588090750196262,\n",
       "  0.1555957044567485,\n",
       "  0.1553674893393816,\n",
       "  0.15498836400132815,\n",
       "  0.15471752375866332,\n",
       "  0.15446604924079652,\n",
       "  0.15408695575882428,\n",
       "  0.15390776209590262,\n",
       "  0.15379234274441397,\n",
       "  0.15346642256148194,\n",
       "  0.15324222740316465,\n",
       "  0.15333665537706379,\n",
       "  0.15349418315095156,\n",
       "  0.15265587339631267,\n",
       "  0.1520400746721221,\n",
       "  0.15241265173710505,\n",
       "  0.15303695763234523,\n",
       "  0.151775692976087,\n",
       "  0.15081345820499964,\n",
       "  0.15198916851743988,\n",
       "  0.15310824369215856,\n",
       "  0.15284222576647405,\n",
       "  0.15157831678164133,\n",
       "  0.150014709207767,\n",
       "  0.15007174252368408,\n",
       "  0.15079117740916623,\n",
       "  0.15020573035361756,\n",
       "  0.14928064964266685,\n",
       "  0.1489542414181813,\n",
       "  0.14881603371859692,\n",
       "  0.14839006980929587,\n",
       "  0.1481466102791782,\n",
       "  0.14793338244395818,\n",
       "  0.14779009322021128,\n",
       "  0.14738722740006482,\n",
       "  0.14708558493971277,\n",
       "  0.1473537753086543,\n",
       "  0.14717265931607024,\n",
       "  0.14659618275956027,\n",
       "  0.146347395904579,\n",
       "  0.1461630611242598,\n",
       "  0.14583021390949238,\n",
       "  0.1455550488615109,\n",
       "  0.14582700696509976,\n",
       "  0.14517058388745072,\n",
       "  0.1449838663589717,\n",
       "  0.14479858826440836,\n",
       "  0.14447328254329483,\n",
       "  0.14408881448859642,\n",
       "  0.1439193447233159,\n",
       "  0.14400425288615146,\n",
       "  0.14410727173318177,\n",
       "  0.1439522081047616,\n",
       "  0.14365737375219967,\n",
       "  0.14305754790330735,\n",
       "  0.1429404517319082,\n",
       "  0.1428486904953742,\n",
       "  0.14243557140984178,\n",
       "  0.14197423941189077,\n",
       "  0.1416470547638845,\n",
       "  0.14161926574126502,\n",
       "  0.14154648573164919,\n",
       "  0.1411361611386901,\n",
       "  0.14083479050472722,\n",
       "  0.1406112443570156,\n",
       "  0.14046547939547352,\n",
       "  0.14027902056073704,\n",
       "  0.13985300216977847,\n",
       "  0.13987315216801982,\n",
       "  0.1393558016049734,\n",
       "  0.13963311445110607,\n",
       "  0.13973440172843873,\n",
       "  0.13901326156137184,\n",
       "  0.13876740613903787,\n",
       "  0.13854768737625747,\n",
       "  0.13818042610085943,\n",
       "  0.13783701498968495,\n",
       "  0.1377209948909228,\n",
       "  0.13745118940040857,\n",
       "  0.13734611860871498,\n",
       "  0.13714917267674143,\n",
       "  0.13683472646779707,\n",
       "  0.1367708934759108,\n",
       "  0.1366773124036891,\n",
       "  0.1361781932858559,\n",
       "  0.13599821895497863,\n",
       "  0.13590930511812704,\n",
       "  0.13566178553502373,\n",
       "  0.13536698461309143,\n",
       "  0.13507013506673926,\n",
       "  0.1347366598125256,\n",
       "  0.13471567320422045,\n",
       "  0.13453312138320841,\n",
       "  0.13414501620151364,\n",
       "  0.13380183845438964,\n",
       "  0.13371638334135183,\n",
       "  0.13380945098527905,\n",
       "  0.13406049970189426,\n",
       "  0.13337412028135603,\n",
       "  0.13248434771211734,\n",
       "  0.13233158141830978,\n",
       "  0.13215915211811546,\n",
       "  0.13207476183338515,\n",
       "  0.1316983333316733,\n",
       "  0.13147757455683776,\n",
       "  0.13103869595032921,\n",
       "  0.1303026011901099,\n",
       "  0.13041142176888074,\n",
       "  0.13059303244350515,\n",
       "  0.1299719919791448,\n",
       "  0.1295049308612372,\n",
       "  0.1288360604876581,\n",
       "  0.12863987157118448,\n",
       "  0.12846842547379445,\n",
       "  0.12822312541066413,\n",
       "  0.1279226629146208,\n",
       "  0.1279032532051548,\n",
       "  0.12775160787572903,\n",
       "  0.1272831518904721,\n",
       "  0.12774391755301956,\n",
       "  0.1263275164740009,\n",
       "  0.12618056745204226,\n",
       "  0.12592229653281056,\n",
       "  0.12560159687928366,\n",
       "  0.12531982747920623,\n",
       "  0.12524770574493027,\n",
       "  0.12519792098820118,\n",
       "  0.12439890808296057,\n",
       "  0.1252503232719522,\n",
       "  0.12573395443546828,\n",
       "  0.12453805466979788,\n",
       "  0.12337990483956535,\n",
       "  0.1233431205016548,\n",
       "  0.12324379066185052,\n",
       "  0.12281058573522027,\n",
       "  0.12251571023902338,\n",
       "  0.12211392066836539,\n",
       "  0.12162556247086839,\n",
       "  0.12160395625493793,\n",
       "  0.12096132522885321,\n",
       "  0.12135224491585998,\n",
       "  0.12094764032645028,\n",
       "  0.11972338926463544,\n",
       "  0.12309591946795377,\n",
       "  0.1243553705182594,\n",
       "  0.12048268973097137,\n",
       "  0.11978933840396759,\n",
       "  0.12055522075655997,\n",
       "  0.11912519504154988,\n",
       "  0.11834760865975302,\n",
       "  0.11981881206789571,\n",
       "  0.11871109150954812,\n",
       "  0.11753061671625784,\n",
       "  0.11832188129151214,\n",
       "  0.11819730841684488,\n",
       "  0.11695710155445438,\n",
       "  0.1164722454150822,\n",
       "  0.11697711892551521,\n",
       "  0.1175591497244185,\n",
       "  0.11569423595896906,\n",
       "  0.11668296611281669,\n",
       "  0.11767873683713297,\n",
       "  0.11557665745660685,\n",
       "  0.11556925362184256,\n",
       "  0.11593595714198503,\n",
       "  0.11413177925448688,\n",
       "  0.11560359213837804,\n",
       "  0.11565173870297706,\n",
       "  0.11326962134374412,\n",
       "  0.11391376804383571,\n",
       "  0.11429171912246605,\n",
       "  0.11387844393985377,\n",
       "  0.11275142196217869,\n",
       "  0.1122949994062209,\n",
       "  0.11194959205288617,\n",
       "  0.11240594043392135,\n",
       "  0.11228798491848921,\n",
       "  0.1110448585010232,\n",
       "  0.11226647268484416,\n",
       "  0.11220085950622515,\n",
       "  0.11071532423883265,\n",
       "  0.1110141230304975,\n",
       "  0.1106089487626213,\n",
       "  0.11008241167933798,\n",
       "  0.10999161403221157,\n",
       "  0.10980560118261194,\n",
       "  0.10973506597702208,\n",
       "  0.10897782996045868,\n",
       "  0.10891059317139717,\n",
       "  0.10914289675208,\n",
       "  0.10873395860605911,\n",
       "  0.1081649353114423,\n",
       "  0.10770678200729043,\n",
       "  0.10746252205890316,\n",
       "  0.10703794105951125,\n",
       "  0.10691933050911423,\n",
       "  0.10655969316296701,\n",
       "  0.10652185168693479,\n",
       "  0.10591090572099408,\n",
       "  0.10586232986375348,\n",
       "  0.10690360775311544,\n",
       "  0.10607766611059447,\n",
       "  0.10494787778825161,\n",
       "  0.10534411874153347,\n",
       "  0.10534943312771652,\n",
       "  0.10504789422244792,\n",
       "  0.10442475139364532,\n",
       "  0.10425651367827908,\n",
       "  0.10407400311499972,\n",
       "  0.10353502377890518,\n",
       "  0.10337920930767132,\n",
       "  0.10309308007427963,\n",
       "  0.10259183161930505,\n",
       "  0.10258555054025664,\n",
       "  0.10217162105290485,\n",
       "  0.10255780628957939,\n",
       "  0.1035150921750945,\n",
       "  0.10350143944824637,\n",
       "  0.10170291494264354,\n",
       "  0.1012211148876691,\n",
       "  0.10120593488307679,\n",
       "  0.10081819018169348,\n",
       "  0.10030365244672274,\n",
       "  0.10034250159951848,\n",
       "  0.10058790624461167,\n",
       "  0.1001094498573182,\n",
       "  0.100143908940906,\n",
       "  0.10013587394041817,\n",
       "  0.09967161592946118,\n",
       "  0.0994954076902242,\n",
       "  0.09892122387018905,\n",
       "  0.09893830373860428,\n",
       "  0.09867501833902566,\n",
       "  0.09801384998317152,\n",
       "  0.09785824396845758,\n",
       "  0.09784734445363055,\n",
       "  0.09753996393947105,\n",
       "  0.09718687747923192,\n",
       "  0.09767379077753283,\n",
       "  0.09714644046280688,\n",
       "  0.09640368425599649,\n",
       "  0.0975824278561116,\n",
       "  0.09695220941743296,\n",
       "  0.09535946344434759,\n",
       "  0.09633052549902549,\n",
       "  0.09676863958240468,\n",
       "  0.09548180064280584,\n",
       "  0.09545094358975818,\n",
       "  0.09468148437658094,\n",
       "  0.09496967582343001,\n",
       "  0.09502693565753481,\n",
       "  0.09369799554074157,\n",
       "  0.09641722649062026,\n",
       "  0.09620155332464536,\n",
       "  0.09352594060655027,\n",
       "  0.09337554396248521,\n",
       "  0.09367181746371855,\n",
       "  0.09384876665349759,\n",
       "  0.09353565288904432,\n",
       "  0.09262545009187863,\n",
       "  0.09390410243506453,\n",
       "  0.09328491216779303,\n",
       "  0.0916917194108868,\n",
       "  0.09287543948540096,\n",
       "  0.09381676284998518,\n",
       "  0.09162846838579061,\n",
       "  0.09157742360238824,\n",
       "  0.09195566758125519,\n",
       "  0.09118904216731306,\n",
       "  0.0912332129058575,\n",
       "  0.09105888229279935,\n",
       "  0.09024816548386175,\n",
       "  0.09138157757235887,\n",
       "  0.09151882933849215,\n",
       "  0.08978662949631627,\n",
       "  0.09077762826067748,\n",
       "  0.09268398370713589,\n",
       "  0.0911257856198876,\n",
       "  0.08918894826224898,\n",
       "  0.09065287500893723,\n",
       "  0.09032632378578916,\n",
       "  0.0892077938726266,\n",
       "  0.0888911212293879,\n",
       "  0.08818687529375871,\n",
       "  0.08951376200543794,\n",
       "  0.08976067658542308,\n",
       "  0.08903627585944866,\n",
       "  0.09097257905723855,\n",
       "  0.09071022181972793,\n",
       "  0.08796689070612877,\n",
       "  0.08724048171209522,\n",
       "  0.08749973818454407,\n",
       "  0.08729841159642382,\n",
       "  0.087651503738729,\n",
       "  0.08734869417288037,\n",
       "  0.08738146050281217,\n",
       "  0.08656374569326228,\n",
       "  0.0863439667781681,\n",
       "  0.08642180281110781,\n",
       "  0.0870312863296243,\n",
       "  0.08659040220481148,\n",
       "  0.08677163221523919,\n",
       "  0.08678489821930209,\n",
       "  0.08562044582542197,\n",
       "  0.08614728843955863,\n",
       "  0.08674035449533696,\n",
       "  0.08525893775600751,\n",
       "  0.08641479921970761,\n",
       "  0.08486911976798388,\n",
       "  0.08534596008669362,\n",
       "  0.08688182876677827,\n",
       "  0.08522697997380727,\n",
       "  0.08575817317363861,\n",
       "  0.0854178060981435,\n",
       "  0.08452949336919631,\n",
       "  0.08617518239784387,\n",
       "  0.08615844061235922,\n",
       "  0.08393460920631064,\n",
       "  0.0838451763503767,\n",
       "  0.08761856413430678,\n",
       "  0.08341883402460384,\n",
       "  0.0867556209175364,\n",
       "  0.08659087828892478,\n",
       "  0.08124496639459458,\n",
       "  0.09379750887248636,\n",
       "  0.08755159209187144,\n",
       "  0.08650563098204994,\n",
       "  0.08942721845727603,\n",
       "  0.08185829854303626,\n",
       "  0.08895469874508713,\n",
       "  0.0872920187871909,\n",
       "  0.08296673847604857,\n",
       "  0.08648174455757346,\n",
       "  0.08179625618923313,\n",
       "  0.08370379077849673,\n",
       "  0.08824155051803151,\n",
       "  0.08228848907657275,\n",
       "  0.0873040962182727,\n",
       "  0.08607081325003053,\n",
       "  0.08124494128168816,\n",
       "  0.08391421819376726,\n",
       "  0.08209634374878491,\n",
       "  0.0819231795338175,\n",
       "  0.08157867347253957,\n",
       "  0.08036590475948445,\n",
       "  0.08279441069999843,\n",
       "  0.08221441187865884,\n",
       "  0.08053186883833288,\n",
       "  0.0840010853059259,\n",
       "  0.08238240480879365,\n",
       "  0.0806709033583974,\n",
       "  0.08161571049124322,\n",
       "  0.0807755802049206,\n",
       "  0.07997155103940876,\n",
       "  0.07947835015848986,\n",
       "  0.07965301612754327,\n",
       "  0.07975847844553578,\n",
       "  0.07921652710364752,\n",
       "  0.07914821889713566,\n",
       "  0.07883499310584383,\n",
       "  0.07885284384122121,\n",
       "  0.08000215497809202,\n",
       "  0.07901638506656401,\n",
       "  0.07887325199694582,\n",
       "  0.07959129599708875,\n",
       "  0.07856305192294566,\n",
       "  0.07927092105424058,\n",
       "  0.07866096764711654,\n",
       "  0.07856523891366093,\n",
       "  0.078463000597669,\n",
       "  0.07752110636800208,\n",
       "  0.08001064765873218,\n",
       "  0.07983386623822894,\n",
       "  0.0776014259715449,\n",
       "  0.07795362200753429,\n",
       "  0.07753090380161863,\n",
       "  0.07764858042801322,\n",
       "  0.07731261282337752,\n",
       "  0.07745248687440733,\n",
       "  0.0768677357635582,\n",
       "  0.0775441245317094,\n",
       "  0.07717649354549498,\n",
       "  0.07673389805768935,\n",
       "  0.07719185342513037,\n",
       "  0.07635465524463157,\n",
       "  0.07724336670514818,\n",
       "  0.07821807986379217,\n",
       "  0.07664206988002768,\n",
       "  0.0769751914997926,\n",
       "  0.07769687467498582,\n",
       "  0.07611489367841032,\n",
       "  0.07843054251411608,\n",
       "  0.07799697028413119,\n",
       "  0.0756325307566028,\n",
       "  0.07674952831467297,\n",
       "  0.07578343130180336,\n",
       "  0.07591818672409467,\n",
       "  0.07622119687372109,\n",
       "  0.07471158046269673,\n",
       "  0.0774446327986615,\n",
       "  0.07719046086255478,\n",
       "  0.07474478201835115,\n",
       "  0.07559351715477466,\n",
       "  0.07535766454193896,\n",
       "  0.07459056221097023,\n",
       "  0.07526251960083682,\n",
       "  0.07484788096197531,\n",
       "  0.07426813676108993,\n",
       "  0.0747057782517458,\n",
       "  0.07451294331738631,\n",
       "  0.0737797849530648,\n",
       "  0.07467826608722462,\n",
       "  0.0748197744054232,\n",
       "  0.0731304064221897,\n",
       "  0.07638700287018947,\n",
       "  0.07625615510627468,\n",
       "  0.07309403890033593,\n",
       "  0.07760995318385762,\n",
       "  0.07420037876227001,\n",
       "  0.07466766568525462,\n",
       "  0.07856696733426172,\n",
       "  0.07504159630280541,\n",
       "  0.07424689993650223,\n",
       "  0.07394144902002939,\n",
       "  0.07357657974041074,\n",
       "  0.07305496212260464,\n",
       "  0.07254135244130906,\n",
       "  0.07257161325964774,\n",
       "  0.07255012744532792,\n",
       "  0.07228978131120457,\n",
       "  0.07247949835443569,\n",
       "  0.07224775602039846,\n",
       "  0.07229385872986197,\n",
       "  0.07267490259812401,\n",
       "  0.07256611450297179,\n",
       "  0.07302977562315796,\n",
       "  0.07246984348591035,\n",
       "  0.07264206055295047,\n",
       "  0.07192550428428475,\n",
       "  0.07211139996694386,\n",
       "  0.07180920149703121,\n",
       "  0.07182703849504407,\n",
       "  0.07138159947546846,\n",
       "  0.07147085679207242,\n",
       "  0.07116108133270765,\n",
       "  0.07063551306450713,\n",
       "  0.07211113570523299,\n",
       "  0.07143596073706701,\n",
       "  0.07103255057955592,\n",
       "  0.07337181477780363,\n",
       "  0.07151967444518441,\n",
       "  0.0742122241585397,\n",
       "  0.07104530969312325,\n",
       "  0.07107045437575847,\n",
       "  0.07382159837012452,\n",
       "  0.07215287758873214,\n",
       "  0.07002293869602005,\n",
       "  0.07013214060360677,\n",
       "  0.06982352089416378,\n",
       "  0.06972705824999495,\n",
       "  0.06986734390852097,\n",
       "  0.06960386665363955,\n",
       "  0.06958296666831737,\n",
       "  0.06931840937019855,\n",
       "  0.06936737335933113,\n",
       "  0.06922627668285808,\n",
       "  0.06945860032191868,\n",
       "  0.06975807945542095,\n",
       "  0.06917910860882784,\n",
       "  0.06887895294435174,\n",
       "  0.06939870406621083,\n",
       "  0.06872223998215991,\n",
       "  0.06967369319263497,\n",
       "  0.07050800090726995,\n",
       "  0.06862918201942356,\n",
       "  0.06939173966166984,\n",
       "  0.06811302568860478,\n",
       "  0.06934354251185018,\n",
       "  0.07142959116383314,\n",
       "  0.0697906140471947,\n",
       "  0.06924367191164453,\n",
       "  0.06813777944395408,\n",
       "  0.06857402071607788,\n",
       "  0.0685419862773298,\n",
       "  0.06733246877857955,\n",
       "  0.06993687764426286,\n",
       "  0.07024828496333468,\n",
       "  0.0675229110610978,\n",
       "  0.06748677237932021,\n",
       "  0.0674197598721313,\n",
       "  0.06751580052591209,\n",
       "  0.06750049932673002,\n",
       "  0.06769640445937423,\n",
       "  0.0675336831173707,\n",
       "  0.06839710586971016,\n",
       "  0.06947213363410137,\n",
       "  0.06789265120056832,\n",
       "  0.06716897518903155,\n",
       "  0.0670845466758628,\n",
       "  0.06891976158843471,\n",
       "  0.06789648112987852,\n",
       "  0.06667119657098791,\n",
       "  0.06636442926357206,\n",
       "  0.06673786775487123,\n",
       "  0.06702914255547122,\n",
       "  0.06657718422082445,\n",
       "  0.06591353486156573,\n",
       "  0.06641253023757777,\n",
       "  0.06663514029057796,\n",
       "  0.0653714724398499,\n",
       "  0.07139183238582261,\n",
       "  0.06776615582509209,\n",
       "  0.06711410743627942,\n",
       "  0.06827692583682526,\n",
       "  0.06578132646532008,\n",
       "  0.06836951217096396,\n",
       "  0.06653385671571604,\n",
       "  0.06735374536621991,\n",
       "  0.06660491703891097,\n",
       "  0.06576099465807948,\n",
       "  0.06542917724362193,\n",
       "  0.06516676096054533,\n",
       "  0.06577297071904903,\n",
       "  0.06470980182586732,\n",
       "  0.06496701405524842,\n",
       "  0.06714899118014171,\n",
       "  0.06489784469625486,\n",
       "  0.06598233840047492,\n",
       "  0.06680205898504893,\n",
       "  0.06484504854149695,\n",
       "  0.06506544138785526,\n",
       "  0.0650591200722487,\n",
       "  0.06424992949135636,\n",
       "  0.06407560749267546,\n",
       "  0.06458360359506257,\n",
       "  0.06416610588432271,\n",
       "  0.06408802145495897,\n",
       "  0.06411407902768342,\n",
       "  0.06401374576125539,\n",
       "  0.06543081028994521,\n",
       "  0.06482729114945779,\n",
       "  0.06334190403794439,\n",
       "  0.06712940726058407,\n",
       "  0.06405539416449905,\n",
       "  0.06513829756339878,\n",
       "  0.06652993112117919,\n",
       "  0.06469036338089254,\n",
       "  0.06386596488140261,\n",
       "  0.06285419482823353,\n",
       "  0.06455395220112838,\n",
       "  0.06509373873289657,\n",
       "  0.06284178954308102,\n",
       "  0.06337321332868354,\n",
       "  0.06302634112616776,\n",
       "  0.06258871344703992,\n",
       "  0.0639846867512324,\n",
       "  0.06366151136846765,\n",
       "  0.06211705736325309,\n",
       "  0.06384246652035946,\n",
       "  0.06335391930288779,\n",
       "  0.06221100764426119,\n",
       "  0.06311571849067946,\n",
       "  0.061749494618880255,\n",
       "  0.06382512546801913,\n",
       "  0.06326306206252637,\n",
       "  0.06186589514223394,\n",
       "  0.061734251318582165,\n",
       "  0.061825683155021474,\n",
       "  0.061696754411935444,\n",
       "  0.0621252965530064,\n",
       "  0.061427875235106275,\n",
       "  0.06402062638384642,\n",
       "  0.06170830922415209,\n",
       "  0.06225574187717613,\n",
       "  0.06273358365432409,\n",
       "  0.061643261416810945,\n",
       "  0.061394528512932804,\n",
       "  0.06095966246623723,\n",
       "  0.061311831712083834,\n",
       "  0.06161256321059799,\n",
       "  0.06190484017133713,\n",
       "  0.06126509169273501,\n",
       "  0.06295624809770362,\n",
       "  0.0613817702151641,\n",
       "  0.06199988595011771,\n",
       "  0.06357094403042544,\n",
       "  0.0602971696242074,\n",
       "  0.062259306529305794,\n",
       "  0.06446099427070863,\n",
       "  0.060555822599627884,\n",
       "  0.0618913204253724,\n",
       "  0.06244131281492721,\n",
       "  0.061605841286514654,\n",
       "  0.06248048634751833,\n",
       "  0.06144921100522986,\n",
       "  0.0608550737544917,\n",
       "  0.060872351719354595,\n",
       "  0.05981567580749211,\n",
       "  0.06186511991673918,\n",
       "  0.060968307918008946,\n",
       "  0.05969127279579366,\n",
       "  0.06016688842389156,\n",
       "  0.0598732542252285,\n",
       "  0.059581933249284444,\n",
       "  0.05954451843023665,\n",
       "  0.05963372825937286,\n",
       "  0.0591316527956569,\n",
       "  0.05908734095817686,\n",
       "  0.059364635908083385,\n",
       "  0.05936918662116502,\n",
       "  0.05904766538424295,\n",
       "  0.058964509956230614,\n",
       "  0.05908792065442248,\n",
       "  0.05924965031045684,\n",
       "  0.05896421244644644,\n",
       "  0.058574257832117504,\n",
       "  0.05841866018076586,\n",
       "  0.05847270040563201,\n",
       "  0.05891523430349268,\n",
       "  0.05978239353911617,\n",
       "  0.05868787288802689,\n",
       "  0.05945127742054086,\n",
       "  0.058116690522404944,\n",
       "  0.05861866802753022,\n",
       "  0.06101358296837412,\n",
       "  0.05981622520509029,\n",
       "  0.058178371300558585,\n",
       "  0.05812121780552141,\n",
       "  0.05766033121080895,\n",
       "  0.0576175549745012,\n",
       "  0.05799635302326763,\n",
       "  0.05781927344262326,\n",
       "  0.05782939355597927,\n",
       "  0.05724082481555333,\n",
       "  0.05752904285572663,\n",
       "  0.05784544687586941,\n",
       "  0.058028904655534674,\n",
       "  0.057531179954364874,\n",
       "  0.058939532563660815,\n",
       "  0.057212367345782916,\n",
       "  0.05709276732861722,\n",
       "  0.05894028150104272,\n",
       "  0.05603100425046034,\n",
       "  0.06198815976313756,\n",
       "  0.06070317350225281,\n",
       "  0.05804527501873729,\n",
       "  0.058974065234567996,\n",
       "  0.0568500447442119,\n",
       "  0.057005208413505995,\n",
       "  0.056170008192338035,\n",
       "  0.05823792091600745,\n",
       "  0.05816102904163495,\n",
       "  0.05723456647617894,\n",
       "  0.05667627731746043,\n",
       "  0.060141260851214524,\n",
       "  0.05805396403301547,\n",
       "  0.05656974956644257,\n",
       "  0.05593787255778247,\n",
       "  0.05669149223595264,\n",
       "  0.05591789569140941,\n",
       "  0.05614417506692786,\n",
       "  0.057837054749542864,\n",
       "  0.05540952737376117,\n",
       "  0.05815823298592297,\n",
       "  0.05840392418171509,\n",
       "  0.05753679230672294,\n",
       "  0.05840580416971655,\n",
       "  0.05553923252895817,\n",
       "  0.057498266044747406,\n",
       "  0.057168720829176504,\n",
       "  0.055663093776195376,\n",
       "  0.055515584828272166,\n",
       "  0.05668187225850764,\n",
       "  0.055439977560072545,\n",
       "  0.05550403235220617,\n",
       "  0.05500731697080694,\n",
       "  0.054892455739686534,\n",
       "  0.05564016205291471,\n",
       "  0.0557727257037966,\n",
       "  0.05611493613051784,\n",
       "  0.05708373444094957,\n",
       "  0.05507043323509543,\n",
       "  0.055091318193748934,\n",
       "  0.055486189746290764,\n",
       "  0.055559301403726224,\n",
       "  0.05451105034141409,\n",
       "  0.05602871515940744,\n",
       "  0.05525449355336828,\n",
       "  0.05482303619863976,\n",
       "  0.056372445373358074,\n",
       "  0.05354903490606348,\n",
       "  0.05643048909297398,\n",
       "  0.05720425763981813,\n",
       "  0.05509297375440324,\n",
       "  0.054548438986446734,\n",
       "  0.05435853185642733,\n",
       "  0.053895655774139156,\n",
       "  0.05386309750709015,\n",
       "  0.054591741361534066,\n",
       "  0.05427856782128946,\n",
       "  0.05400033247804021,\n",
       "  0.053895274208570515,\n",
       "  0.054640561821449773,\n",
       "  0.05324933944329916,\n",
       "  0.05439285913433817,\n",
       "  0.05404180998002588,\n",
       "  0.055209011318672674,\n",
       "  0.05512401942523114,\n",
       "  0.05389430557033917,\n",
       "  0.05269673185431574,\n",
       "  0.054746964863668354,\n",
       "  0.0542969001288801,\n",
       "  0.052851687152663196,\n",
       "  0.05612750353140451,\n",
       "  0.052577721043439954,\n",
       "  0.05578196214178255,\n",
       "  0.055531898636091356,\n",
       "  0.05207959187158582,\n",
       "  0.05621554424121953,\n",
       "  0.054119454528480354,\n",
       "  0.052787237879099926,\n",
       "  0.05467111542592735,\n",
       "  0.05177035976834538,\n",
       "  0.05582927745891019,\n",
       "  0.05369341101463183,\n",
       "  0.05349120372196069,\n",
       "  0.056960577781313596,\n",
       "  0.05483825850495883,\n",
       "  0.05687737948679084,\n",
       "  0.052522017142719736,\n",
       "  0.057945623215156714,\n",
       "  0.05698422108638524,\n",
       "  0.05267642300353481,\n",
       "  0.05158176087128258,\n",
       "  0.051492375057835306,\n",
       "  0.051366842590032824,\n",
       "  0.05128460903159966,\n",
       "  0.0512072144255932,\n",
       "  0.051351029060746775,\n",
       "  0.0515030636687465,\n",
       "  0.0512121597541693,\n",
       "  0.05076745523291014,\n",
       "  0.05264455247755985,\n",
       "  0.053028194367839954,\n",
       "  0.050826876271647295,\n",
       "  0.05204941676059598,\n",
       "  0.052045945882523405,\n",
       "  0.05075398802894181,\n",
       "  0.05089736420655506,\n",
       "  0.05136301398140365,\n",
       "  0.050704972473348264,\n",
       "  0.051002591951061355,\n",
       "  0.05073740498760393,\n",
       "  0.05030572782933803,\n",
       "  0.051105298404260945,\n",
       "  0.051000121518809796,\n",
       "  0.050415345083973495,\n",
       "  0.05005538795616871,\n",
       "  0.05002215894289988,\n",
       "  0.04991784180652858,\n",
       "  0.050087564594895335,\n",
       "  0.04980029922212612,\n",
       "  0.05056606400843601,\n",
       "  0.049929896594919435,\n",
       "  0.0493831988908978,\n",
       "  0.05238706308201663,\n",
       "  0.05016554469045873,\n",
       "  0.051477798989547525,\n",
       "  0.05098422403357478,\n",
       "  0.049936858797374584,\n",
       "  0.05110488623198105,\n",
       "  0.04982340152148631,\n",
       "  0.0492464108578005,\n",
       "  0.049363428798103404,\n",
       "  0.04932719880963175,\n",
       "  0.049538634951364756,\n",
       "  0.05027574110222812,\n",
       "  0.04860507332324799,\n",
       "  0.049991267358179665,\n",
       "  0.05121739102016912,\n",
       "  0.048899560767421316,\n",
       "  0.04880819361297496,\n",
       "  0.049173811153946576,\n",
       "  0.049201255872457716,\n",
       "  0.048356658538053265,\n",
       "  0.04830602767547824,\n",
       "  0.04828939630234953,\n",
       "  0.04901389253951872,\n",
       "  0.0481807740025827,\n",
       "  0.04917626190719524,\n",
       "  0.04956936450592409,\n",
       "  0.04881808876557701,\n",
       "  0.048446499381733524,\n",
       "  0.0505867286615222,\n",
       "  0.0487255131338037,\n",
       "  0.0482082005710003,\n",
       "  0.04783278683151987,\n",
       "  0.04856979963334012,\n",
       "  0.04882558962903563,\n",
       "  0.04821495410585477,\n",
       "  0.04857544153700561,\n",
       "  0.04735369166818549,\n",
       "  0.05012729032047857,\n",
       "  0.04858344929095979,\n",
       "  0.05003634671385401,\n",
       "  0.04773761166022893,\n",
       "  0.05013602870483676,\n",
       "  0.05025475535536068,\n",
       "  0.047185300873623874,\n",
       "  0.05020952981199276,\n",
       "  0.048716087362074376,\n",
       "  0.04755441115949457,\n",
       "  0.04799567443100453,\n",
       "  0.04676050446345649,\n",
       "  0.04985183236085072,\n",
       "  0.04794424413995758,\n",
       "  0.04691692779431117,\n",
       "  0.04962721751691919,\n",
       "  0.04761894681643564,\n",
       "  0.0494392286717983,\n",
       "  0.04792742868158025,\n",
       "  0.047953211991929354,\n",
       "  0.04609499737095504,\n",
       "  0.0499862730217838,\n",
       "  0.04826198893087199,\n",
       "  0.047102163821048065,\n",
       "  0.04831881401275968,\n",
       "  0.04764088533054267,\n",
       "  0.04884406467856894,\n",
       "  0.04753595454244665,\n",
       "  0.04814905129019371,\n",
       "  0.0468973553758029,\n",
       "  0.046327084126050765,\n",
       "  0.04579907224711835,\n",
       "  0.047792537115046475,\n",
       "  0.046982393376129875,\n",
       "  0.046207192369592684,\n",
       "  0.048397314519397515,\n",
       "  0.04584254914891848,\n",
       "  0.04728023588064071,\n",
       "  ...],\n",
       " 'accuracy': [0.32159266,\n",
       "  0.4287902,\n",
       "  0.5788668,\n",
       "  0.7411945,\n",
       "  0.7825421,\n",
       "  0.76875955,\n",
       "  0.76875955,\n",
       "  0.7565084,\n",
       "  0.7565084,\n",
       "  0.7565084,\n",
       "  0.7595712,\n",
       "  0.76875955,\n",
       "  0.7748851,\n",
       "  0.7901991,\n",
       "  0.79938745,\n",
       "  0.8085758,\n",
       "  0.8116386,\n",
       "  0.82082695,\n",
       "  0.8300153,\n",
       "  0.84379786,\n",
       "  0.84992343,\n",
       "  0.856049,\n",
       "  0.8606432,\n",
       "  0.8667688,\n",
       "  0.8667688,\n",
       "  0.8667688,\n",
       "  0.87595713,\n",
       "  0.8805513,\n",
       "  0.8897397,\n",
       "  0.8973966,\n",
       "  0.9035222,\n",
       "  0.91424197,\n",
       "  0.91730475,\n",
       "  0.92036754,\n",
       "  0.9218989,\n",
       "  0.9249617,\n",
       "  0.92802453,\n",
       "  0.92802453,\n",
       "  0.9310873,\n",
       "  0.9341501,\n",
       "  0.93568146,\n",
       "  0.9372129,\n",
       "  0.93874425,\n",
       "  0.93874425,\n",
       "  0.94027567,\n",
       "  0.93874425,\n",
       "  0.94027567,\n",
       "  0.94180703,\n",
       "  0.9372129,\n",
       "  0.9372129,\n",
       "  0.93874425,\n",
       "  0.93874425,\n",
       "  0.93874425,\n",
       "  0.93874425,\n",
       "  0.9341501,\n",
       "  0.9326187,\n",
       "  0.9341501,\n",
       "  0.93568146,\n",
       "  0.9372129,\n",
       "  0.94027567,\n",
       "  0.9372129,\n",
       "  0.93874425,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.93874425,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.93874425,\n",
       "  0.93874425,\n",
       "  0.94027567,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.94180703,\n",
       "  0.94027567,\n",
       "  0.94180703,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94027567,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94333845,\n",
       "  0.94180703,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.9372129,\n",
       "  0.9372129,\n",
       "  0.93874425,\n",
       "  0.93874425,\n",
       "  0.94027567,\n",
       "  0.94180703,\n",
       "  0.94640124,\n",
       "  0.9448698,\n",
       "  0.949464,\n",
       "  0.949464,\n",
       "  0.9448698,\n",
       "  0.9448698,\n",
       "  0.94180703,\n",
       "  0.94027567,\n",
       "  0.94180703,\n",
       "  0.94640124,\n",
       "  0.94640124,\n",
       "  0.94640124,\n",
       "  0.949464,\n",
       "  0.949464,\n",
       "  0.949464,\n",
       "  0.94640124,\n",
       "  0.94640124,\n",
       "  0.94180703,\n",
       "  0.93874425,\n",
       "  0.93874425,\n",
       "  0.93874425,\n",
       "  0.94027567,\n",
       "  0.94027567,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.94180703,\n",
       "  0.9448698,\n",
       "  0.9448698,\n",
       "  0.94640124,\n",
       "  0.9448698,\n",
       "  0.9448698,\n",
       "  0.94333845,\n",
       "  0.9479326,\n",
       "  0.9448698,\n",
       "  0.9448698,\n",
       "  0.94640124,\n",
       "  0.94333845,\n",
       "  0.94640124,\n",
       "  0.949464,\n",
       "  0.9509954,\n",
       "  0.949464,\n",
       "  0.94640124,\n",
       "  0.9448698,\n",
       "  0.9448698,\n",
       "  0.94640124,\n",
       "  0.94640124,\n",
       "  0.949464,\n",
       "  0.9509954,\n",
       "  0.9509954,\n",
       "  0.9509954,\n",
       "  0.9525268,\n",
       "  0.9525268,\n",
       "  0.9525268,\n",
       "  0.9525268,\n",
       "  0.9525268,\n",
       "  0.9525268,\n",
       "  0.9509954,\n",
       "  0.9479326,\n",
       "  0.9509954,\n",
       "  0.9525268,\n",
       "  0.94640124,\n",
       "  0.94640124,\n",
       "  0.94640124,\n",
       "  0.9509954,\n",
       "  0.9525268,\n",
       "  0.9525268,\n",
       "  0.9540582,\n",
       "  0.9525268,\n",
       "  0.9448698,\n",
       "  0.94640124,\n",
       "  0.9448698,\n",
       "  0.9448698,\n",
       "  0.9509954,\n",
       "  0.9555896,\n",
       "  0.9555896,\n",
       "  0.9540582,\n",
       "  0.9555896,\n",
       "  0.9509954,\n",
       "  0.9555896,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.9525268,\n",
       "  0.9509954,\n",
       "  0.949464,\n",
       "  0.9509954,\n",
       "  0.9555896,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.95712095,\n",
       "  0.9509954,\n",
       "  0.9525268,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.95712095,\n",
       "  0.95712095,\n",
       "  0.9540582,\n",
       "  0.9540582,\n",
       "  0.9555896,\n",
       "  0.9555896,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.95712095,\n",
       "  0.95712095,\n",
       "  0.9555896,\n",
       "  0.9555896,\n",
       "  0.95712095,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.9586524,\n",
       "  0.95712095,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.9586524,\n",
       "  0.95712095,\n",
       "  0.9586524,\n",
       "  0.96018374,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.9586524,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.96018374,\n",
       "  0.96018374,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.96018374,\n",
       "  0.9586524,\n",
       "  0.95712095,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.96018374,\n",
       "  0.9586524,\n",
       "  0.96018374,\n",
       "  0.96171516,\n",
       "  0.96018374,\n",
       "  0.9586524,\n",
       "  0.9586524,\n",
       "  0.95712095,\n",
       "  0.9586524,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.96018374,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96018374,\n",
       "  0.9586524,\n",
       "  0.96171516,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.9632466,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.9632466,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.96171516,\n",
       "  0.96630937,\n",
       "  0.96018374,\n",
       "  0.96171516,\n",
       "  0.9632466,\n",
       "  0.96477795,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.9632466,\n",
       "  0.9632466,\n",
       "  0.9632466,\n",
       "  0.96171516,\n",
       "  0.9632466,\n",
       "  0.96477795,\n",
       "  0.9632466,\n",
       "  0.96477795,\n",
       "  0.96171516,\n",
       "  0.96171516,\n",
       "  0.9632466,\n",
       "  0.96630937,\n",
       "  0.96477795,\n",
       "  0.96630937,\n",
       "  0.9632466,\n",
       "  0.9632466,\n",
       "  0.9632466,\n",
       "  0.96630937,\n",
       "  0.96171516,\n",
       "  0.96630937,\n",
       "  0.9632466,\n",
       "  0.9632466,\n",
       "  0.96630937,\n",
       "  0.9632466,\n",
       "  0.9632466,\n",
       "  0.96477795,\n",
       "  0.9632466,\n",
       "  0.9632466,\n",
       "  0.96477795,\n",
       "  0.96630937,\n",
       "  0.96477795,\n",
       "  0.96630937,\n",
       "  0.9632466,\n",
       "  0.9632466,\n",
       "  0.96477795,\n",
       "  0.96630937,\n",
       "  0.9632466,\n",
       "  0.9632466,\n",
       "  0.96477795,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96477795,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.9632466,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.96630937,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.96477795,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96630937,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96630937,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96630937,\n",
       "  0.96784073,\n",
       "  0.96630937,\n",
       "  0.96630937,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96937215,\n",
       "  0.96784073,\n",
       "  0.96937215,\n",
       "  0.96784073,\n",
       "  0.96630937,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96937215,\n",
       "  0.96784073,\n",
       "  0.96630937,\n",
       "  0.96937215,\n",
       "  0.9709035,\n",
       "  0.97243494,\n",
       "  0.97243494,\n",
       "  0.96784073,\n",
       "  0.96630937,\n",
       "  0.96784073,\n",
       "  0.97243494,\n",
       "  0.97243494,\n",
       "  0.9709035,\n",
       "  0.96784073,\n",
       "  0.96784073,\n",
       "  0.96937215,\n",
       "  0.9739663,\n",
       "  0.97243494,\n",
       "  0.97243494,\n",
       "  0.96937215,\n",
       "  0.96784073,\n",
       "  0.96937215,\n",
       "  0.97243494,\n",
       "  0.97243494,\n",
       "  0.97243494,\n",
       "  0.9709035,\n",
       "  0.96937215,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.97243494,\n",
       "  0.97243494,\n",
       "  0.96937215,\n",
       "  0.96937215,\n",
       "  0.9739663,\n",
       "  0.97243494,\n",
       "  0.96937215,\n",
       "  0.9709035,\n",
       "  0.9739663,\n",
       "  0.9709035,\n",
       "  0.96937215,\n",
       "  0.9709035,\n",
       "  0.9739663,\n",
       "  0.97243494,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9709035,\n",
       "  0.96937215,\n",
       "  0.9709035,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9709035,\n",
       "  0.9709035,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.9709035,\n",
       "  0.9709035,\n",
       "  0.9739663,\n",
       "  0.97243494,\n",
       "  0.9739663,\n",
       "  0.97243494,\n",
       "  0.9709035,\n",
       "  0.96937215,\n",
       "  0.97243494,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9709035,\n",
       "  0.96630937,\n",
       "  0.97243494,\n",
       "  0.96937215,\n",
       "  0.9754977,\n",
       "  0.96937215,\n",
       "  0.96937215,\n",
       "  0.9754977,\n",
       "  0.97243494,\n",
       "  0.9739663,\n",
       "  0.9709035,\n",
       "  0.9709035,\n",
       "  0.97243494,\n",
       "  0.9709035,\n",
       "  0.97243494,\n",
       "  0.9739663,\n",
       "  0.9709035,\n",
       "  0.97243494,\n",
       "  0.9754977,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.97243494,\n",
       "  0.9709035,\n",
       "  0.9770291,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.9709035,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9770291,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.97243494,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9770291,\n",
       "  0.9754977,\n",
       "  0.9770291,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9709035,\n",
       "  0.9754977,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9770291,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9770291,\n",
       "  0.97243494,\n",
       "  0.9739663,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9739663,\n",
       "  0.97243494,\n",
       "  0.9770291,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9739663,\n",
       "  0.97243494,\n",
       "  0.9754977,\n",
       "  0.9770291,\n",
       "  0.9754977,\n",
       "  0.9754977,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9754977,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9739663,\n",
       "  0.9754977,\n",
       "  0.9770291,\n",
       "  0.9754977,\n",
       "  0.9785605,\n",
       "  0.9739663,\n",
       "  0.9739663,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9754977,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9739663,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9754977,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9816233,\n",
       "  0.9770291,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9754977,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9816233,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.98009187,\n",
       "  0.98315465,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9770291,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9770291,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9770291,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98315465,\n",
       "  0.9816233,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.9846861,\n",
       "  0.98009187,\n",
       "  0.9770291,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9816233,\n",
       "  0.98315465,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9816233,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.98009187,\n",
       "  0.9770291,\n",
       "  0.9770291,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9785605,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.98315465,\n",
       "  0.9785605,\n",
       "  0.9785605,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.9846861,\n",
       "  0.9816233,\n",
       "  0.9785605,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9846861,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.9770291,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9846861,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9785605,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.98315465,\n",
       "  0.9816233,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.98009187,\n",
       "  0.98009187,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.9816233,\n",
       "  0.98009187,\n",
       "  0.9816233,\n",
       "  0.9846861,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.98009187,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.98315465,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.9816233,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.9816233,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.9816233,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.98621744,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.9816233,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.98621744,\n",
       "  0.98621744,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.98621744,\n",
       "  0.9846861,\n",
       "  0.98621744,\n",
       "  0.98621744,\n",
       "  0.9846861,\n",
       "  0.98315465,\n",
       "  0.9846861,\n",
       "  0.98621744,\n",
       "  0.98315465,\n",
       "  0.98315465,\n",
       "  0.98621744,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.98621744,\n",
       "  0.98621744,\n",
       "  0.9846861,\n",
       "  0.9846861,\n",
       "  0.9816233,\n",
       "  0.98621744,\n",
       "  ...]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x6472657d0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_vloss = history.history['val_loss']\n",
    "y_acc = history.history['accuracy']\n",
    "\n",
    "x_len = np.arange(len(y_acc))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
    "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 자동중단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38145, saving model to ./model/01-0.3815.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38145 to 0.36604, saving model to ./model/02-0.3660.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.36604\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.36604\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.36604\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.36604 to 0.35353, saving model to ./model/06-0.3535.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35353 to 0.33802, saving model to ./model/07-0.3380.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.33802 to 0.32787, saving model to ./model/08-0.3279.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.32787 to 0.32499, saving model to ./model/09-0.3250.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.32499 to 0.32420, saving model to ./model/10-0.3242.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.32420 to 0.31804, saving model to ./model/11-0.3180.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.31804 to 0.30628, saving model to ./model/12-0.3063.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.30628 to 0.29438, saving model to ./model/13-0.2944.hdf5\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.29438 to 0.28377, saving model to ./model/14-0.2838.hdf5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.28377 to 0.27237, saving model to ./model/15-0.2724.hdf5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.27237 to 0.25962, saving model to ./model/16-0.2596.hdf5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.25962 to 0.24898, saving model to ./model/17-0.2490.hdf5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.24898 to 0.24265, saving model to ./model/18-0.2427.hdf5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.24265 to 0.23827, saving model to ./model/19-0.2383.hdf5\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.23827 to 0.23236, saving model to ./model/20-0.2324.hdf5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.23236 to 0.22769, saving model to ./model/21-0.2277.hdf5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.22769 to 0.22479, saving model to ./model/22-0.2248.hdf5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.22479 to 0.22171, saving model to ./model/23-0.2217.hdf5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.22171 to 0.21830, saving model to ./model/24-0.2183.hdf5\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.21830 to 0.21672, saving model to ./model/25-0.2167.hdf5\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.21672 to 0.21586, saving model to ./model/26-0.2159.hdf5\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.21586 to 0.21442, saving model to ./model/27-0.2144.hdf5\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.21442 to 0.21211, saving model to ./model/28-0.2121.hdf5\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.21211 to 0.21084, saving model to ./model/29-0.2108.hdf5\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.21084 to 0.21011, saving model to ./model/30-0.2101.hdf5\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.21011 to 0.20917, saving model to ./model/31-0.2092.hdf5\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.20917 to 0.20806, saving model to ./model/32-0.2081.hdf5\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.20806 to 0.20711, saving model to ./model/33-0.2071.hdf5\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.20711 to 0.20594, saving model to ./model/34-0.2059.hdf5\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.20594 to 0.20434, saving model to ./model/35-0.2043.hdf5\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.20434 to 0.20280, saving model to ./model/36-0.2028.hdf5\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.20280 to 0.20117, saving model to ./model/37-0.2012.hdf5\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.20117 to 0.19984, saving model to ./model/38-0.1998.hdf5\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.19984 to 0.19834, saving model to ./model/39-0.1983.hdf5\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.19834 to 0.19699, saving model to ./model/40-0.1970.hdf5\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.19699 to 0.19593, saving model to ./model/41-0.1959.hdf5\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.19593 to 0.19520, saving model to ./model/42-0.1952.hdf5\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.19520 to 0.19435, saving model to ./model/43-0.1943.hdf5\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.19435 to 0.19347, saving model to ./model/44-0.1935.hdf5\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.19347 to 0.19287, saving model to ./model/45-0.1929.hdf5\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.19287 to 0.19232, saving model to ./model/46-0.1923.hdf5\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.19232 to 0.19174, saving model to ./model/47-0.1917.hdf5\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.19174 to 0.19124, saving model to ./model/48-0.1912.hdf5\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.19124 to 0.19065, saving model to ./model/49-0.1906.hdf5\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.19065 to 0.19016, saving model to ./model/50-0.1902.hdf5\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.19016 to 0.18923, saving model to ./model/51-0.1892.hdf5\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.18923 to 0.18821, saving model to ./model/52-0.1882.hdf5\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.18821 to 0.18696, saving model to ./model/53-0.1870.hdf5\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.18696 to 0.18560, saving model to ./model/54-0.1856.hdf5\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.18560 to 0.18450, saving model to ./model/55-0.1845.hdf5\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.18450 to 0.18371, saving model to ./model/56-0.1837.hdf5\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.18371 to 0.18311, saving model to ./model/57-0.1831.hdf5\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.18311 to 0.18268, saving model to ./model/58-0.1827.hdf5\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.18268 to 0.18229, saving model to ./model/59-0.1823.hdf5\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.18229\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.18229 to 0.18206, saving model to ./model/61-0.1821.hdf5\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.18206 to 0.18171, saving model to ./model/62-0.1817.hdf5\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.18171 to 0.18107, saving model to ./model/63-0.1811.hdf5\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.18107 to 0.18041, saving model to ./model/64-0.1804.hdf5\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.18041 to 0.17958, saving model to ./model/65-0.1796.hdf5\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.17958 to 0.17872, saving model to ./model/66-0.1787.hdf5\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.17872 to 0.17789, saving model to ./model/67-0.1779.hdf5\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.17789 to 0.17678, saving model to ./model/68-0.1768.hdf5\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.17678 to 0.17568, saving model to ./model/69-0.1757.hdf5\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.17568 to 0.17468, saving model to ./model/70-0.1747.hdf5\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.17468 to 0.17362, saving model to ./model/71-0.1736.hdf5\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.17362 to 0.17256, saving model to ./model/72-0.1726.hdf5\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.17256 to 0.17145, saving model to ./model/73-0.1715.hdf5\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.17145 to 0.17095, saving model to ./model/74-0.1710.hdf5\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.17095\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.17095 to 0.17070, saving model to ./model/76-0.1707.hdf5\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.17070 to 0.17031, saving model to ./model/77-0.1703.hdf5\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.17031 to 0.16929, saving model to ./model/78-0.1693.hdf5\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.16929 to 0.16783, saving model to ./model/79-0.1678.hdf5\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.16783 to 0.16629, saving model to ./model/80-0.1663.hdf5\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.16629 to 0.16516, saving model to ./model/81-0.1652.hdf5\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.16516 to 0.16436, saving model to ./model/82-0.1644.hdf5\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.16436 to 0.16397, saving model to ./model/83-0.1640.hdf5\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.16397\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.16397\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.16397 to 0.16353, saving model to ./model/86-0.1635.hdf5\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.16353 to 0.16326, saving model to ./model/87-0.1633.hdf5\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.16326 to 0.16221, saving model to ./model/88-0.1622.hdf5\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.16221 to 0.16111, saving model to ./model/89-0.1611.hdf5\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.16111 to 0.16010, saving model to ./model/90-0.1601.hdf5\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.16010 to 0.15871, saving model to ./model/91-0.1587.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00092: val_loss improved from 0.15871 to 0.15759, saving model to ./model/92-0.1576.hdf5\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.15759 to 0.15744, saving model to ./model/93-0.1574.hdf5\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.15744 to 0.15585, saving model to ./model/94-0.1559.hdf5\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.15585 to 0.15449, saving model to ./model/95-0.1545.hdf5\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.15449 to 0.15367, saving model to ./model/96-0.1537.hdf5\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.15367 to 0.15275, saving model to ./model/97-0.1528.hdf5\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.15275 to 0.15216, saving model to ./model/98-0.1522.hdf5\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.15216 to 0.15115, saving model to ./model/99-0.1512.hdf5\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.15115 to 0.15015, saving model to ./model/100-0.1501.hdf5\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.15015 to 0.14959, saving model to ./model/101-0.1496.hdf5\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.14959 to 0.14912, saving model to ./model/102-0.1491.hdf5\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.14912 to 0.14837, saving model to ./model/103-0.1484.hdf5\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.14837 to 0.14756, saving model to ./model/104-0.1476.hdf5\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.14756 to 0.14708, saving model to ./model/105-0.1471.hdf5\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.14708 to 0.14641, saving model to ./model/106-0.1464.hdf5\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.14641\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.14641 to 0.14547, saving model to ./model/108-0.1455.hdf5\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.14547 to 0.14416, saving model to ./model/109-0.1442.hdf5\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.14416 to 0.14372, saving model to ./model/110-0.1437.hdf5\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.14372 to 0.14337, saving model to ./model/111-0.1434.hdf5\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.14337 to 0.14224, saving model to ./model/112-0.1422.hdf5\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.14224 to 0.14107, saving model to ./model/113-0.1411.hdf5\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.14107 to 0.14002, saving model to ./model/114-0.1400.hdf5\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.14002 to 0.13916, saving model to ./model/115-0.1392.hdf5\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.13916 to 0.13834, saving model to ./model/116-0.1383.hdf5\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.13834 to 0.13746, saving model to ./model/117-0.1375.hdf5\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.13746 to 0.13707, saving model to ./model/118-0.1371.hdf5\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.13707 to 0.13616, saving model to ./model/119-0.1362.hdf5\n",
      "\n",
      "Epoch 00120: val_loss improved from 0.13616 to 0.13493, saving model to ./model/120-0.1349.hdf5\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.13493 to 0.13408, saving model to ./model/121-0.1341.hdf5\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.13408 to 0.13309, saving model to ./model/122-0.1331.hdf5\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.13309\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.13309\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.13309 to 0.13227, saving model to ./model/125-0.1323.hdf5\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.13227 to 0.13214, saving model to ./model/126-0.1321.hdf5\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.13214\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.13214\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.13214 to 0.13185, saving model to ./model/129-0.1318.hdf5\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.13185 to 0.13012, saving model to ./model/130-0.1301.hdf5\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.13012\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.13012 to 0.12974, saving model to ./model/132-0.1297.hdf5\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.12974\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.12974\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.12974 to 0.12951, saving model to ./model/135-0.1295.hdf5\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.12951 to 0.12790, saving model to ./model/136-0.1279.hdf5\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.12790\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.12790 to 0.12715, saving model to ./model/138-0.1272.hdf5\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.12715\n",
      "\n",
      "Epoch 00140: val_loss improved from 0.12715 to 0.12651, saving model to ./model/140-0.1265.hdf5\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.12651 to 0.12488, saving model to ./model/141-0.1249.hdf5\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.12488 to 0.12423, saving model to ./model/142-0.1242.hdf5\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.12423\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.12423\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.12423 to 0.12291, saving model to ./model/145-0.1229.hdf5\n",
      "\n",
      "Epoch 00146: val_loss improved from 0.12291 to 0.12178, saving model to ./model/146-0.1218.hdf5\n",
      "\n",
      "Epoch 00147: val_loss improved from 0.12178 to 0.12122, saving model to ./model/147-0.1212.hdf5\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.12122\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.12122\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.12122 to 0.11948, saving model to ./model/150-0.1195.hdf5\n",
      "\n",
      "Epoch 00151: val_loss improved from 0.11948 to 0.11818, saving model to ./model/151-0.1182.hdf5\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.11818 to 0.11816, saving model to ./model/152-0.1182.hdf5\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.11816\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.11816 to 0.11766, saving model to ./model/154-0.1177.hdf5\n",
      "\n",
      "Epoch 00155: val_loss improved from 0.11766 to 0.11741, saving model to ./model/155-0.1174.hdf5\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.11741\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.11741\n",
      "\n",
      "Epoch 00158: val_loss improved from 0.11741 to 0.11545, saving model to ./model/158-0.1155.hdf5\n",
      "\n",
      "Epoch 00159: val_loss improved from 0.11545 to 0.11485, saving model to ./model/159-0.1149.hdf5\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.11485 to 0.11448, saving model to ./model/160-0.1145.hdf5\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.11448\n",
      "\n",
      "Epoch 00162: val_loss improved from 0.11448 to 0.11412, saving model to ./model/162-0.1141.hdf5\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.11412 to 0.11108, saving model to ./model/163-0.1111.hdf5\n",
      "\n",
      "Epoch 00164: val_loss improved from 0.11108 to 0.10976, saving model to ./model/164-0.1098.hdf5\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.10976\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.10976\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.10976 to 0.10836, saving model to ./model/167-0.1084.hdf5\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.10836 to 0.10735, saving model to ./model/168-0.1074.hdf5\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.10735 to 0.10634, saving model to ./model/169-0.1063.hdf5\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.10634\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.10634 to 0.10529, saving model to ./model/171-0.1053.hdf5\n",
      "\n",
      "Epoch 00172: val_loss improved from 0.10529 to 0.10340, saving model to ./model/172-0.1034.hdf5\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.10340 to 0.10285, saving model to ./model/173-0.1028.hdf5\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.10285\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.10285\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.10285\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.10285 to 0.10090, saving model to ./model/177-0.1009.hdf5\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.10090 to 0.10044, saving model to ./model/178-0.1004.hdf5\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.10044\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.10044\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.10044 to 0.09816, saving model to ./model/181-0.0982.hdf5\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.09816 to 0.09781, saving model to ./model/182-0.0978.hdf5\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.09781 to 0.09686, saving model to ./model/183-0.0969.hdf5\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.09686\n",
      "\n",
      "Epoch 00185: val_loss improved from 0.09686 to 0.09578, saving model to ./model/185-0.0958.hdf5\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.09578 to 0.09517, saving model to ./model/186-0.0952.hdf5\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.09517\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.09517\n",
      "\n",
      "Epoch 00189: val_loss improved from 0.09517 to 0.09458, saving model to ./model/189-0.0946.hdf5\n",
      "\n",
      "Epoch 00190: val_loss improved from 0.09458 to 0.09448, saving model to ./model/190-0.0945.hdf5\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.09448 to 0.09416, saving model to ./model/191-0.0942.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00192: val_loss did not improve from 0.09416\n",
      "\n",
      "Epoch 00193: val_loss improved from 0.09416 to 0.09353, saving model to ./model/193-0.0935.hdf5\n",
      "\n",
      "Epoch 00194: val_loss improved from 0.09353 to 0.09257, saving model to ./model/194-0.0926.hdf5\n",
      "\n",
      "Epoch 00195: val_loss improved from 0.09257 to 0.09189, saving model to ./model/195-0.0919.hdf5\n",
      "\n",
      "Epoch 00196: val_loss improved from 0.09189 to 0.09132, saving model to ./model/196-0.0913.hdf5\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.09132\n",
      "\n",
      "Epoch 00198: val_loss improved from 0.09132 to 0.09052, saving model to ./model/198-0.0905.hdf5\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.09052\n",
      "\n",
      "Epoch 00200: val_loss improved from 0.09052 to 0.09025, saving model to ./model/200-0.0902.hdf5\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.09025\n",
      "\n",
      "Epoch 00202: val_loss improved from 0.09025 to 0.08954, saving model to ./model/202-0.0895.hdf5\n",
      "\n",
      "Epoch 00203: val_loss improved from 0.08954 to 0.08917, saving model to ./model/203-0.0892.hdf5\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.08917\n",
      "\n",
      "Epoch 00205: val_loss improved from 0.08917 to 0.08831, saving model to ./model/205-0.0883.hdf5\n",
      "\n",
      "Epoch 00206: val_loss improved from 0.08831 to 0.08815, saving model to ./model/206-0.0882.hdf5\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.08815\n",
      "\n",
      "Epoch 00208: val_loss improved from 0.08815 to 0.08763, saving model to ./model/208-0.0876.hdf5\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.08763\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.08763\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.08763\n",
      "\n",
      "Epoch 00212: val_loss improved from 0.08763 to 0.08689, saving model to ./model/212-0.0869.hdf5\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.08689\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.08689\n",
      "\n",
      "Epoch 00215: val_loss improved from 0.08689 to 0.08578, saving model to ./model/215-0.0858.hdf5\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.08578\n",
      "\n",
      "Epoch 00217: val_loss improved from 0.08578 to 0.08507, saving model to ./model/217-0.0851.hdf5\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.08507\n",
      "\n",
      "Epoch 00219: val_loss improved from 0.08507 to 0.08466, saving model to ./model/219-0.0847.hdf5\n",
      "\n",
      "Epoch 00220: val_loss improved from 0.08466 to 0.08413, saving model to ./model/220-0.0841.hdf5\n",
      "\n",
      "Epoch 00221: val_loss improved from 0.08413 to 0.08365, saving model to ./model/221-0.0837.hdf5\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.08365\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.08365\n",
      "\n",
      "Epoch 00224: val_loss improved from 0.08365 to 0.08349, saving model to ./model/224-0.0835.hdf5\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.08349 to 0.08336, saving model to ./model/225-0.0834.hdf5\n",
      "\n",
      "Epoch 00226: val_loss improved from 0.08336 to 0.08327, saving model to ./model/226-0.0833.hdf5\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.08327\n",
      "\n",
      "Epoch 00228: val_loss improved from 0.08327 to 0.08247, saving model to ./model/228-0.0825.hdf5\n",
      "\n",
      "Epoch 00229: val_loss improved from 0.08247 to 0.08215, saving model to ./model/229-0.0821.hdf5\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.08215\n",
      "\n",
      "Epoch 00231: val_loss improved from 0.08215 to 0.08140, saving model to ./model/231-0.0814.hdf5\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.08140\n",
      "\n",
      "Epoch 00233: val_loss improved from 0.08140 to 0.08116, saving model to ./model/233-0.0812.hdf5\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.08116\n",
      "\n",
      "Epoch 00235: val_loss improved from 0.08116 to 0.08088, saving model to ./model/235-0.0809.hdf5\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.08088\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.08088\n",
      "\n",
      "Epoch 00238: val_loss improved from 0.08088 to 0.08004, saving model to ./model/238-0.0800.hdf5\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.08004\n",
      "\n",
      "Epoch 00240: val_loss improved from 0.08004 to 0.07990, saving model to ./model/240-0.0799.hdf5\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.07990\n",
      "\n",
      "Epoch 00242: val_loss improved from 0.07990 to 0.07944, saving model to ./model/242-0.0794.hdf5\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.07944\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.07944\n",
      "\n",
      "Epoch 00245: val_loss improved from 0.07944 to 0.07890, saving model to ./model/245-0.0789.hdf5\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.07890\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.07890\n",
      "\n",
      "Epoch 00248: val_loss improved from 0.07890 to 0.07825, saving model to ./model/248-0.0782.hdf5\n",
      "\n",
      "Epoch 00249: val_loss improved from 0.07825 to 0.07794, saving model to ./model/249-0.0779.hdf5\n",
      "\n",
      "Epoch 00250: val_loss improved from 0.07794 to 0.07767, saving model to ./model/250-0.0777.hdf5\n",
      "\n",
      "Epoch 00251: val_loss improved from 0.07767 to 0.07744, saving model to ./model/251-0.0774.hdf5\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.07744 to 0.07714, saving model to ./model/252-0.0771.hdf5\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.07714 to 0.07672, saving model to ./model/253-0.0767.hdf5\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.07672\n",
      "\n",
      "Epoch 00255: val_loss improved from 0.07672 to 0.07620, saving model to ./model/255-0.0762.hdf5\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.07620\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.07620\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.07620\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.07620\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.07620 to 0.07617, saving model to ./model/260-0.0762.hdf5\n",
      "\n",
      "Epoch 00261: val_loss improved from 0.07617 to 0.07577, saving model to ./model/261-0.0758.hdf5\n",
      "\n",
      "Epoch 00262: val_loss improved from 0.07577 to 0.07534, saving model to ./model/262-0.0753.hdf5\n",
      "\n",
      "Epoch 00263: val_loss improved from 0.07534 to 0.07520, saving model to ./model/263-0.0752.hdf5\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.07520\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.07520\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.07520\n",
      "\n",
      "Epoch 00267: val_loss improved from 0.07520 to 0.07491, saving model to ./model/267-0.0749.hdf5\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.07491\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.07491\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.07491 to 0.07486, saving model to ./model/270-0.0749.hdf5\n",
      "\n",
      "Epoch 00271: val_loss improved from 0.07486 to 0.07482, saving model to ./model/271-0.0748.hdf5\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.07482\n",
      "\n",
      "Epoch 00273: val_loss improved from 0.07482 to 0.07451, saving model to ./model/273-0.0745.hdf5\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.07451\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.07451\n",
      "\n",
      "Epoch 00276: val_loss improved from 0.07451 to 0.07386, saving model to ./model/276-0.0739.hdf5\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.07386\n",
      "\n",
      "Epoch 00278: val_loss improved from 0.07386 to 0.07347, saving model to ./model/278-0.0735.hdf5\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.07347\n",
      "\n",
      "Epoch 00280: val_loss improved from 0.07347 to 0.07315, saving model to ./model/280-0.0732.hdf5\n",
      "\n",
      "Epoch 00281: val_loss improved from 0.07315 to 0.07301, saving model to ./model/281-0.0730.hdf5\n",
      "\n",
      "Epoch 00282: val_loss improved from 0.07301 to 0.07283, saving model to ./model/282-0.0728.hdf5\n",
      "\n",
      "Epoch 00283: val_loss improved from 0.07283 to 0.07234, saving model to ./model/283-0.0723.hdf5\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.07234\n",
      "\n",
      "Epoch 00285: val_loss improved from 0.07234 to 0.07194, saving model to ./model/285-0.0719.hdf5\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.07194\n",
      "\n",
      "Epoch 00287: val_loss improved from 0.07194 to 0.07193, saving model to ./model/287-0.0719.hdf5\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.07193\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.07193\n",
      "\n",
      "Epoch 00290: val_loss improved from 0.07193 to 0.07181, saving model to ./model/290-0.0718.hdf5\n",
      "\n",
      "Epoch 00291: val_loss improved from 0.07181 to 0.07174, saving model to ./model/291-0.0717.hdf5\n",
      "\n",
      "Epoch 00292: val_loss improved from 0.07174 to 0.07132, saving model to ./model/292-0.0713.hdf5\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.07132\n",
      "\n",
      "Epoch 00294: val_loss improved from 0.07132 to 0.07118, saving model to ./model/294-0.0712.hdf5\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.07118\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.07118\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.07118\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.07118\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.07118 to 0.07091, saving model to ./model/299-0.0709.hdf5\n",
      "\n",
      "Epoch 00300: val_loss improved from 0.07091 to 0.07039, saving model to ./model/300-0.0704.hdf5\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.07039\n",
      "\n",
      "Epoch 00302: val_loss improved from 0.07039 to 0.07015, saving model to ./model/302-0.0702.hdf5\n",
      "\n",
      "Epoch 00303: val_loss improved from 0.07015 to 0.06988, saving model to ./model/303-0.0699.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00304: val_loss did not improve from 0.06988\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.06988\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.06988\n",
      "\n",
      "Epoch 00307: val_loss improved from 0.06988 to 0.06987, saving model to ./model/307-0.0699.hdf5\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.06987\n",
      "\n",
      "Epoch 00309: val_loss improved from 0.06987 to 0.06941, saving model to ./model/309-0.0694.hdf5\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.06941\n",
      "\n",
      "Epoch 00311: val_loss improved from 0.06941 to 0.06920, saving model to ./model/311-0.0692.hdf5\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.06920\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.06920\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.06920\n",
      "\n",
      "Epoch 00315: val_loss improved from 0.06920 to 0.06914, saving model to ./model/315-0.0691.hdf5\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.06914\n",
      "\n",
      "Epoch 00317: val_loss improved from 0.06914 to 0.06903, saving model to ./model/317-0.0690.hdf5\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.06903\n",
      "\n",
      "Epoch 00319: val_loss improved from 0.06903 to 0.06851, saving model to ./model/319-0.0685.hdf5\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.06851\n",
      "\n",
      "Epoch 00321: val_loss improved from 0.06851 to 0.06778, saving model to ./model/321-0.0678.hdf5\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.06778\n",
      "\n",
      "Epoch 00323: val_loss improved from 0.06778 to 0.06752, saving model to ./model/323-0.0675.hdf5\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.06752\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.06752\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.06752\n",
      "\n",
      "Epoch 00327: val_loss improved from 0.06752 to 0.06725, saving model to ./model/327-0.0673.hdf5\n",
      "\n",
      "Epoch 00328: val_loss improved from 0.06725 to 0.06720, saving model to ./model/328-0.0672.hdf5\n",
      "\n",
      "Epoch 00329: val_loss improved from 0.06720 to 0.06705, saving model to ./model/329-0.0671.hdf5\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.06705\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.06705\n",
      "\n",
      "Epoch 00332: val_loss improved from 0.06705 to 0.06681, saving model to ./model/332-0.0668.hdf5\n",
      "\n",
      "Epoch 00333: val_loss improved from 0.06681 to 0.06679, saving model to ./model/333-0.0668.hdf5\n",
      "\n",
      "Epoch 00334: val_loss improved from 0.06679 to 0.06678, saving model to ./model/334-0.0668.hdf5\n",
      "\n",
      "Epoch 00335: val_loss improved from 0.06678 to 0.06670, saving model to ./model/335-0.0667.hdf5\n",
      "\n",
      "Epoch 00336: val_loss improved from 0.06670 to 0.06641, saving model to ./model/336-0.0664.hdf5\n",
      "\n",
      "Epoch 00337: val_loss improved from 0.06641 to 0.06611, saving model to ./model/337-0.0661.hdf5\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.06611\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.06611\n",
      "\n",
      "Epoch 00340: val_loss improved from 0.06611 to 0.06600, saving model to ./model/340-0.0660.hdf5\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.06600\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.06600\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.06600\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.06600\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.06600\n",
      "\n",
      "Epoch 00346: val_loss improved from 0.06600 to 0.06598, saving model to ./model/346-0.0660.hdf5\n",
      "\n",
      "Epoch 00347: val_loss improved from 0.06598 to 0.06583, saving model to ./model/347-0.0658.hdf5\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.06583\n",
      "\n",
      "Epoch 00349: val_loss improved from 0.06583 to 0.06578, saving model to ./model/349-0.0658.hdf5\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.06578\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.06578\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.06578\n",
      "\n",
      "Epoch 00353: val_loss improved from 0.06578 to 0.06521, saving model to ./model/353-0.0652.hdf5\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.06521\n",
      "\n",
      "Epoch 00355: val_loss improved from 0.06521 to 0.06509, saving model to ./model/355-0.0651.hdf5\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.06509\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.06509\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.06509\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.06509\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.06509 to 0.06486, saving model to ./model/360-0.0649.hdf5\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.06486\n",
      "\n",
      "Epoch 00362: val_loss improved from 0.06486 to 0.06411, saving model to ./model/362-0.0641.hdf5\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.06411\n",
      "\n",
      "Epoch 00364: val_loss improved from 0.06411 to 0.06394, saving model to ./model/364-0.0639.hdf5\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.06394\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.06394\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.06394\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.06394\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.06394\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.06394\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.06394\n",
      "\n",
      "Epoch 00372: val_loss improved from 0.06394 to 0.06353, saving model to ./model/372-0.0635.hdf5\n",
      "\n",
      "Epoch 00373: val_loss improved from 0.06353 to 0.06309, saving model to ./model/373-0.0631.hdf5\n",
      "\n",
      "Epoch 00374: val_loss improved from 0.06309 to 0.06293, saving model to ./model/374-0.0629.hdf5\n",
      "\n",
      "Epoch 00375: val_loss improved from 0.06293 to 0.06289, saving model to ./model/375-0.0629.hdf5\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.06289\n",
      "\n",
      "Epoch 00389: val_loss improved from 0.06289 to 0.06246, saving model to ./model/389-0.0625.hdf5\n",
      "\n",
      "Epoch 00390: val_loss improved from 0.06246 to 0.06229, saving model to ./model/390-0.0623.hdf5\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.06229\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.06229\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.06229\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.06229\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.06229\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.06229\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.06229\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.06229\n",
      "\n",
      "Epoch 00399: val_loss improved from 0.06229 to 0.06135, saving model to ./model/399-0.0614.hdf5\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.06135\n",
      "\n",
      "Epoch 00401: val_loss improved from 0.06135 to 0.06128, saving model to ./model/401-0.0613.hdf5\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.06128\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.06128\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.06128\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.06128\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.06128\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.06128\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.06128\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.06128\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.06128\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.06128\n",
      "\n",
      "Epoch 00412: val_loss improved from 0.06128 to 0.06091, saving model to ./model/412-0.0609.hdf5\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.06091\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.06091\n",
      "\n",
      "Epoch 00415: val_loss improved from 0.06091 to 0.06075, saving model to ./model/415-0.0607.hdf5\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.06075\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.06075 to 0.06073, saving model to ./model/417-0.0607.hdf5\n",
      "\n",
      "Epoch 00418: val_loss improved from 0.06073 to 0.06050, saving model to ./model/418-0.0605.hdf5\n",
      "\n",
      "Epoch 00419: val_loss improved from 0.06050 to 0.06010, saving model to ./model/419-0.0601.hdf5\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.06010\n",
      "\n",
      "Epoch 00421: val_loss improved from 0.06010 to 0.05984, saving model to ./model/421-0.0598.hdf5\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.05984\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.05984\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.05984\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.05984\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.05984\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.05984\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.05984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00429: val_loss did not improve from 0.05984\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.05984\n",
      "\n",
      "Epoch 00431: val_loss improved from 0.05984 to 0.05979, saving model to ./model/431-0.0598.hdf5\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.05979\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.05979\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.05979\n",
      "\n",
      "Epoch 00435: val_loss improved from 0.05979 to 0.05961, saving model to ./model/435-0.0596.hdf5\n",
      "\n",
      "Epoch 00436: val_loss improved from 0.05961 to 0.05920, saving model to ./model/436-0.0592.hdf5\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.05920\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.05920\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.05920\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.05920\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.05920\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.05920\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.05920\n",
      "\n",
      "Epoch 00444: val_loss improved from 0.05920 to 0.05896, saving model to ./model/444-0.0590.hdf5\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.05896\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.05896\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.05896\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.05896\n",
      "\n",
      "Epoch 00449: val_loss improved from 0.05896 to 0.05891, saving model to ./model/449-0.0589.hdf5\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.05891\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.05891\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.05891\n",
      "\n",
      "Epoch 00453: val_loss improved from 0.05891 to 0.05824, saving model to ./model/453-0.0582.hdf5\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.05824\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.05824\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.05824\n",
      "\n",
      "Epoch 00457: val_loss improved from 0.05824 to 0.05776, saving model to ./model/457-0.0578.hdf5\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.05776\n",
      "\n",
      "Epoch 00477: val_loss improved from 0.05776 to 0.05681, saving model to ./model/477-0.0568.hdf5\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.05681\n",
      "\n",
      "Epoch 00494: val_loss improved from 0.05681 to 0.05651, saving model to ./model/494-0.0565.hdf5\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.05651\n",
      "\n",
      "Epoch 00496: val_loss improved from 0.05651 to 0.05645, saving model to ./model/496-0.0564.hdf5\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.05645\n",
      "\n",
      "Epoch 00517: val_loss improved from 0.05645 to 0.05631, saving model to ./model/517-0.0563.hdf5\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.05631\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.05631\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.05631\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.05631\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.05631\n",
      "\n",
      "Epoch 00523: val_loss improved from 0.05631 to 0.05617, saving model to ./model/523-0.0562.hdf5\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.05617\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.05617\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.05617\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.05617\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.05617\n",
      "\n",
      "Epoch 00529: val_loss improved from 0.05617 to 0.05610, saving model to ./model/529-0.0561.hdf5\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.05610\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.05610\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.05610\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.05610\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.05610\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.05610\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.05610\n",
      "\n",
      "Epoch 00537: val_loss improved from 0.05610 to 0.05585, saving model to ./model/537-0.0559.hdf5\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.05585\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.05585\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.05585\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.05585\n",
      "\n",
      "Epoch 00542: val_loss improved from 0.05585 to 0.05536, saving model to ./model/542-0.0554.hdf5\n",
      "\n",
      "Epoch 00543: val_loss improved from 0.05536 to 0.05494, saving model to ./model/543-0.0549.hdf5\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.05494\n",
      "\n",
      "Epoch 00567: val_loss improved from 0.05494 to 0.05471, saving model to ./model/567-0.0547.hdf5\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.05471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00573: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.05471\n",
      "\n",
      "Epoch 00590: val_loss improved from 0.05471 to 0.05460, saving model to ./model/590-0.0546.hdf5\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.05460\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.05460\n",
      "\n",
      "Epoch 00593: val_loss improved from 0.05460 to 0.05457, saving model to ./model/593-0.0546.hdf5\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.05457\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.05457\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.05457\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.05457\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.05457\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.05457\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.05457\n",
      "\n",
      "Epoch 00601: val_loss improved from 0.05457 to 0.05414, saving model to ./model/601-0.0541.hdf5\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.05414\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.05414\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.05414\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.05414\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.05414\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.05414\n",
      "\n",
      "Epoch 00608: val_loss improved from 0.05414 to 0.05378, saving model to ./model/608-0.0538.hdf5\n",
      "\n",
      "Epoch 00609: val_loss improved from 0.05378 to 0.05329, saving model to ./model/609-0.0533.hdf5\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.05329\n",
      "\n",
      "Epoch 00642: val_loss improved from 0.05329 to 0.05316, saving model to ./model/642-0.0532.hdf5\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.05316\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.05316\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.05316\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.05316\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.05316\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.05316\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.05316\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.05316\n",
      "\n",
      "Epoch 00651: val_loss improved from 0.05316 to 0.05302, saving model to ./model/651-0.0530.hdf5\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.05302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00738: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.05302\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.05302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x64bce7810>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "import glob\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
    "\n",
    "del checkpointer\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "files = glob.glob(''.join([MODEL_DIR, \"*\"]))\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "del model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y, validation_split=0.2, epochs=3500, batch_size=500, verbose=0, callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975/975 [==============================] - 0s 28us/step\n",
      "Accuracy: 0.9856410026550293\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(model.evaluate(X, Y)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
